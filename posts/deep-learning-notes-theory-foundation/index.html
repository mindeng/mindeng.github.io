<!doctype html><html lang=zh-cn><head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#"><meta charset=UTF-8><meta name=generator content="Hugo 0.128.1"><meta name=theme-color content="#fff"><meta name=color-scheme content="light dark"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no, date=no, address=no, email=no"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><title>深度学习笔记：理论基础 | MinCodes</title>
<link rel=stylesheet href=/css/meme.min.91fac3abe9ca5457437df0f6d067955f198963c5e78e0dbba380c40eab8ca7d7.css><script src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js defer></script><script src=/js/meme.min.238001acc8d107d31839007c618ccd38c64210ddf0869d445de90497fdbe5861.js></script><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Source+Code+Pro:ital,wght@0,400;0,700;1,400;1,700&amp;family=Comfortaa:wght@700&amp;display=swap" media=print onload='this.media="all"'><noscript><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Source+Code+Pro:ital,wght@0,400;0,700;1,400;1,700&amp;family=Comfortaa:wght@700&amp;display=swap"></noscript><meta name=author content="Min Deng"><meta name=description content="深度学习和神经网络 深度学习是一种实现机器学习的技术，而神经网络是实现深度学习的基本结构。 具体来说： 神经网络: 神经网络是一种模仿人脑处理信息方式的计算系统。它由大…"><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=mask-icon href=/icons/safari-pinned-tab.svg color=#2a6df4><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-title content="MinCodes"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=mobile-web-app-capable content="yes"><meta name=application-name content="MinCodes"><meta name=msapplication-starturl content="../../"><meta name=msapplication-TileColor content="#fff"><meta name=msapplication-TileImage content="../../icons/mstile-150x150.png"><link rel=manifest href=/manifest.json><link rel=canonical href=https://mincodes.com/posts/deep-learning-notes-theory-foundation/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","datePublished":"2023-12-07T16:34:00+08:00","dateModified":"2024-04-28T18:16:19+08:00","url":"https://mincodes.com/posts/deep-learning-notes-theory-foundation/","headline":"深度学习笔记：理论基础","description":"深度学习和神经网络 深度学习是一种实现机器学习的技术，而神经网络是实现深度学习的基本结构。 具体来说： 神经网络: 神经网络是一种模仿人脑处理信息方式的计算系统。它由大…","inLanguage":"zh-cn","articleSection":"posts","wordCount":6328,"image":["https://mincodes.com/ox-hugo/2024-04-28_16-29-15_screenshot.png"],"author":{"@type":"Person","description":"为之，则难者亦易矣；不为，则易者亦难矣。","image":"https://mincodes.com/icons/apple-touch-icon.png","url":"https://mincodes.com/","name":"Min Deng"},"license":"[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)","publisher":{"@type":"Organization","name":"MinCodes","logo":{"@type":"ImageObject","url":"https://mincodes.com/icons/apple-touch-icon.png"},"url":"https://mincodes.com/"},"mainEntityOfPage":{"@type":"WebSite","@id":"https://mincodes.com/"}}</script><meta name=twitter:card content="summary_large_image"><meta property="og:title" content="深度学习笔记：理论基础"><meta property="og:description" content="深度学习和神经网络 深度学习是一种实现机器学习的技术，而神经网络是实现深度学习的基本结构。 具体来说： 神经网络: 神经网络是一种模仿人脑处理信息方式的计算系统。它由大…"><meta property="og:url" content="https://mincodes.com/posts/deep-learning-notes-theory-foundation/"><meta property="og:site_name" content="MinCodes"><meta property="og:locale" content="en"><meta property="og:image" content="https://mincodes.com/ox-hugo/2024-04-28_16-29-15_screenshot.png"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-12-07T16:34:00+08:00"><meta property="article:modified_time" content="2024-04-28T18:16:19+08:00"><meta property="article:section" content="posts"></head><body><div class=container><header class=header><div class=header-wrapper><div class="header-inner single"><div class=site-brand><a href=/ class=brand>MinCodes</a></div><nav class=nav><ul class=menu id=menu><li class="menu-item active"><a href=/posts/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon archive"><path d="M32 448c0 17.7 14.3 32 32 32h384c17.7.0 32-14.3 32-32V160H32v288zm160-212c0-6.6 5.4-12 12-12h104c6.6.0 12 5.4 12 12v8c0 6.6-5.4 12-12 12H204c-6.6.0-12-5.4-12-12v-8zM480 32H32C14.3 32 0 46.3.0 64v48c0 8.8 7.2 16 16 16h480c8.8.0 16-7.2 16-16V64c0-17.7-14.3-32-32-32z"/></svg><span class=menu-item-name>Posts</span></a></li><li class=menu-item><a href=/tags/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="icon tags"><path d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a48 48 0 0014.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM112 160c-26.51.0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882.0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397.0h48.721a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882z"/></svg><span class=menu-item-name>Tags</span></a></li><li class=menu-item><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon user-circle"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6.0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7.0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4.0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9s28-2.7 40.9-6.9c2.3-.7 4.7-1.1 7.1-1.1 42.9.0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><span class=menu-item-name>About</span></a></li><li class=menu-item><a id=theme-switcher href=#><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5 242 7a18 18 0 0128 0l48.8 97.5L422.2 70A18 18 0 01442 89.8l-34.5 103.4L505 242a18 18 0 010 28l-97.5 48.8L442 422.2A18 18 0 01422.2 442l-103.4-34.5L270 505a18 18 0 01-28 0l-48.8-97.5L89.8 442A18 18 0 0170 422.2l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8L70 89.8A18 18 0 0189.8 70zM256 128a128 128 0 10.01.0M256 160a96 96 0 10.01.0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412A256 256 0 10181 5a11.5 11.5.0 00-5 20A201.5 201.5.0 0142 399a11.5 11.5.0 00-15 13"/></svg></a></li><li class="menu-item search-item"><form id=search class=search role=search><label for=search-input><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon search-icon"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></label>
<input type=search id=search-input class=search-input></form><template id=search-result hidden><article class="content post"><h2 class=post-title><a class=summary-title-link></a></h2><summary class=summary></summary><div class=read-more-container><a class=read-more-link>Read More »</a></div></article></template></li></ul></nav></div></div><input type=checkbox id=nav-toggle aria-hidden=true>
<label for=nav-toggle class=nav-toggle></label>
<label for=nav-toggle class=nav-curtain></label></header><main class="main single" id=main><div class=main-inner><article class="content post h-entry" data-small-caps=true data-align=default data-type=posts data-toc-num=true><h1 class="post-title p-name">深度学习笔记：理论基础</h1><div class=post-meta><time datetime=2023-12-07T16:34:00+08:00 class="post-meta-item published dt-published"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M148 288h-40c-6.6.0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5.0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v52h48c26.5.0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3.0 6-2.7 6-6z"/></svg>&nbsp;2023.12.7</time>
<time datetime=2024-04-28T18:16:19+08:00 class="post-meta-item modified dt-updated"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M4e2 64h-48V12c0-6.627-5.373-12-12-12h-40c-6.627.0-12 5.373-12 12v52H160V12c0-6.627-5.373-12-12-12h-40c-6.627.0-12 5.373-12 12v52H48C21.49 64 0 85.49.0 112v352c0 26.51 21.49 48 48 48h352c26.51.0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm-6 4e2H54a6 6 0 01-6-6V160h352v298a6 6 0 01-6 6zm-52.849-200.65L198.842 404.519c-4.705 4.667-12.303 4.637-16.971-.068l-75.091-75.699c-4.667-4.705-4.637-12.303.068-16.971l22.719-22.536c4.705-4.667 12.303-4.637 16.97.069l44.104 44.461 111.072-110.181c4.705-4.667 12.303-4.637 16.971.068l22.536 22.718c4.667 4.705 4.636 12.303-.069 16.97z"/></svg>&nbsp;2024.4.28</time>
<span class="post-meta-item wordcount"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3.0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9.0l60.1 60.1c18.8 18.7 18.8 49.1.0 67.9zM284.2 99.8 21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3.0-17l-111-111c-4.8-4.7-12.4-4.7-17.1.0zM124.1 339.9c-5.5-5.5-5.5-14.3.0-19.8l154-154c5.5-5.5 14.3-5.5 19.8.0s5.5 14.3.0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8.0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;6328</span>
<span class="post-meta-item reading-time"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5.0-2e2-89.5-2e2-2e2S145.5 56 256 56s2e2 89.5 2e2 2e2-89.5 2e2-2e2 2e2zm61.8-104.4-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6.0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;13&nbsp;mins</span></div><nav class=contents><h2 id=contents class=contents-title>Contents</h2><ol class=toc><li><a id=contents:深度学习和神经网络 href=#深度学习和神经网络>深度学习和神经网络</a></li><li><a id=contents:神经网络和模型 href=#神经网络和模型>神经网络和模型</a></li><li><a id=contents:神经网络的学习过程 href=#神经网络的学习过程>神经网络的学习过程</a><ol><li><a id=contents:批量学习-batch-learning href=#批量学习-batch-learning>批量学习（Batch Learning）</a></li><li><a id=contents:训练数据的顺序 href=#训练数据的顺序>训练数据的顺序</a></li></ol></li><li><a id=contents:对神经元的理解 href=#对神经元的理解>对神经元的理解</a></li><li><a id=contents:损失函数 href=#损失函数>损失函数</a></li><li><a id=contents:梯度 href=#梯度>梯度</a><ol><li><a id=contents:导数-偏导数和梯度 href=#导数-偏导数和梯度>导数、偏导数和梯度</a></li></ol></li><li><a id=contents:梯度下降-gradient-descent href=#梯度下降-gradient-descent>梯度下降（Gradient Descent）</a><ol><li><a id=contents:成本表面和等值线图-cost-surface-and-contour-plot href=#成本表面和等值线图-cost-surface-and-contour-plot>成本表面和等值线图（Cost Surface & Contour Plot）</a></li><li><a id=contents:如何根据损失函数的梯度-学习率更新权重 href=#如何根据损失函数的梯度-学习率更新权重>如何根据损失函数的梯度、学习率更新权重</a></li></ol></li><li><a id=contents:常见的神经网络类型 href=#常见的神经网络类型>常见的神经网络类型</a></li><li><a id=contents:参考资料 href=#参考资料>参考资料</a></li></ol></nav><div class="post-body e-content"><h2 id=深度学习和神经网络><a href=#深度学习和神经网络 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:深度学习和神经网络 class=headings>深度学习和神经网络</a></h2><p style=text-indent:0><span class=drop-cap>深</span>度学习是一种实现机器学习的技术，而神经网络是实现深度学习的基本结构。</p><p>具体来说：</p><ol><li><p><strong>神经网络</strong>:</p><ul><li>神经网络是一种模仿人脑处理信息方式的计算系统。它由大量的节点（神经元）组成，这些节点通过层次结构相互连接。</li><li>神经网络可以是浅层的（少数层），也可以是深层的（多层）。浅层神经网络常用于较简单的模式识别和数据分类任务。</li></ul></li><li><p><strong>深度学习</strong>:</p><ul><li>深度学习是一种机器学习技术，它特别依赖于深层神经网络，即包含多个隐藏层的神经网络。</li><li>深层神经网络能够学习和模拟数据中的高度复杂的模式和关系，这使得深度学习在图像识别、自然语言处理和许多其他领域非常有效。</li></ul></li><li><p><strong>关系</strong>:</p><ul><li>所有深度学习模型都是神经网络，但并非所有神经网络都用于深度学习。深度学习特指使用深层神经网络来进行学习和预测的方法。</li><li>深度学习的“深度”指的是网络的层数。更多的层使得网络能够捕获更复杂、更抽象的数据特征。</li></ul></li></ol><p>神经网络是构成深度学习的基础，而深度学习则是利用这些神经网络来实现复杂任务的学习方法。通过构建和训练深层神经网络，深度学习模型能够学习从简单到复杂的数据模式，解决各种复杂的实际问题。</p><h2 id=神经网络和模型><a href=#神经网络和模型 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:神经网络和模型 class=headings>神经网络和模型</a></h2><p>神经网络和模型之间的关系可以这样来阐述：</p><ol><li><p><strong>神经网络</strong>:</p><ul><li>神经网络是一个由多层神经元组成的计算结构，用于模拟复杂的函数映射。每层包含若干神经元，每个神经元通过权重和偏差与其他神经元相连。</li><li>神经网络的设计（例如，层数、每层的神经元数量、激活函数等）定义了其结构和潜在的计算能力。</li></ul></li><li><p><strong>模型</strong>:</p><ul><li>当我们谈论“模型”时，通常指的是训练好的神经网络，包括其所有参数（权重和偏差）的特定设置。</li><li>模型是神经网络训练过程的产物，它捕捉了训练数据中的模式和关系。</li></ul></li><li><p><strong>训练过程</strong>:</p><ul><li>在训练过程中，神经网络通过调整其权重和偏差来学习数据中的模式。</li><li>这些调整是通过计算损失函数的梯度并应用优化算法（如梯度下降）来实现的。</li><li>训练完成后，网络的权重和偏差被“冻结”，形成了最终的模型。</li></ul></li><li><p><strong>关系和理解</strong>:</p><ul><li>可以将神经网络视为模型的“蓝图”，而训练后的网络则是根据这个蓝图构建的具体实例。</li><li>模型是训练过程的结果，它包含了为了最大化性能而调整和优化的参数集合。</li></ul></li></ol><p>综上，神经网络在训练过程中自动更新其权重和参数，而训练完成后，这些优化过的权重和参数值构成了最终的模型。这个模型可以用来做预测或进一步分析。</p><h2 id=神经网络的学习过程><a href=#神经网络的学习过程 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:神经网络的学习过程 class=headings>神经网络的学习过程</a></h2><p>以图片分类问题为例：</p><ol><li><p><strong>初始状态</strong>:</p><ul><li>在训练开始时，神经网络的权重和偏差通常是随机初始化的。这意味着网络对于分类任务“一无所知”，其初始预测是基于这些随机参数。</li></ul></li><li><p><strong>喂入数据</strong>:</p><ul><li>然后，我们向网络喂入带有标签的训练数据。在分类问题中，这通常是一组标记了正确类别的图片。</li></ul></li><li><p><strong>进行预测</strong>:</p><ul><li>对于每张图片，网络根据当前的权重和偏差进行预测，尝试判断图片所属的类别。</li><li>在 PyTorch 中，相当于 <code>forward()</code> 或者 <code>model()</code> 所做的事情。</li></ul></li><li><p><strong>计算损失</strong>:</p><ul><li>网络的预测结果会与实际的标签进行比较，使用损失函数来计算预测值和真实值之间的差异。例如，如果使用交叉熵损失函数，在分类问题中，这个函数会衡量网络预测的概率分布与实际标签的概率分布之间的差异。</li></ul></li><li><p><strong>调整权重和偏差</strong>:</p><ul><li>根据损失函数的结果，网络通过梯度下降（或其他优化算法）来调整其权重和偏差。这个调整是为了减少损失，即减少预测值和真实值之间的差异。</li><li>这个过程涉及到计算损失函数相对于每个权重和偏差的梯度，然后按这些梯度的反方向来更新参数。</li><li>在 PyTorch 中，相当于执行 <code>optimizer.step()</code> 。</li></ul></li><li><p><strong>迭代和学习</strong>:</p><ul><li>这个过程在整个训练集上反复进行，网络逐渐学习并提升其预测的准确性。</li><li>随着训练的进行，网络能够更好地理解数据中的模式和特征，从而在面对新的、未见过的数据时也能做出更准确的预测。</li></ul></li></ol><p>综上，这是一个迭代的学习过程，神经网络通过不断调整自身的参数，以最小化损失函数，进而提高对数据的理解和分类的准确性。</p><h3 id=批量学习-batch-learning><a href=#批量学习-batch-learning class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:批量学习-batch-learning class=headings>批量学习（Batch Learning）</a></h3><p>神经网络在计算损失时通常采用的是分组（批处理）的方式。这种方法被称为批量学习（Batch Learning）。在这种方法中，网络不是对每张图片单独进行预测和计算损失，而是对一组图片（即一个批次）进行预测，然后计算这个批次的总损失。以下是这个过程的关键点：</p><ol><li><p><strong>批处理（Batching）</strong>:</p><ul><li>在训练神经网络时，数据通常被分成小批次（batches）。一个批次包含多张图片，批次的大小（即批大小，Batch Size）可以根据需要和硬件资源进行设置。</li><li>批大小是一个重要的超参数，它影响着模型的内存占用、训练速度和收敛性。</li></ul></li><li><p><strong>预测和损失计算</strong>:</p><ul><li>网络对整个批次的所有图片同时进行预测。</li><li>然后，损失函数计算每张图片的预测结果与其实际标签之间的差异，并将这些差异合成整个批次的总损失。在分类问题中，如果使用交叉熵损失函数，这意味着对批次中每张图片的预测概率分布与实际标签的概率分布之间的差异进行计算和求和。</li></ul></li><li><p><strong>梯度计算和参数更新</strong>:</p><ul><li>根据这个批次的总损失，计算损失相对于网络参数的梯度。</li><li>这些梯度用于更新网络的权重和偏差，以减少未来预测的总损失。</li></ul></li><li><p><strong>优势</strong>:</p><ul><li>批处理可以提高数据处理的效率，特别是在使用 GPU 等硬件加速时。</li><li>它还有助于稳定学习过程，因为每次更新是基于多个样本的平均损失，而不是单个样本的损失。</li></ul></li></ol><p>通过批量处理方法，神经网络在每次更新参数之前，都会考虑一组数据的总体表现，而不是单个数据点。这种方法有助于提高训练的效率和稳定性。</p><h3 id=训练数据的顺序><a href=#训练数据的顺序 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:训练数据的顺序 class=headings>训练数据的顺序</a></h3><p>在训练分类模型时，输入数据的随机化（即将不同类别的样本混合）通常是推荐的做法。以下是这种做法的原因：</p><ol><li><p><strong>避免偏见</strong>:</p><ul><li>如果神经网络先训练所有的猫的图片，然后再训练所有的狗的图片，它可能会在训练初期过度适应（即过拟合）猫的特征，从而忽略了狗的特征。这可能导致模型在初始阶段形成偏见，使得后期难以正确学习和识别狗的特征。</li></ul></li><li><p><strong>提高泛化能力</strong>:</p><ul><li>通过随机混合不同类别的图片，模型能够更平衡地学习各个类别的特征。这有助于提高模型的泛化能力，即在未见过的新数据上的表现。</li></ul></li><li><p><strong>避免过早收敛</strong>:</p><ul><li>如果数据是有序的，模型可能会在训练的早期阶段过早地收敛到对当前顺序敏感的解决方案。通过随机化数据，可以促使模型探索更多的可能性，从而找到更具泛化性的解决方案。</li></ul></li><li><p><strong>实践中的操作</strong>:</p><ul><li>在实际操作中，通常会在每个训练周期（epoch）开始前随机打乱数据。这意味着每个批次（batch）中的图片都是随机选择的，包含不同类别的混合。</li><li>这种方法有助于确保每个类别在整个训练过程中都被公平地表示。</li></ul></li></ol><p>因此，为了获得最佳的训练效果，推荐将不同标签的图片混合在一起，然后进行随机化处理后输入训练。这有助于确保模型能够更有效地学习区分这两个类别的特征。</p><h2 id=对神经元的理解><a href=#对神经元的理解 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:对神经元的理解 class=headings>对神经元的理解</a></h2><p>神经元在神经网络中可以看作是一个数学函数。以下是神经元的本质以及权重和偏差在其中的作用：</p><ol><li><p><strong>神经元的本质</strong>:</p><ul><li>神经元（或称为节点）基本上是一个接收输入、产生输出的数学函数。</li><li>它接收来自前一层神经元的多个输入，这些输入通常是加权和的形式，其中每个输入都被相应的权重所乘。</li></ul></li><li><p><strong>激活函数</strong>:</p><ul><li>神经元的输出不仅仅是其输入的线性组合。输入的加权和通常会通过一个非线性函数，即激活函数，如 Sigmoid、ReLU 或 tanh 等。</li><li>这个激活函数使得神经网络能够捕捉和学习复杂的非线性关系。</li></ul></li><li><p><strong>权重和偏差</strong>:</p><ul><li>权重（Weights）决定了输入信号在影响输出时的强度和重要性。每个输入都有一个相应的权重。</li><li>偏差（Bias）是加到加权和上的一个常数，它提供了额外的自由度，使得神经元即使在所有输入都是零时也能有非零的输出。</li></ul></li><li><p><strong>数学表示</strong>:</p><ul><li>从数学的角度来看，一个神经元的操作可以表示为
\(f(w_1x_1+w_2x_2+&mldr;+w_nx_n+b)\) ，其中 \(w_1,w_2,&mldr;,w_n\) 是权重，
\(x_1,x_2,&mldr;,x_n\) 是输入， \(b\) 是偏差， \(f\) 是激活函数。</li></ul></li><li><p><strong>函数的角色</strong>:</p><ul><li>在这个框架下，权重和偏差类似于函数方程的系数和常量，它们调节输入如何影响输出。</li><li>神经网络通过学习这些权重和偏差的最佳值来拟合和预测数据。</li></ul></li></ol><p>因此，可以将神经元视为一个通过权重和偏差调整其输入的函数，通过激活函数引入非线性，使得整个网络能够学习和模拟复杂的数据关系和模式。</p><h2 id=损失函数><a href=#损失函数 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:损失函数 class=headings>损失函数</a></h2><p>损失函数在机器学习和深度学习中扮演着至关重要的角色。它是一个用来衡量模型性能的函数，特别是在模型预测和实际数据之间的差异。以下是损失函数的一些关键特点：</p><ol><li><p><strong>定义</strong>:</p><ul><li>损失函数，有时也被称为代价函数，是一个衡量单个样本预测错误的度量。在给定输入和模型的情况下，损失函数输出一个数值，这个数值表示预测值和真实值之间的差距。</li></ul></li><li><p><strong>目的</strong>:</p><ul><li>损失函数的主要目的是指导模型的学习。模型训练的目标是最小化损失函数，即减少预测值和真实值之间的差异。</li></ul></li><li><p><strong>常见类型</strong>:</p><ul><li><strong>均方误差（MSE）</strong>: 在回归问题中常用，计算预测值和实际值之差的平方的平均值。</li><li><strong>交叉熵损失</strong>: 在分类问题中常用，特别是在二分类或多分类问题中。它衡量的是模型预测的概率分布与实际标签的概率分布之间的差异。</li></ul></li><li><p><strong>在神经网络中的作用</strong>:</p><ul><li>在神经网络训练过程中，通过计算损失函数并使用诸如梯度下降之类的优化算法来调整网络的权重，使得损失函数的值最小化。</li></ul></li><li><p><strong>选择损失函数</strong>:</p><ul><li>选择哪种损失函数取决于特定的机器学习任务（如回归、分类、聚类等）和数据特性。</li></ul></li></ol><p>损失函数是连接模型预测和真实数据的桥梁，它提供了一种量化模型性能的方式，使得我们可以通过数学方法来优化模型。</p><h2 id=梯度><a href=#梯度 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:梯度 class=headings>梯度</a></h2><p>在神经网络中，梯度是一个非常重要的概念。下面是对梯度及其在神经网络中作用的解释：</p><ol><li><p><strong>梯度的含义</strong>:</p><ul><li>在数学和物理学中，梯度通常指的是一个函数在每个点上的斜率或变化率。在多维空间中，它表示该函数在每个方向上的斜率。</li><li>在神经网络中，梯度通常指代损失函数（或目标函数）关于网络参数（如权重和偏差）的偏导数。它表明了损失函数在参数空间的每个维度上增加或减少的速率。</li></ul></li><li><p><strong>梯度在神经网络中的作用</strong>:</p><ul><li>梯度用于优化神经网络的参数。通过计算损失函数相对于每个参数的梯度，神经网络可以了解如何调整其参数以减少总损失。</li><li>这通常通过一个过程称为梯度下降来完成，其中参数沿着梯度的反方向调整，以减少损失函数的值。</li></ul></li></ol><h3 id=导数-偏导数和梯度><a href=#导数-偏导数和梯度 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:导数-偏导数和梯度 class=headings>导数、偏导数和梯度</a></h3><ol><li><p><strong>导数（Derivative）</strong>:</p><ul><li>导数是一个单变量函数在某一点上的瞬时变化率。它告诉我们，当输入变量发生非常小的变化时，函数值将如何变化。</li><li>在数学上，如果有一个函数 f(x)，那么在点 x 的导数 f′(x) 表示当 x 发生微小变化时，f(x) 的变化量。</li></ul></li><li><p><strong>偏导数（Partial Derivative）</strong>:</p><ul><li>偏导数是多变量函数对其中一个变量的导数，同时假设其他变量保持不变。例如，对于函数 f(x,y)，对 x 的偏导数 \(\frac{∂f}{∂x}\) 表示当 x 发生微小变化而 y 保持不变时，函数值的变化率。</li><li>偏导数描述的是在多维空间中，函数沿着某一个坐标轴的变化率。</li></ul></li><li><p><strong>梯度（Gradient）</strong>:</p><ul><li>梯度是导数在多维空间中的推广。它不是针对单一变量，而是针对多变量函数的每个独立变量的偏导数的集合。</li><li>在数学上，对于多变量函数 f(x,y,z,&mldr;)，梯度是一个向量，其每个分量都是对应于一个变量的偏导数。梯度指向函数增长最快的方向。</li></ul></li><li><p><strong>区别</strong>:</p><ul><li><strong>维度</strong>: 导数是一维的，而梯度是多维的。</li><li><strong>方向</strong>: 导数只有大小，没有方向（或者可以说是正或负的方向），而梯度是一个向量，具有大小和方向。</li><li><strong>应用</strong>: 在单变量函数的情境中使用导数，而在多变量函数的情境中使用梯度。</li></ul></li></ol><p>在神经网络的背景下，考虑到网络参数通常是多维的（例如权重矩阵），因此更多地使用梯度而不是单一的导数。梯度指明了在参数空间中减少损失函数的最快路径。</p><h2 id=梯度下降-gradient-descent><a href=#梯度下降-gradient-descent class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:梯度下降-gradient-descent class=headings>梯度下降（Gradient Descent）</a></h2><p>梯度下降算法是一种用于优化神经网络参数的方法，其目的是最小化损失函数。我们可以用一个比喻来理解梯度下降：</p><p>想象你在一个山的顶部，目标是到达山谷（这里的山谷代表损失函数的最小值）。但是，你的眼睛被蒙住了，看不到周围的环境。你只能通过感觉脚下的坡度（这就像是梯度）来判断哪个方向是下坡。</p><ol><li><p><strong>检查坡度（计算梯度）</strong>:</p><ul><li>在神经网络中，你首先计算损失函数在当前位置（即当前参数值）的梯度。梯度告诉你损失函数上升和下降最快的方向。在我们的比喻中，这就像是感觉脚下地面的坡度，了解哪个方向是下坡。</li></ul></li><li><p><strong>迈出一步（更新参数）</strong>:</p><ul><li>接下来，你会朝着梯度下降最快的方向迈出一步。这一步的大小称为学习率。在神经网络中，这意味着你会根据梯度和学习率来调整网络的权重和偏差。</li><li>学习率很重要：如果太大，你可能会跨过山谷，错过最低点；如果太小，你则需要很长时间才能到达山谷。</li></ul></li><li><p><strong>重复过程</strong>:</p><ul><li>重复这个过程，每次都根据损失函数的梯度来更新你的位置（即神经网络的参数），直到你感觉到自己已经到达了山谷（即损失函数不再显著下降）。</li></ul></li></ol><p>总体来讲，梯度下降是一个迭代过程，它通过不断地计算梯度并更新参数，来帮助神经网络找到损失函数的最小值。这个过程就像是在盲目中找到一条通往山谷的路径。</p><h3 id=成本表面和等值线图-cost-surface-and-contour-plot><a href=#成本表面和等值线图-cost-surface-and-contour-plot class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:成本表面和等值线图-cost-surface-and-contour-plot class=headings>成本表面和等值线图（Cost Surface & Contour Plot）</a></h3><ul><li>高度代表成本</li><li>其余两个轴分别为权重 <strong>w</strong>​eight、偏差 <strong>b</strong>​ias</li></ul><figure><img src=/ox-hugo/2024-04-28_16-29-15_screenshot.png></figure><p>上面两张图有助于理解梯度下降算法（图片参考自 <a href=https://www.coursera.org/learn/deep-neural-networks-with-pytorch/lecture/cEwJV/pytorch-linear-regression-training-slope-and-bias target=_blank rel=noopener>PyTorch Linear Regression Training Slope and Bias</a> ）。</p><p>上图中的 \(l\)（损失函数）是均方误差（MSE），即：</p><p>\[
l(w,b)=\frac{1}{N}\sum_{n=1}^N(y_n-(wx_n+b))^2
\]</p><p>学习（训练）的目标，就是找到让这个函数的函数值最小的 w 和 b。</p><h3 id=如何根据损失函数的梯度-学习率更新权重><a href=#如何根据损失函数的梯度-学习率更新权重 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:如何根据损失函数的梯度-学习率更新权重 class=headings>如何根据损失函数的梯度、学习率更新权重</a></h3><p>在深度学习中，使用梯度来更新权重是优化模型的关键步骤，这通常在反向传播过程中进行。梯度本质上是损失函数相对于模型权重的偏导数，它指示了损失函数相对于每个权重增加或减少的方向和幅度。通过梯度，我们可以知道如何调整权重以减少损失。</p><p>权重更新的基本公式如下：</p><p>\[
W_{new}=W_{old}−η⋅∇L(W_{old})
\]</p><p>其中：</p><ul><li>\(W_{new}\) 是更新后的权重。</li><li>\(W_{old}\) 是当前的权重。</li><li>η 是学习率。</li><li>\(∇L(W_{old})\) 是损失函数相对于当前权重的梯度。</li></ul><p>梯度的作用可以这样理解：</p><dl><dt>方向</dt><dd>梯度的方向指向损失函数增长最快的方向。在优化过程中，我们希望减少损失，因此需要向梯度的相反方向更新权重。</dd><dt>幅度</dt><dd>梯度的幅度（大小）告诉我们在该方向上损失函数变化的速度。如果梯度很大，意味着在那个方向上损失函数变化很快。</dd></dl><p>学习率的作用：</p><ul><li>学习率决定了我们在梯度指示的方向上移动的步长。较高的学习率意味着更大的步长，可以加快学习过程，但也可能导致超过最优点，甚至导致模型不稳定。相反，较低的学习率意味着更小的步长，学习过程更稳定，但训练速度会减慢，且可能陷入局部最小值。</li></ul><p>因此，选择合适的学习率是非常重要的。太高或太低的学习率都可能导致训练效果不佳。在实践中，通常需要通过实验来确定最佳的学习率。此外，也有一些自适应学习率的优化算法（如 Adam、RMSprop 等），它们可以在训练过程中自动调整学习率，以提高训练的效果和稳定性。</p><h2 id=常见的神经网络类型><a href=#常见的神经网络类型 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:常见的神经网络类型 class=headings>常见的神经网络类型</a></h2><p>常见的神经网络类型：</p><ol><li><p><strong>全连接神经网络（Fully Connected Networks）</strong>:</p><ul><li>也被称为密集神经网络，是最基本的神经网络形式。在这种网络中，每个神经元与前一层的所有神经元相连。</li><li>适用于结构化数据，如表格数据或简单的分类任务。</li></ul></li><li><p><strong>卷积神经网络（Convolutional Neural Networks, CNNs）</strong>:</p><ul><li>特别适用于处理图像数据。通过卷积层，CNN 能够捕捉图像中的局部特征。</li><li>广泛应用于图像分类、物体检测、图像分割等任务。</li></ul></li><li><p><strong>循环神经网络（Recurrent Neural Networks, RNNs）</strong>:</p><ul><li>设计用来处理序列数据，如时间序列数据或自然语言。</li><li>RNN 能够处理输入数据的时间动态特性，适用于语音识别、语言建模等任务。</li></ul></li><li><p><strong>长短时记忆网络（Long Short-Term Memory, LSTM）</strong>:</p><ul><li>LSTM 是 RNN 的一种变体，它能够学习长期依赖关系，解决了普通 RNN 难以捕捉长期依赖的问题。</li><li>常用于复杂的序列任务，如机器翻译、文本生成等。</li></ul></li><li><p><strong>生成对抗网络（Generative Adversarial Networks, GANs）</strong>:</p><ul><li>由两部分组成：生成器和鉴别器。GANs 能够生成新的、与真实数据类似的数据。</li><li>应用于图像生成、图像风格转换等领域。</li></ul></li><li><p><strong>变换器网络（Transformer Networks）</strong>:</p><ul><li>主要用于处理序列数据，特别是在自然语言处理领域表现出色。</li><li>依赖于“注意力机制”，能够同时处理整个序列，提高了处理长序列的能力。</li></ul></li></ol><p>这些网络可以根据具体问题和数据类型进行选择和定制。</p><h2 id=参考资料><a href=#参考资料 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:参考资料 class=headings>参考资料</a></h2><ul><li><a href=https://www.coursera.org/learn/deep-neural-networks-with-pytorch/home/week/1 target=_blank rel=noopener>Deep Neural Networks with PyTorch</a></li><li><a href=https://pytorch.org/tutorials/beginner/basics/intro.html target=_blank rel=noopener>Learn the Basics — PyTorch Tutorials 2.3.0+cu121 documentation</a></li></ul></div><ul class=post-copyright><li class="copyright-item author"><span class=copyright-item-text>Author</span>: <a href=https://mincodes.com/ class="p-author h-card" target=_blank rel=noopener>Min Deng</a></li><li class="copyright-item link"><span class=copyright-item-text>Link</span>: <a href=/posts/deep-learning-notes-theory-foundation/ target=_blank rel=noopener>https://mincodes.com/posts/deep-learning-notes-theory-foundation/</a></li><li class="copyright-item license"><span class=copyright-item-text>License</span>: <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en target=_blank rel=noopener>CC BY-NC-SA 4.0</a></li></ul></article><div class=updated-badge-container><span title="Updated @ 2024-04-28 18:16:19 +0800" style=cursor:help><svg xmlns="http://www.w3.org/2000/svg" width="130" height="20" class="updated-badge"><linearGradient id="b" x2="0" y2="100%"><stop offset="0" stop-color="#bbb" stop-opacity=".1"/><stop offset="1" stop-opacity=".1"/></linearGradient><clipPath id="a"><rect width="130" height="20" rx="3" fill="#fff"/></clipPath><g clip-path="url(#a)"><path class="updated-badge-left" d="M0 0h55v20H0z"/><path class="updated-badge-right" d="M55 0h75v20H55z"/><path fill="url(#b)" d="M0 0h130v20H0z"/></g><g fill="#fff" text-anchor="middle" font-size="110"><text x="285" y="150" fill="#010101" fill-opacity=".3" textLength="450" transform="scale(.1)">updated</text><text x="285" y="140" textLength="450" transform="scale(.1)">updated</text><text x="915" y="150" fill="#010101" fill-opacity=".3" textLength="650" transform="scale(.1)">2024-04-28</text><text x="915" y="140" textLength="650" transform="scale(.1)">2024-04-28</text></g></svg></span></div><div class=post-share><div class=share-items><div class="share-item facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=https://mincodes.com/posts/deep-learning-notes-theory-foundation/&amp;hashtag=%23Ai" title="Share on Facebook" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon facebook-icon"><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14.0 55.52 4.84 55.52 4.84v61h-31.28c-30.8.0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg></a></div><div class="share-item mastodon"><a href="/fedishare.html#title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80&amp;description=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%92%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%20%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%98%af%e4%b8%80%e7%a7%8d%e5%ae%9e%e7%8e%b0%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%8a%80%e6%9c%af%ef%bc%8c%e8%80%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e5%ae%9e%e7%8e%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e6%9c%ac%e7%bb%93%e6%9e%84%e3%80%82%20%e5%85%b7%e4%bd%93%e6%9d%a5%e8%af%b4%ef%bc%9a%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c:%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e4%b8%80%e7%a7%8d%e6%a8%a1%e4%bb%bf%e4%ba%ba%e8%84%91%e5%a4%84%e7%90%86%e4%bf%a1%e6%81%af%e6%96%b9%e5%bc%8f%e7%9a%84%e8%ae%a1%e7%ae%97%e7%b3%bb%e7%bb%9f%e3%80%82%e5%ae%83%e7%94%b1%e5%a4%a7%e2%80%a6&amp;url=https://mincodes.com/posts/deep-learning-notes-theory-foundation/" title="Share on Mastodon" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon mastodon-icon"><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48.0.0.0-63.72 28.5-63.72 125.7.0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.54 102.54.0 01-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5zm-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg></a></div><div class="share-item fediverse"><a href="/fedishare.html#title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80&amp;description=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%92%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%20%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%98%af%e4%b8%80%e7%a7%8d%e5%ae%9e%e7%8e%b0%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%8a%80%e6%9c%af%ef%bc%8c%e8%80%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e5%ae%9e%e7%8e%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e6%9c%ac%e7%bb%93%e6%9e%84%e3%80%82%20%e5%85%b7%e4%bd%93%e6%9d%a5%e8%af%b4%ef%bc%9a%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c:%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e4%b8%80%e7%a7%8d%e6%a8%a1%e4%bb%bf%e4%ba%ba%e8%84%91%e5%a4%84%e7%90%86%e4%bf%a1%e6%81%af%e6%96%b9%e5%bc%8f%e7%9a%84%e8%ae%a1%e7%ae%97%e7%b3%bb%e7%bb%9f%e3%80%82%e5%ae%83%e7%94%b1%e5%a4%a7%e2%80%a6&amp;url=https://mincodes.com/posts/deep-learning-notes-theory-foundation/" title="Share on Fediverse" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="64 163 873 873" class="icon fediverse-icon"><defs><linearGradient id="fediverse-gradient"><stop offset="0" stop-color="#ff0101"/><stop offset="10%" stop-color="#9501ff"/><stop offset="50%" stop-color="#ffca01"/><stop offset="75%" stop-color="#01a3ff"/><stop offset="100%" stop-color="#65ff01"/></linearGradient></defs><path d="M539 176q-32 0-55 22t-25 55 20.5 58 56 27 58.5-20.5 27-56-20.5-59T544 176h-5zm-87 95-232 118q20 20 25 48l231-118q-19-20-24-48zm167 27q-13 25-38 38l183 184q13-25 39-38zM477 320 342 585l40 40 143-280q-28-5-48-25zm104 16q-22 11-46 10l-8-1 21 132 56 9zM155 370q-32 0-55 22.5t-25 55 20.5 58 56.5 27 59-21 26.5-56-21-58.5-55.5-27h-6zm90 68q1 9 1 18-1 19-10 35l132 21 26-50zm225 36-26 51 311 49q-1-8-1-17 1-19 10-36zm372 6q-32 1-55 23t-24.5 55 21 58 56 27 58.5-20.5 27-56.5-20.5-59-56.5-27h-6zM236 493q-13 25-39 38l210 210 51-25zm-40 38q-21 11-44 10l-9-1 40 256q21-10 45-9l8 1zm364 22 48 311q21-10 44-9l10 1-46-294zm195 23-118 60 8 56 135-68q-20-20-25-48zm26 49-119 231q28 5 48 25l119-231q-28-5-48-25zM306 654l-68 134q28 5 48 25l60-119zm262 17-281 143q19 20 24 48l265-135zM513 771l-51 25 106 107q13-25 39-38zM222 795q-32 0-55.5 22.5t-25 55 21 57.5 56 27 58.5-20.5 27-56-20.5-58.5-56.5-27h-5zm89 68q2 9 1 18-1 19-9 35l256 41q-1-9-1-18 1-18 10-35zm335 0q-32 0-55 22.5t-24.5 55 20.5 58 56 27 59-21 27-56-20.5-58.5-56.5-27h-6z"/></svg></a></div><div class="share-item twitter"><a href="https://twitter.com/share?url=https://mincodes.com/posts/deep-learning-notes-theory-foundation/&amp;text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80&amp;hashtags=Ai,Deep-Learning,&amp;via=%25!s%28%3cnil%3e%29" title="Share on Twitter" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon twitter-icon"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a></div><div class="share-item linkedin"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://mincodes.com/posts/deep-learning-notes-theory-foundation/&amp;title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80&amp;summary=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%92%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%20%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%98%af%e4%b8%80%e7%a7%8d%e5%ae%9e%e7%8e%b0%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%8a%80%e6%9c%af%ef%bc%8c%e8%80%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e5%ae%9e%e7%8e%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e6%9c%ac%e7%bb%93%e6%9e%84%e3%80%82%20%e5%85%b7%e4%bd%93%e6%9d%a5%e8%af%b4%ef%bc%9a%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c:%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e4%b8%80%e7%a7%8d%e6%a8%a1%e4%bb%bf%e4%ba%ba%e8%84%91%e5%a4%84%e7%90%86%e4%bf%a1%e6%81%af%e6%96%b9%e5%bc%8f%e7%9a%84%e8%ae%a1%e7%ae%97%e7%b3%bb%e7%bb%9f%e3%80%82%e5%ae%83%e7%94%b1%e5%a4%a7%e2%80%a6&amp;source=MinCodes" title="Share on LinkedIn" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon linkedin-icon"><path d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></a></div><div class="share-item telegram"><a href="https://t.me/share/url?url=https://mincodes.com/posts/deep-learning-notes-theory-foundation/&amp;text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80" title="Share on Telegram" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon telegram-icon"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm121.8 169.9-40.7 191.8c-3 13.6-11.1 16.9-22.4 10.5l-62-45.7-29.9 28.8c-3.3 3.3-6.1 6.1-12.5 6.1l4.4-63.1 114.9-103.8c5-4.4-1.1-6.9-7.7-2.5l-142 89.4-61.2-19.1c-13.3-4.2-13.6-13.3 2.8-19.7l239.1-92.2c11.1-4 20.8 2.7 17.2 19.5z"/></svg></a></div><div class="share-item weibo"><a href="https://service.weibo.com/share/share.php?&amp;url=https://mincodes.com/posts/deep-learning-notes-theory-foundation/&amp;title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80&amp;pic=https://mincodes.com/ox-hugo/2024-04-28_16-29-15_screenshot.png&amp;searchPic=false" title="Share on Weibo" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon weibo-icon"><path d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7.0 395.3.0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"/></svg></a></div><div class="share-item douban"><a href="https://www.douban.com/share/service?href=https://mincodes.com/posts/deep-learning-notes-theory-foundation/&amp;name=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80&amp;text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%92%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%20%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%98%af%e4%b8%80%e7%a7%8d%e5%ae%9e%e7%8e%b0%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%8a%80%e6%9c%af%ef%bc%8c%e8%80%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e5%ae%9e%e7%8e%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e6%9c%ac%e7%bb%93%e6%9e%84%e3%80%82%20%e5%85%b7%e4%bd%93%e6%9d%a5%e8%af%b4%ef%bc%9a%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c:%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e4%b8%80%e7%a7%8d%e6%a8%a1%e4%bb%bf%e4%ba%ba%e8%84%91%e5%a4%84%e7%90%86%e4%bf%a1%e6%81%af%e6%96%b9%e5%bc%8f%e7%9a%84%e8%ae%a1%e7%ae%97%e7%b3%bb%e7%bb%9f%e3%80%82%e5%ae%83%e7%94%b1%e5%a4%a7%e2%80%a6" title="Share on Douban" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon douban-icon"><path d="M.643.92v2.412h22.714V.92H.643zm1.974 4.926v9.42h18.764v-9.42H2.617zm2.72 2.408H18.69v4.605H5.338V8.254zm1.657 7.412-2.512.938c1.037 1.461 1.87 2.825 2.512 4.091H0v2.385h24v-2.385h-6.678c.818-1.176 1.589-2.543 2.303-4.091l-2.73-.938a29.952 29.952.0 01-2.479 5.03h-4.75c-.786-1.962-1.677-3.641-2.672-5.03z"/></svg></a></div><div class="share-item qq"><a href="https://connect.qq.com/widget/shareqq/index.html?url=https://mincodes.com/posts/deep-learning-notes-theory-foundation/&amp;title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80&amp;summary=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%92%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%20%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%98%af%e4%b8%80%e7%a7%8d%e5%ae%9e%e7%8e%b0%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%8a%80%e6%9c%af%ef%bc%8c%e8%80%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e5%ae%9e%e7%8e%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e6%9c%ac%e7%bb%93%e6%9e%84%e3%80%82%20%e5%85%b7%e4%bd%93%e6%9d%a5%e8%af%b4%ef%bc%9a%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c:%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e4%b8%80%e7%a7%8d%e6%a8%a1%e4%bb%bf%e4%ba%ba%e8%84%91%e5%a4%84%e7%90%86%e4%bf%a1%e6%81%af%e6%96%b9%e5%bc%8f%e7%9a%84%e8%ae%a1%e7%ae%97%e7%b3%bb%e7%bb%9f%e3%80%82%e5%ae%83%e7%94%b1%e5%a4%a7%e2%80%a6&amp;pics=https://mincodes.com/ox-hugo/2024-04-28_16-29-15_screenshot.png&amp;site=MinCodes" title="Share on QQ" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon qq-icon"><path d="M433.754 420.445c-11.526 1.393-44.86-52.741-44.86-52.741.0 31.345-16.136 72.247-51.051 101.786 16.842 5.192 54.843 19.167 45.803 34.421-7.316 12.343-125.51 7.881-159.632 4.037-34.122 3.844-152.316 8.306-159.632-4.037-9.045-15.25 28.918-29.214 45.783-34.415-34.92-29.539-51.059-70.445-51.059-101.792.0.0-33.334 54.134-44.859 52.741-5.37-.65-12.424-29.644 9.347-99.704 10.261-33.024 21.995-60.478 40.144-105.779C60.683 98.063 108.982.006 224 0c113.737.006 163.156 96.133 160.264 214.963 18.118 45.223 29.912 72.85 40.144 105.778 21.768 70.06 14.716 99.053 9.346 99.704z"/></svg></a></div><div class="share-item qzone"><a href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://mincodes.com/posts/deep-learning-notes-theory-foundation/&amp;title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80&amp;summary=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%92%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%20%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%98%af%e4%b8%80%e7%a7%8d%e5%ae%9e%e7%8e%b0%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%8a%80%e6%9c%af%ef%bc%8c%e8%80%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e5%ae%9e%e7%8e%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e6%9c%ac%e7%bb%93%e6%9e%84%e3%80%82%20%e5%85%b7%e4%bd%93%e6%9d%a5%e8%af%b4%ef%bc%9a%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c:%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e4%b8%80%e7%a7%8d%e6%a8%a1%e4%bb%bf%e4%ba%ba%e8%84%91%e5%a4%84%e7%90%86%e4%bf%a1%e6%81%af%e6%96%b9%e5%bc%8f%e7%9a%84%e8%ae%a1%e7%ae%97%e7%b3%bb%e7%bb%9f%e3%80%82%e5%ae%83%e7%94%b1%e5%a4%a7%e2%80%a6&amp;pics=https://mincodes.com/ox-hugo/2024-04-28_16-29-15_screenshot.png&amp;site=MinCodes" title="Share on Qzone" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon qzone-icon"><path d="M23.985 9.202c-.032-.099-.127-.223-.334-.258-.207-.036-7.351-1.406-7.351-1.406s-.105-.022-.198-.07c-.092-.047-.127-.167-.127-.167S12.447.956 12.349.77C12.25.583 12.104.532 12 .532s-.251.051-.349.238c-.098.186-3.626 6.531-3.626 6.531s-.035.12-.128.167c-.092.047-.197.07-.197.07S.556 8.908.348 8.943c-.208.036-.302.16-.333.258a.477.477.0 00.125.449l5.362 5.49s.072.08.119.172c.016.104.005.21.005.21s-1.189 7.242-1.22 7.45.075.369.159.43c.083.062.233.106.421.013.189-.093 6.812-3.261 6.812-3.261s.098-.044.201-.061.201.061.201.061 6.623 3.168 6.812 3.261c.188.094.338.049.421-.013a.463.463.0 00.159-.43c-.021-.14-.93-5.677-.93-5.677.876-.54 1.425-1.039 1.849-1.747-2.594.969-6.006 1.717-9.415 1.866-.915.041-2.41.097-3.473-.015-.678-.071-1.17-.144-1.243-.438-.053-.215.054-.46.545-.831a2640.5 2640.5.0 012.861-2.155c1.285-.968 3.559-2.47 3.559-2.731.0-.285-2.144-.781-4.037-.781-1.945.0-2.275.132-2.811.168-.488.034-.769.005-.804-.138-.06-.248.183-.389.588-.568.709-.314 1.86-.594 1.984-.626.194-.052 3.082-.805 5.618-.535 1.318.14 3.244.668 3.244 1.276.0.342-1.721 1.494-3.225 2.597-1.149.843-2.217 1.561-2.217 1.688.0.342 3.533 1.241 6.689 1.01l.003-.022c.048-.092.119-.172.119-.172l5.362-5.49a.477.477.0 00.127-.449z"/></svg></a></div><div class="share-item pocket"><a href="https://getpocket.com/edit.php?url=https://mincodes.com/posts/deep-learning-notes-theory-foundation/" title="Share on Pocket" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon pocket-icon"><path d="M407.6 64h-367C18.5 64 0 82.5.0 104.6v135.2C0 364.5 99.7 464 224.2 464c124 0 223.8-99.5 223.8-224.2V104.6c0-22.4-17.7-40.6-40.4-40.6zm-162 268.5c-12.4 11.8-31.4 11.1-42.4.0C89.5 223.6 88.3 227.4 88.3 209.3c0-16.9 13.8-30.7 30.7-30.7 17 0 16.1 3.8 105.2 89.3 90.6-86.9 88.6-89.3 105.5-89.3 16.9.0 30.7 13.8 30.7 30.7.0 17.8-2.9 15.7-114.8 123.2z"/></svg></a></div><div class="share-item hackernews"><a href="https://news.ycombinator.com/submitlink?u=https://mincodes.com/posts/deep-learning-notes-theory-foundation/&amp;t=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80" title="Share on Hacker News" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon hackernews-icon"><path d="M0 32v448h448V32H0zm21.2 197.2H21c.1-.1.2-.3.3-.4.0.1.0.3-.1.4zm218 53.9V384h-31.4V281.3L128 128h37.3c52.5 98.3 49.2 101.2 59.3 125.6 12.3-27 5.8-24.4 60.6-125.6H320l-80.8 155.1z"/></svg></a></div><div class="share-item qrcode"><div class=qrcode-container title="Share via QR Code"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon qrcode-icon"><path d="M0 224h192V32H0v192zM64 96h64v64H64V96zm192-64v192h192V32H256zm128 128h-64V96h64v64zM0 480h192V288H0v192zm64-128h64v64H64v-64zm352-64h32v128h-96v-32h-32v96h-64V288h96v32h64v-32zm0 160h32v32h-32v-32zm-64 0h32v32h-32v-32z"/></svg><div id=qrcode-img></div></div><script src=https://cdn.jsdelivr.net/npm/qrcode-generator@1.4.4/qrcode.min.js></script><script>const typeNumber=0,errorCorrectionLevel="L",qr=qrcode(typeNumber,errorCorrectionLevel);qr.addData("https://mincodes.com/posts/deep-learning-notes-theory-foundation/"),qr.make(),document.getElementById("qrcode-img").innerHTML=qr.createImgTag()</script></div><div class="share-item email"><a href="mailto:?subject=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%9a%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80&amp;body=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%92%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%20%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%98%af%e4%b8%80%e7%a7%8d%e5%ae%9e%e7%8e%b0%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%8a%80%e6%9c%af%ef%bc%8c%e8%80%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e5%ae%9e%e7%8e%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e6%9c%ac%e7%bb%93%e6%9e%84%e3%80%82%20%e5%85%b7%e4%bd%93%e6%9d%a5%e8%af%b4%ef%bc%9a%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c:%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%98%af%e4%b8%80%e7%a7%8d%e6%a8%a1%e4%bb%bf%e4%ba%ba%e8%84%91%e5%a4%84%e7%90%86%e4%bf%a1%e6%81%af%e6%96%b9%e5%bc%8f%e7%9a%84%e8%ae%a1%e7%ae%97%e7%b3%bb%e7%bb%9f%e3%80%82%e5%ae%83%e7%94%b1%e5%a4%a7%e2%80%a6%0Ahttps://mincodes.com/posts/deep-learning-notes-theory-foundation/" title="Share via email" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon email-icon"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V4e2c0 26.5-21.5 48-48 48H48c-26.5.0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5.0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg></a></div></div></div><div class=post-tags><a href=/tags/ai/ rel=tag class=post-tags-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49.0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882.0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>Ai</a>
<a href=/tags/deep-learning/ rel=tag class=post-tags-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49.0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882.0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>Deep-Learning</a></div><ul class=post-nav><li class=post-nav-prev><a href=/posts/basic-usage-of-doom-emacs/ rel=prev>&lt; Doom Emacs 的基本用法</a></li><li class=post-nav-next><a href=/posts/design-patterns-behavioral/ rel=next>设计模式：行为型 (Behavioral) ></a></li></ul></div></main><div id=back-to-top class=back-to-top><a href=#><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6.0-33.9L207 39c9.4-9.4 24.6-9.4 33.9.0l194.3 194.3c9.4 9.4 9.4 24.6.0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3.0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a></div><footer id=footer class=footer><div class=footer-inner><div class=site-info>©&nbsp;2020–2024&nbsp;<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon footer-icon"><path d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3.0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"/></svg>&nbsp;Min Deng</div><div class=site-copyright><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en target=_blank rel=noopener>CC BY-NC-SA 4.0</a></div><div class=custom-footer><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?7b760b677a021bf17f98d03f83124b62",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script></div></div></footer></div><script>typeof MathJax=="undefined"?(window.MathJax={loader:{load:["[tex]/mhchem"]},tex:{inlineMath:{"[+]":[["$","$"]]},tags:"ams",packages:{"[+]":["mhchem"]}}},function(){const e=document.createElement("script");e.src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.texReset(),MathJax.typeset())</script><script src=https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js></script><script>let imgNodes=document.querySelectorAll("div.post-body img");imgNodes=Array.from(imgNodes).filter(e=>e.parentNode.tagName!=="A"),mediumZoom(imgNodes,{background:"hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)"})</script><script src=https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js type=module defer></script></body></html>